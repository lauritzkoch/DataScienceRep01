{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/danielstephensen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danielstephensen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import porter\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2 as pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleantext cleans the input string with the following functions: Characters are set to lowercase, \n",
    "#urls are substituted with <URL>, dates are substitured with <DATE>, emails are substitured with <EMAIL>\n",
    "#numbers are substitured with <NUM>, newlines and non-letter characters are removed.\n",
    "def cleantext(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<|>', \"\", text)\n",
    "    text = re.sub(r'(https?:\\/\\/)?w{0,3}\\.?[a-z]+\\.[a-z]\\w*[\\w\\/-]*', \"<URL>\", text)\n",
    "    text = re.sub(r'(jan\\.?(uary)?|feb\\.?(uary)?|mar\\.?(ch)?|apr\\.?(il)?|may|jun\\.(e)?|jul\\.(y)?|aug\\.?(ust)?|sep\\.?(tember)?|oct\\.?(ober)?|nov\\.?(ember)?|dec\\.?(ember)?|monday|tuesday|wednesday|thursday|friday|saturday|sunday) (the )?\\d{1,2}((th)?,?( \\d{4})?)?', \"<DATE>\", text)\n",
    "    text = re.sub(r'\\w+@\\w+\\.[a-zA-Z]{2,3}', \"<EMAIL>\", text)\n",
    "    text = re.sub(r'[0-9]+', \"<NUM>\", text)\n",
    "    text = re.sub(r'(\\\\n)+|\\s{2,}|(\\\\t+)', \" \", text)\n",
    "    text = re.sub(r'\\.|,|\\\\|-|\\?|\\(|\\)|\\||&|\"|”|“|:|!|\\+|-|–|—|\\/|\\$|%|€|#|;|\\[|\\]|©|®|…|=', \"\", text)\n",
    "    return text\n",
    "\n",
    "#cleanMetaKeywords cleans the input string with the following functions: \n",
    "#Characters are set to lowercase, newlines and non-letter characters are removed.\n",
    "def cleanMetaKeywords(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(\\\\n)+|\\s{2,}|(\\\\t+)', \" \", text)\n",
    "    text = re.sub(r'\\.|\\\\|-|\\?|\\(|\\)|\\||&|\"|”|“|:|!|\\+|-|\\'|–|—|\\/|\\$|%|€|#|;|\\[|\\]|©|®|…|=|<|>', \"\", text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def stopword(word_list):\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    return [word for word in word_list if word not in stop_words]\n",
    "\n",
    "def stemming(word_list):\n",
    "    stemmer = porter.PorterStemmer()\n",
    "\n",
    "    return [stemmer.stem(word) for word in word_list]\n",
    "\n",
    "def getSoup(url):\n",
    "    response = requests.get(next_page)\n",
    "    contents = response.content\n",
    "    return BeautifulSoup(contents, 'html.parser')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group members: Angelina Näsström (nzv947), Daniel Stephensen (fbp131), Kristina Wilke (mlt790), Lauritz Koch (hdg618)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the following procedures: cleaning, tokenizing, removing stopwords and stemming the data. When cleaning the data we made sure of the following: \n",
    "1. all letters are in lowercase\n",
    "2. all urls are written as < URL >\n",
    "3. all dates are written as < DATE >\n",
    "4. all emails are written as < EMAIL >\n",
    "5. all numbers are written as < NUM >\n",
    "6. all unimportant symbols are removed\n",
    "\n",
    "Converting all letters to lowercase makes it easier to compare different words. Point 2-5 are useful because it makes it possible to count the number of urls, dates, emails and numbers. Also, removing these makes sure that they are not treated as words. Removing unimportant symbols makes sure that these are not treated as words. \n",
    "\n",
    "Tokenization makes processing of the data easier, as it eliminates blank spaces and punctuations etc, making the text more homogeneous. In the tokenization process, we, for example, made all the data lower-case, thus not having two different results when processing 'Hello' and 'hello'.\n",
    "\n",
    "Removing stopwords is useful because these words do not help giving meaning to the documents, in other words they are noise.\n",
    "\n",
    "Stemming the data is useful because it makes sure that different variants of the same word is converted into the rood of the word. This way it is possible to make sure that two different words (same word with different endings) are understood the same way, because they actually have the exact same meaning.\n",
    "\n",
    "Implementing task 2 we have used the Pandas library, nltk library and re library. The Pandas library has just been used to read the data from the 'news_sample.csv' file. word_tokenize is a sublibrary of nltk that has some useful functions for tokenizing. stopwords is a sublibrary of nltk.corpus that has some useful functions for removing stopwords. porter is a sublibrary of nltk.stem that has some useful functions for stemming data. These three sublibraries are useful because you do not need to create your own complex functions to tokenize, remove stopwords and stem the data. We have not used the clean_text library because we it did not have all the functionality needed for the task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data from news_sample.csv\n",
    "datasample = pd.read_csv('news_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the content\n",
    "cleaned_content = [cleantext(article_content) for article_content in datasample['content']]\n",
    "\n",
    "#Tokenizing the cleaned data\n",
    "tokens = [tokenize(clean_text) for clean_text in cleaned_content]\n",
    "\n",
    "#Removing stopwords\n",
    "stopwords = [stopword(token_list) for token_list in tokens]\n",
    "\n",
    "#Stemming the data\n",
    "stemmed_data = [stemming(stopword_list) for stopword_list in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-trivial observation 1: How many percent of articles with the word \"trump\" in it is fake news?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 % of articles where the name 'trump' is present, is a fake news article\n"
     ]
    }
   ],
   "source": [
    "articles_vocabulary = [set(i) for i in tokens]\n",
    "trump_included = [vocabulary for vocabulary in range(len(articles_vocabulary)) if \"trump\" in articles_vocabulary[vocabulary]]\n",
    "trump_fake_news = 0\n",
    "\n",
    "for i in range(len(trump_included)):\n",
    "    if datasample['type'][i] == \"fake\":\n",
    "        trump_fake_news += 1\n",
    "\n",
    "print(int(trump_fake_news*100/len(trump_included)),\"% of articles where the name 'trump' is present, is a fake news article\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-trivial observation 2: Is the number of articles spread out tolerably evenly between the domains?\n",
    "\n",
    "non-trivial observation 3: Is there a link between which domain an article comes from and if it is fake news?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each of the 29 domains present in the corpus has the following amount of articles in the corpus:\n",
      " [  1.   1.   1.   1.   2.   5.   1.   2.   4.   1.   1.   1.   4.  17.\n",
      "   1.   2.   6.   7.   1.   1.   2. 155.  24.   1.   1.   3.   2.   1.\n",
      "   1.]\n",
      "\n",
      "Each of the 29 domains present in the corpus has the following amount of fake news articles:\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 155.   0.   0.   0.   0.   0.   0.\n",
      "   0.]\n",
      "\n",
      "This means that Beforeitsnews.com has 155 of the articles in the corpus and 62 % of all articles. Thus, the number of articles in the corpus are very unevenly spreed between the domains\n",
      "\n",
      "Also, 100 % of Beforeitsnews.com's articles are fake news and no other domain has fake news in its articles. Thus, there is a link between which domain an article comes from and if it is fake news\n"
     ]
    }
   ],
   "source": [
    "domainList = datasample['domain']\n",
    "typeList = datasample['type']\n",
    "domains = set(domainList)\n",
    "\n",
    "fakeDomainScore = np.zeros(len(domains))\n",
    "totalDomainScore = np.zeros(len(domains)) \n",
    "\n",
    "for i in range (len(domainList)):\n",
    "    if (typeList[i] == 'fake'):\n",
    "        index = 0 \n",
    "        for domain in domains:\n",
    "            if  domainList[i] == domain:\n",
    "                fakeDomainScore[index] += 1\n",
    "            index+=1\n",
    "    index = 0 \n",
    "    for domain in domains:\n",
    "        if  domainList[i] == domain:\n",
    "            totalDomainScore[index] += 1\n",
    "        index+=1\n",
    "\n",
    "print(\"Each of the 29 domains present in the corpus has the following amount of articles in the corpus:\\n\", totalDomainScore)\n",
    "print(\"\\nEach of the 29 domains present in the corpus has the following amount of fake news articles:\\n\", fakeDomainScore)\n",
    "print(\"\\nThis means that Beforeitsnews.com has\", int(totalDomainScore[np.where(fakeDomainScore == 155)]),\"of the articles in the corpus and\", int(fakeDomainScore[np.where(fakeDomainScore == 155)]*100/sum(totalDomainScore)), \"% of all articles. Thus, the number of articles in the corpus are very unevenly spreed between the domains\")\n",
    "print(\"\\nAlso,\", int(fakeDomainScore[np.where(fakeDomainScore == 155)]*100/totalDomainScore[np.where(fakeDomainScore == 155)]), \"% of Beforeitsnews.com's articles are fake news and no other domain has fake news in its articles. Thus, there is a link between which domain an article comes from and if it is fake news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-trivial observation 4: How many articles have missing author value? \n",
    "\n",
    "non-trivial observation 5: How much does missing author increase the likelihood that an article is fake news? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 % of the articles does not have an author\n",
      "63 % of the no-author articles are fake news\n",
      "61 % of the articles are fake news\n",
      "\n",
      "Thus we see, that having no author on an article only adds two percent points to the likelihood of it being fake\n"
     ]
    }
   ],
   "source": [
    "authors = datasample[\"authors\"]\n",
    "no_author_counter = 0\n",
    "no_author_fake_news = 0\n",
    "no_author_total = 0\n",
    "author_fake_news = 0\n",
    "author_total = 0\n",
    "\n",
    "for i in range(len(authors)):\n",
    "    if not type(authors[i]) == str:\n",
    "        no_author_counter += 1\n",
    "        if datasample[\"type\"][i] == \"fake\":\n",
    "            no_author_fake_news += 1\n",
    "        no_author_total += 1\n",
    "    elif datasample[\"type\"][i] == \"fake\":\n",
    "        author_fake_news += 1\n",
    "        author_total += 1\n",
    "    else: \n",
    "        author_total += 1\n",
    "\n",
    "print(int(no_author_counter*100/len(authors)), \"% of the articles does not have an author\")\n",
    "print(int(no_author_fake_news*100/no_author_total),'% of the no-author articles are fake news')\n",
    "print(int(author_fake_news*100/author_total),'% of the articles are fake news')\n",
    "print('\\nThus we see, that having no author on an article only adds two percent points to the likelihood of it being fake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following 'article start letters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCDEFGHIJK\n"
     ]
    }
   ],
   "source": [
    "group_nr = 1\n",
    "article_start_letters = \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr%23:group_nr%23+10]\n",
    "print(article_start_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The algorithm stops finding new articles when 'stop_searching' is set to True\n",
    "stop_searching = False\n",
    "\n",
    "#Finding the nextpage link in the first iteration is a little different, and therefore this value is needed\n",
    "first_iteration = True\n",
    "\n",
    "#The root url is the domain of wikinews\n",
    "root_link = 'https://en.wikinews.org'\n",
    "\n",
    "#next_page is the webpage that the algorithm searches for articles in next iteration of the while-loop\n",
    "next_page = root_link + '/w/index.php?title=Category:Politics_and_conflicts'\n",
    "\n",
    "#The links to the articles starting with the 'article_start_letters' are appended to 'article links'\n",
    "article_links = []\n",
    "\n",
    "#For each iteration this list gets some values if the first letter \n",
    "#of the first article in the next webpage is between A and K\n",
    "first_letter_between_B_K = []\n",
    "\n",
    "#A regex used for 'first_letter_between_B_K'\n",
    "continue_iterations = re.compile(r\"pagefrom=[A-K]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not(stop_searching):\n",
    "    soup = getSoup(next_page)\n",
    "    articles = soup.find(id=\"mw-pages\")\n",
    "    \n",
    "    links = [link.get(\"href\") for link in articles.find_all('a')]\n",
    "    \n",
    "    if first_iteration:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[0])\n",
    "        first_iteration = False\n",
    "        next_page = root_link + links[0]\n",
    "        article_links += [root_link + group_link for group_link in links[1:] if group_link[6] in article_start_letters]\n",
    "    else:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[1])\n",
    "        next_page = root_link + links[1]\n",
    "        article_links += [root_link + group_link for group_link in links[2:] if group_link[6] in article_start_letters]\n",
    "    \n",
    "    if len(first_letter_between_B_K) == 0:\n",
    "        stop_searching = True\n",
    "    \n",
    "    first_letter_between_B_K = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following box takes some time to run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_source_code = [getSoup(article) for article in article_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = range(1,len(article_links))\n",
    "article_titles = [article.find('h1').get_text() for article in article_source_code]\n",
    "article_release_date = [str(article.find(id=\"publishDate\"))[50:60] for article in article_source_code]\n",
    "article_urls = article_links\n",
    "article_content = [\" \".join([p.get_text() for p in ((article.find(id=\"mw-content-text\")).find(class=\"mw-parser-output\")).findall('p')]) for article in article_source_code]\n",
    "article_sources = [\", \".join([element.get('href') for element in ((article.find('ul')).find_all('a', rel = 'nofollow', class_ ='external text'))]) for article in article_source_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_articles = pd.DataFrame()\n",
    "\n",
    "pd_articles['Id'] = article_id\n",
    "pd_articles['Titles'] = article_titles\n",
    "pd_articles['Release_Date'] = article_release_date\n",
    "pd_articles['url'] = article_urls\n",
    "pd_articles['content'] = article_content\n",
    "pd_articles['Sources']= article_sources\n",
    "\n",
    "pd_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
