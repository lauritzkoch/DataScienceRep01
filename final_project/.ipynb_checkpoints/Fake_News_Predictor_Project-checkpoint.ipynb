{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/danielstephensen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danielstephensen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import porter\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2 as pc\n",
    "from IPython import display\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import transformers as ppb # pytorch transformers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3071: DtypeWarning: Columns (0,1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "#Reading the data from 1mio-raw.csv\n",
    "dataTotal = pd.read_csv('250t-raw.csv')\n",
    "\n",
    "#We will only analyse a smaller part of the data set\n",
    "data = dataTotal.sample(n=25000, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_database_login = \"dbname=datascience user=postgres password=****\"\n",
    "SQLtables_path = \"/Users/krist/Desktop/Uni/milestone/DataScienceRep01/SQLtables/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleantext cleans the input string with the following functions: Characters are set to lowercase, \n",
    "#urls are substituted with <URL>, dates are substitured with <DATE>, emails are substitured with <EMAIL>\n",
    "#numbers are substitured with <NUM>, newlines and non-letter characters are removed.\n",
    "def cleantext(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<|>', \"\", text)\n",
    "    text = re.sub(r'(https?:\\/\\/)?w{0,3}\\.?[a-z]+\\.[a-z]\\w*[\\w\\/-]*', \"<URL>\", text)\n",
    "    text = re.sub(r'(jan\\.?(uary)?|feb\\.?(uary)?|mar\\.?(ch)?|apr\\.?(il)?|may|jun\\.(e)?|jul\\.(y)?|aug\\.?(ust)?|sep\\.?(tember)?|oct\\.?(ober)?|nov\\.?(ember)?|dec\\.?(ember)?|monday|tuesday|wednesday|thursday|friday|saturday|sunday) (the )?\\d{1,2}((th)?,?( \\d{4})?)?', \"<DATE>\", text)\n",
    "    text = re.sub(r'\\w+@\\w+\\.[a-zA-Z]{2,3}', \"<EMAIL>\", text)\n",
    "    text = re.sub(r'[0-9]+', \"<NUM>\", text)\n",
    "    text = re.sub(r'(\\\\n)+|\\s{2,}|(\\\\t+)', \" \", text)\n",
    "    text = re.sub(r'\\.|,|\\\\|-|\\?|\\(|\\)|\\||&|\"|”|“|:|!|\\+|-|–|—|\\/|\\$|%|€|#|;|\\[|\\]|©|®|…|=', \"\", text)\n",
    "    return text\n",
    "\n",
    "#cleanMetaKeywords cleans the input string with the following functions: \n",
    "#Characters are set to lowercase, newlines and non-letter characters are removed.\n",
    "def cleanMetaKeywords(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(\\\\n)+|\\s{2,}|(\\\\t+)', \" \", text)\n",
    "    text = re.sub(r'\\.|\\\\|-|\\?|\\(|\\)|\\||&|\"|”|“|:|!|\\+|-|\\'|–|—|\\/|\\$|%|€|#|;|\\[|\\]|©|®|…|=|<|>', \"\", text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def stopword(word_list):\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    return [word for word in word_list if word not in stop_words]\n",
    "\n",
    "def stemming(word_list):\n",
    "    stemmer = porter.PorterStemmer()\n",
    "\n",
    "    return [stemmer.stem(word) for word in word_list]\n",
    "\n",
    "def getSoup(url):\n",
    "    response = requests.get(url)\n",
    "    contents = response.content\n",
    "    return BeautifulSoup(contents, 'html.parser')\n",
    "\n",
    "def executeSQL(filename, cur):\n",
    "    fd = open(filename, 'r')\n",
    "    sqlFile = fd.read()\n",
    "    fd.close()\n",
    "    sqlCommands = sqlFile.split(';')\n",
    "    for command in sqlCommands:\n",
    "            cur.execute(command)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping data from Politics and Conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_nr = 1\n",
    "article_start_letters = \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr%23:group_nr%23+10]\n",
    "print(article_start_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The algorithm stops finding new articles when 'stop_searching' is set to True\n",
    "stop_searching = False\n",
    "\n",
    "#Finding the nextpage link in the first iteration is a little different, and therefore this value is needed\n",
    "first_iteration = True\n",
    "\n",
    "#The root url is the domain of wikinews\n",
    "root_link = 'https://en.wikinews.org'\n",
    "\n",
    "#next_page is the webpage that the algorithm searches for articles in next iteration of the while-loop\n",
    "next_page = root_link + '/w/index.php?title=Category:Politics_and_conflicts'\n",
    "\n",
    "#The links to the articles starting with the 'article_start_letters' are appended to 'article links'\n",
    "article_links = []\n",
    "\n",
    "#For each iteration this list gets some values if the first letter \n",
    "#of the first article in the next webpage is between A and K\n",
    "first_letter_between_B_K = []\n",
    "\n",
    "#A regex used for 'first_letter_between_B_K'\n",
    "continue_iterations = re.compile(r\"pagefrom=[A-K]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not(stop_searching):\n",
    "    soup = getSoup(next_page)\n",
    "    articles = soup.find(id=\"mw-pages\")\n",
    "    \n",
    "    links = [link.get(\"href\") for link in articles.find_all('a')]\n",
    "    \n",
    "    if first_iteration:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[0])\n",
    "        first_iteration = False\n",
    "        next_page = root_link + links[0]\n",
    "        article_links += [root_link + group_link for group_link in links[1:] if group_link[6] in article_start_letters]\n",
    "    else:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[1])\n",
    "        next_page = root_link + links[1]\n",
    "        article_links += [root_link + group_link for group_link in links[2:] if group_link[6] in article_start_letters]\n",
    "    \n",
    "    if len(first_letter_between_B_K) == 0:\n",
    "        stop_searching = True\n",
    "    \n",
    "    first_letter_between_B_K = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_source_code = [getSoup(article) for article in article_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These id's has to be different from the other articles\n",
    "article_id = range(len(data),len(data)+len(article_links))\n",
    "article_titles = [article.find('h1').get_text() for article in article_source_code]\n",
    "article_release_date = [str(article.find(id=\"publishDate\"))[50:60] for article in article_source_code]\n",
    "article_urls = article_links\n",
    "article_content = [\" \".join([p.get_text() for p in (article.find(id=\"mw-content-text\")).find_all('p')]) for article in article_source_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_articles = pd.DataFrame()\n",
    "\n",
    "scraped_articles['id'] = article_id\n",
    "scraped_articles['content'] = [cleantext(content) for content in article_content]\n",
    "scraped_articles['title'] = article_titles\n",
    "scraped_articles['release_date'] = article_release_date\n",
    "scraped_articles['url'] = article_urls\n",
    "\n",
    "scraped_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_articles.to_csv(\"SQLtables/scraped_articles.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up a connection with the SQL server. Make sure that you write your own dbname, user and password as input\n",
    "conn = pc.connect(SQL_database_login)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executeSQL('SQLfiles/createTableScraped.sql', cur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicate articles\n",
    "data = data.drop_duplicates(subset=\"content\")\n",
    "data = data.dropna(subset=[\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the content\n",
    "cleaned_content = [cleantext(article_content) for article_content in data['content']]\n",
    "\n",
    "#Tokenizing the cleaned data\n",
    "#tokens = [tokenize(clean_text) for clean_text in cleaned_content]\n",
    "\n",
    "#Removing stopwords\n",
    "#stopwords = [stopword(token_list) for token_list in tokens]\n",
    "\n",
    "#Stemming the data (this is used for the 'keywords' attribute)\n",
    "#stemmed_data = [stemming(stopword_list) for stopword_list in stopwords]\n",
    "\n",
    "#Cleaning meta keywords\n",
    "clean_meta_keywords = [cleanMetaKeywords(metakeyword) for metakeyword in data[\"meta_keywords\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making sure that each element of 'tags', 'authors' and 'meta_keywords' are stripped stings and converting them to arrays\n",
    "data[\"tags\"] = [[tag.strip() for tag in (str(i)).split(\",\")] for i in data[\"tags\"]]\n",
    "data[\"authors\"] = [[author.strip() for author in (str(i)).split(\",\")] for i in data[\"authors\"]]\n",
    "data[\"meta_keywords\"] = [[meta_keyword.strip() for meta_keyword in (str(i)).split(\",\")] for i in clean_meta_keywords]\n",
    "data[\"content\"] = cleaned_content\n",
    "data[\"id\"] = range(0,len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entity tables\n",
    "articles = data[['id','content','url','meta_description','title']]\n",
    "\n",
    "meta_keywords = pd.DataFrame(set(data[['meta_keywords']].explode('meta_keywords')))\n",
    "meta_keywords = meta_keywords.rename(columns={0: 'meta_keywords'})\n",
    "meta_keywords[\"ide\"] = range(0,len(meta_keywords))\n",
    "article_meta_keywords = pd.merge(meta_keywords, data[['id','meta_keywords']].explode('meta_keywords'), on = \"meta_keywords\")[['id','ide']]\n",
    "article_meta_keywords = owns.rename(columns={'id': 'article_id', 'ide': 'meta_keyword_id'})\n",
    "\n",
    "\n",
    "domains = pd.DataFrame(set(data['domain']))\n",
    "domains = domains.rename(columns={0: 'domain'})\n",
    "domains[\"ide\"] = range(0,len(domains))\n",
    "owns = pd.merge(domains, data, on = \"domain\")[['id','ide']]\n",
    "owns = owns.rename(columns={'id': 'article_id', 'ide': 'domain_id'})\n",
    "\n",
    "authors = pd.DataFrame(set(data[['authors']].explode('authors')))\n",
    "authors = authors.rename(columns={0: 'authors'})\n",
    "authors[\"ide\"] = range(0,len(authors))\n",
    "article_authors = pd.merge(authors, data[['id','authors']].explode('authors'), on = \"authors\")[['id','ide']]\n",
    "article_authors = owns.rename(columns={'id': 'article_id', 'ide': 'author_id'})\n",
    "\n",
    "types = pd.DataFrame(set(data['type']))\n",
    "types = types.rename(columns={0: 'type'})\n",
    "types[\"ide\"] = range(0,len(types))\n",
    "article_types = pd.merge(types, data, on = \"type\")[['id','ide']]\n",
    "article_types = owns.rename(columns={'id': 'article_id', 'ide': 'type_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entities to CSV\n",
    "articles.to_csv(\"SQLtables/articles.csv\",index=False,header=False)\n",
    "meta_keywords.to_csv(\"SQLtables/meta_keywords.csv\",index=False,header=False)\n",
    "authors.to_csv(\"SQLtables/authors.csv\",index=False,header=False)\n",
    "domains.to_csv(\"SQLtables/domains.csv\",index=False,header=False)\n",
    "types.to_csv(\"SQLtables/types.csv\",index=False,header=False)\n",
    "\n",
    "#Relations to CSV\n",
    "owns.to_csv(\"SQLtables/owns.csv\",index=False,header=False)\n",
    "article_authors.to_csv(\"SQLtables/article_authors.csv\",index=False,header=False)\n",
    "article_meta_keywords.to_csv(\"SQLtables/article_meta_keywords.csv\",index=False,header=False)\n",
    "article_types.to_csv(\"SQLtables/article_types.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executeSQL('SQLfiles/createTables.sql', cur)\n",
    "executeSQL('SQLfiles/setUpTables.sql', cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = data[~data['type'].isin(['unreliable','junksci','rumor', 'hate', 'unknown'])] #removes these types\n",
    "data_1 = data_1.dropna(subset = ['type']) #drops where type is nan\n",
    "data_1.loc[data_1['type'].isin(['fake','satire','bias', 'conspiracy']), 'label'] = 'Fake' #labels types 'fake'\n",
    "data_1.loc[data_1['type'].isin(['reliable','political','clickbait']), 'label'] = 'True' #labels types 'true'\n",
    "data_1\n",
    "data_1 = data_1.sample(n=10000, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()\n",
    "content_tfidf = vect.fit_transform(data_1['content'])\n",
    "content_tfidf_df = pd.DataFrame(content_tfidf.todense(),columns = vect.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(content_tfidf, data_1['label'], test_size=0.40, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KNeighborsClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-448f1322a7fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mk_nearest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mk_nearest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KNeighborsClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "k_nearest = KNeighborsClassifier(n_neighbors=15, weights='distance')\n",
    "\n",
    "# Fit the model\n",
    "k_nearest.fit(X_train,y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "k_nearest_pred = k_nearest.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"k_nearest accuracy:\" + str(accuracy_score(y_test,k_nearest_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nDistilBertModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1ed0a22183e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/transformers/utils/dummy_pt_objects.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m         \u001b[0mrequires_backends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__name__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBACKENDS_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackends\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBACKENDS_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackends\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \nDistilBertModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n"
     ]
    }
   ],
   "source": [
    "#BERT\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
