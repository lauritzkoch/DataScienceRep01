{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\krist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\krist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import porter\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2 as pc\n",
    "from IPython import display\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krist\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3071: DtypeWarning: Columns (0,1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "#Reading the data from 1mio-raw.csv\n",
    "dataTotal = pd.read_csv('250t-raw.csv')\n",
    "\n",
    "#We will only analyse a smaller part of the data set\n",
    "data = dataTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_database_login = \"dbname=datascience user=postgres password=2419\"\n",
    "SQLtables_path = \"/Users/krist/Desktop/Uni/milestone/DataScienceRep01/final_project/SQLtables/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleantext cleans the input string with the following functions: Characters are set to lowercase, \n",
    "#urls are substituted with <URL>, dates are substitured with <DATE>, emails are substitured with <EMAIL>\n",
    "#numbers are substitured with <NUM>, newlines and non-letter characters are removed.\n",
    "def cleantext(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<|>', \"\", text)\n",
    "    text = re.sub(r'(https?:\\/\\/)?w{0,3}\\.?[a-z]+\\.[a-z]\\w*[\\w\\/-]*', \"<URL>\", text)\n",
    "    text = re.sub(r'(jan\\.?(uary)?|feb\\.?(uary)?|mar\\.?(ch)?|apr\\.?(il)?|may|jun\\.(e)?|jul\\.(y)?|aug\\.?(ust)?|sep\\.?(tember)?|oct\\.?(ober)?|nov\\.?(ember)?|dec\\.?(ember)?|monday|tuesday|wednesday|thursday|friday|saturday|sunday) (the )?\\d{1,2}((th)?,?( \\d{4})?)?', \"<DATE>\", text)\n",
    "    text = re.sub(r'\\w+@\\w+\\.[a-zA-Z]{2,3}', \"<EMAIL>\", text)\n",
    "    text = re.sub(r'[0-9]+', \"<NUM>\", text)\n",
    "    text = re.sub(r'(\\\\n)+|\\s{2,}|(\\\\t+)', \" \", text)\n",
    "    text = re.sub(r'\\.|,|\\\\|-|\\?|\\(|\\)|\\||&|\"|”|“|:|!|\\+|-|–|—|\\/|\\$|%|€|#|;|\\[|\\]|©|®|…|=', \"\", text)\n",
    "    return text\n",
    "\n",
    "#cleanMetaKeywords cleans the input string with the following functions: \n",
    "#Characters are set to lowercase, newlines and non-letter characters are removed.\n",
    "def cleanMetaKeywords(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(\\\\n)+|\\s{2,}|(\\\\t+)', \" \", text)\n",
    "    text = re.sub(r'\\.|\\\\|-|\\?|\\(|\\)|\\||&|\"|”|“|:|!|\\+|-|\\'|–|—|\\/|\\$|%|€|#|;|\\[|\\]|©|®|…|=|<|>', \"\", text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def stopword(word_list):\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    return [word for word in word_list if word not in stop_words]\n",
    "\n",
    "def stemming(word_list):\n",
    "    stemmer = porter.PorterStemmer()\n",
    "\n",
    "    return [stemmer.stem(word) for word in word_list]\n",
    "\n",
    "def getSoup(url):\n",
    "    response = requests.get(url)\n",
    "    contents = response.content\n",
    "    return BeautifulSoup(contents, 'html.parser')\n",
    "\n",
    "def executeSQL(filename, cur):\n",
    "    fd = open(filename, 'r')\n",
    "    sqlFile = fd.read()\n",
    "    fd.close()\n",
    "    sqlCommands = sqlFile.split(';')\n",
    "    for command in sqlCommands:\n",
    "            cur.execute(command)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping data from Politics and Conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_nr = 1\n",
    "article_start_letters = \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr%23:group_nr%23+10]\n",
    "print(article_start_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The algorithm stops finding new articles when 'stop_searching' is set to True\n",
    "stop_searching = False\n",
    "\n",
    "#Finding the nextpage link in the first iteration is a little different, and therefore this value is needed\n",
    "first_iteration = True\n",
    "\n",
    "#The root url is the domain of wikinews\n",
    "root_link = 'https://en.wikinews.org'\n",
    "\n",
    "#next_page is the webpage that the algorithm searches for articles in next iteration of the while-loop\n",
    "next_page = root_link + '/w/index.php?title=Category:Politics_and_conflicts'\n",
    "\n",
    "#The links to the articles starting with the 'article_start_letters' are appended to 'article links'\n",
    "article_links = []\n",
    "\n",
    "#For each iteration this list gets some values if the first letter \n",
    "#of the first article in the next webpage is between A and K\n",
    "first_letter_between_B_K = []\n",
    "\n",
    "#A regex used for 'first_letter_between_B_K'\n",
    "continue_iterations = re.compile(r\"pagefrom=[A-K]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not(stop_searching):\n",
    "    soup = getSoup(next_page)\n",
    "    articles = soup.find(id=\"mw-pages\")\n",
    "    \n",
    "    links = [link.get(\"href\") for link in articles.find_all('a')]\n",
    "    \n",
    "    if first_iteration:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[0])\n",
    "        first_iteration = False\n",
    "        next_page = root_link + links[0]\n",
    "        article_links += [root_link + group_link for group_link in links[1:] if group_link[6] in article_start_letters]\n",
    "    else:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[1])\n",
    "        next_page = root_link + links[1]\n",
    "        article_links += [root_link + group_link for group_link in links[2:] if group_link[6] in article_start_letters]\n",
    "    \n",
    "    if len(first_letter_between_B_K) == 0:\n",
    "        stop_searching = True\n",
    "    \n",
    "    first_letter_between_B_K = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_source_code = [getSoup(article) for article in article_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These id's has to be different from the other articles\n",
    "article_id = range(len(data),len(data)+len(article_links))\n",
    "article_titles = [article.find('h1').get_text() for article in article_source_code]\n",
    "article_release_date = [str(article.find(id=\"publishDate\"))[50:60] for article in article_source_code]\n",
    "article_urls = article_links\n",
    "article_content = [\" \".join([p.get_text() for p in (article.find(id=\"mw-content-text\")).find_all('p')]) for article in article_source_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_articles = pd.DataFrame()\n",
    "\n",
    "scraped_articles['id'] = article_id\n",
    "scraped_articles['content'] = [cleantext(content) for content in article_content]\n",
    "scraped_articles['title'] = article_titles\n",
    "scraped_articles['release_date'] = article_release_date\n",
    "scraped_articles['url'] = article_urls\n",
    "\n",
    "scraped_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_articles.to_csv(\"SQLtables/scraped_articles.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up a connection with the SQL server. Make sure that you write your own dbname, user and password as input\n",
    "conn = pc.connect(SQL_database_login)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executeSQL('SQLfiles/createTableScraped.sql', cur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data[\"title\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicate articles\n",
    "data = data.drop_duplicates(subset=\"content\")\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the content\n",
    "cleaned_content = [cleantext(article_content) for article_content in data['content']]\n",
    "\n",
    "#Tokenizing the cleaned data\n",
    "#tokens = [tokenize(clean_text) for clean_text in cleaned_content]\n",
    "\n",
    "#Removing stopwords\n",
    "#stopwords = [stopword(token_list) for token_list in tokens]\n",
    "\n",
    "#Stemming the data (this is used for the 'keywords' attribute)\n",
    "#stemmed_data = [stemming(stopword_list) for stopword_list in stopwords]\n",
    "\n",
    "#Cleaning meta keywords\n",
    "clean_meta_keywords = [cleanMetaKeywords(metakeyword) for metakeyword in data[\"meta_keywords\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making sure that each element of 'tags', 'authors' and 'meta_keywords' are stripped stings and converting them to arrays\n",
    "data[\"tags\"] = [[tag.strip() for tag in (str(i)).split(\",\")] for i in data[\"tags\"]]\n",
    "data[\"authors\"] = [[author.strip() for author in (str(i)).split(\",\")] for i in data[\"authors\"]]\n",
    "data[\"meta_keywords\"] = [[meta_keyword.strip() for meta_keyword in (str(i)).split(\",\")] for i in clean_meta_keywords]\n",
    "data[\"content\"] = cleaned_content\n",
    "data[\"id\"] = range(0,len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entity tables\n",
    "articles = data[['id','content','url','meta_description','title']]\n",
    "\n",
    "meta_keywords = pd.DataFrame((data[['meta_keywords']].explode('meta_keywords')).drop_duplicates(subset = 'meta_keywords'))\n",
    "meta_keywords = meta_keywords.rename(columns={0: 'meta_keywords'})\n",
    "meta_keywords[\"ide\"] = range(0,len(meta_keywords))\n",
    "article_meta_keywords = pd.merge(meta_keywords, data[['id','meta_keywords']].explode('meta_keywords'), on = \"meta_keywords\")[['id','ide']]\n",
    "article_meta_keywords = article_meta_keywords.rename(columns={'id': 'article_id', 'ide': 'meta_keyword_id'})\n",
    "\n",
    "domains = pd.DataFrame((data['domain']).drop_duplicates())\n",
    "domains = domains.rename(columns={0: 'domain'})\n",
    "domains[\"ide\"] = range(0,len(domains))\n",
    "owns = pd.merge(domains, data, on = \"domain\")[['id','ide']]\n",
    "owns = owns.rename(columns={'id': 'article_id', 'ide': 'domain_id'})\n",
    "\n",
    "authors = pd.DataFrame((data[['authors']].explode('authors')).drop_duplicates(subset = 'authors'))\n",
    "authors = authors.rename(columns={0: 'authors'})\n",
    "authors[\"ide\"] = range(0,len(authors))\n",
    "article_authors = pd.merge(authors, data[['id','authors']].explode('authors'), on = \"authors\")[['id','ide']]\n",
    "article_authors = article_authors.rename(columns={'id': 'article_id', 'ide': 'author_id'})\n",
    "\n",
    "types = pd.DataFrame((data['type']).drop_duplicates())\n",
    "types = types.rename(columns={0: 'type'})\n",
    "types[\"ide\"] = range(0,len(types))\n",
    "article_types = pd.merge(types, data, on = \"type\")[['id','ide']]\n",
    "article_types = article_types.rename(columns={'id': 'article_id', 'ide': 'type_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entities to CSV\n",
    "articles.to_csv(\"SQLtables/articles.csv\",index=False,header=False)\n",
    "meta_keywords.to_csv(\"SQLtables/meta_keywords.csv\",index=False,header=False)\n",
    "authors.to_csv(\"SQLtables/authors.csv\",index=False,header=False)\n",
    "domains.to_csv(\"SQLtables/domains.csv\",index=False,header=False)\n",
    "types.to_csv(\"SQLtables/types.csv\",index=False,header=False)\n",
    "\n",
    "#Relations to CSV\n",
    "owns.to_csv(\"SQLtables/owns.csv\",index=False,header=False)\n",
    "article_authors.to_csv(\"SQLtables/article_authors.csv\",index=False,header=False)\n",
    "article_meta_keywords.to_csv(\"SQLtables/article_meta_keywords.csv\",index=False,header=False)\n",
    "article_types.to_csv(\"SQLtables/article_types.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#executeSQL('SQLfiles/createTables.sql', cur)\n",
    "#executeSQL('SQLfiles/setUpTables.sql', cur)\n",
    "cur.execute(\"BEGIN TRANSACTION;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.article(id integer, content text COLLATE pg_catalog.\\\"default\\\", url text COLLATE pg_catalog.\\\"default\\\", meta_description text COLLATE pg_catalog.\\\"default\\\", title text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.article_meta_keywords(article_id integer, meta_keyword_id integer) WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.authors(name text COLLATE pg_catalog.\\\"default\\\", id integer) WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.authors_of(article_id integer, author_id integer) WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.domain(name text COLLATE pg_catalog.\\\"default\\\", id integer) WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.meta_keywords(meta_keyword text COLLATE pg_catalog.\\\"default\\\", id integer) WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.owns(article_id integer, domain_id integer) WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.types(type text COLLATE pg_catalog.\\\"default\\\", id integer) WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.article_types(article_id integer, type_id integer) WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"COMMIT TRANSACTION;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"BEGIN TRANSACTION;\")\n",
    "cur.execute(\"delete from article *; copy article from '\" + SQLtables_path + \"articles.csv' with (format csv);\")\n",
    "cur.execute(\"delete from authors *; copy authors from '\" + SQLtables_path + \"authors.csv' with (format csv);\")\n",
    "cur.execute(\"delete from authors_of *; copy authors_of from '\" + SQLtables_path + \"article_authors.csv' with (format csv);\")\n",
    "cur.execute(\"delete from owns *; copy owns from '\" + SQLtables_path + \"owns.csv' with (format csv);\")\n",
    "cur.execute(\"delete from meta_keywords *; copy meta_keywords from '\" + SQLtables_path + \"meta_keywords.csv' with (format csv);\")\n",
    "cur.execute(\"delete from domain *; copy domain from '\" + SQLtables_path + \"domains.csv' with (format csv);\")\n",
    "cur.execute(\"delete from article_meta_keywords *; copy article_meta_keywords from '\" + SQLtables_path + \"article_meta_keywords.csv' with (format csv);\")\n",
    "cur.execute(\"delete from types *; copy types from '\" + SQLtables_path + \"types.csv' with (format csv);\")\n",
    "cur.execute(\"delete from article_types *; copy article_types from '\" + SQLtables_path + \"article_types.csv' with (format csv);\")\n",
    "cur.execute(\"COMMIT TRANSACTION;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>380</td>\n",
       "      <td>the us geological survey usgs map tracks where...</td>\n",
       "      <td>rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>416</td>\n",
       "      <td>europhile mr tajani is one of three names mr b...</td>\n",
       "      <td>rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>life is an illusion at least on a quantum leve...</td>\n",
       "      <td>rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>355</td>\n",
       "      <td>the train was heading towards milan before it ...</td>\n",
       "      <td>rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>426</td>\n",
       "      <td>when will gomorrah season &lt;NUM&gt; be released th...</td>\n",
       "      <td>rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>506</td>\n",
       "      <td>christians pray together in beijing  world wat...</td>\n",
       "      <td>bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>203</td>\n",
       "      <td>for most of us the death of a child is our wor...</td>\n",
       "      <td>bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>361</td>\n",
       "      <td>theologian john piper  john piperfacebook  joh...</td>\n",
       "      <td>bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>744</td>\n",
       "      <td>the shocking lack of evidence supporting flu v...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>745</td>\n",
       "      <td>although all &lt;NUM&gt;&lt;NUM&gt; articles on &lt;URL&gt; are ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>936 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                            content   type\n",
       "0    380  the us geological survey usgs map tracks where...  rumor\n",
       "1    416  europhile mr tajani is one of three names mr b...  rumor\n",
       "2      0  life is an illusion at least on a quantum leve...  rumor\n",
       "3    355  the train was heading towards milan before it ...  rumor\n",
       "4    426  when will gomorrah season <NUM> be released th...  rumor\n",
       "..   ...                                                ...    ...\n",
       "931  506  christians pray together in beijing  world wat...   bias\n",
       "932  203  for most of us the death of a child is our wor...   bias\n",
       "933  361  theologian john piper  john piperfacebook  joh...   bias\n",
       "934  744  the shocking lack of evidence supporting flu v...   None\n",
       "935  745  although all <NUM><NUM> articles on <URL> are ...   None\n",
       "\n",
       "[936 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataframe\n",
    "cur.execute(\"\"\"select a.id, a.content, t.type\n",
    "            from article as a, article_types as at, types as t \n",
    "            where a.id = at.article_id and t.id = at.type_id\"\"\")\n",
    "SQL_articles = pd.DataFrame(cur.fetchall())\n",
    "SQL_articles.columns = ['id', 'content', 'type']\n",
    "print(type(SQL_articles))\n",
    "SQL_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>type</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>864</td>\n",
       "      <td>human rights voices anne bayefsky is the direc...</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>557</td>\n",
       "      <td>each year there are two incredible meteor show...</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>908</td>\n",
       "      <td>jayme evans jayme evans is a veteran of the un...</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>857</td>\n",
       "      <td>hal rounds hal rounds is a resident of tenness...</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>856</td>\n",
       "      <td>henry lamb most recent articles by henry lamb ...</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>268</td>\n",
       "      <td>two african american prolife leaders filed a l...</td>\n",
       "      <td>bias</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>442</td>\n",
       "      <td>victims and others look on as rachael denholla...</td>\n",
       "      <td>bias</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>506</td>\n",
       "      <td>christians pray together in beijing  world wat...</td>\n",
       "      <td>bias</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>203</td>\n",
       "      <td>for most of us the death of a child is our wor...</td>\n",
       "      <td>bias</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>361</td>\n",
       "      <td>theologian john piper  john piperfacebook  joh...</td>\n",
       "      <td>bias</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>728 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                            content        type label\n",
       "177  864  human rights voices anne bayefsky is the direc...  conspiracy  Fake\n",
       "178  557  each year there are two incredible meteor show...  conspiracy  Fake\n",
       "179  908  jayme evans jayme evans is a veteran of the un...  conspiracy  Fake\n",
       "180  857  hal rounds hal rounds is a resident of tenness...  conspiracy  Fake\n",
       "181  856  henry lamb most recent articles by henry lamb ...  conspiracy  Fake\n",
       "..   ...                                                ...         ...   ...\n",
       "929  268  two african american prolife leaders filed a l...        bias  Fake\n",
       "930  442  victims and others look on as rachael denholla...        bias  Fake\n",
       "931  506  christians pray together in beijing  world wat...        bias  Fake\n",
       "932  203  for most of us the death of a child is our wor...        bias  Fake\n",
       "933  361  theologian john piper  john piperfacebook  joh...        bias  Fake\n",
       "\n",
       "[728 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SQL_articles = SQL_articles[~SQL_articles['type'].isin(['unreliable','junksci','rumor', 'hate'])] #removes these types\n",
    "SQL_articles = SQL_articles.dropna(subset = ['type']) #drops where type is nan\n",
    "SQL_articles.loc[SQL_articles['type'].isin(['fake','satire','bias', 'conspiracy']), 'label'] = 'Fake' #labels types 'fake'\n",
    "SQL_articles.loc[SQL_articles['type'].isin(['reliable','political','clickbait']), 'label'] = 'True' #labels types 'true'\n",
    "SQL_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()\n",
    "content_tfidf = vect.fit_transform(SQL_articles['content'])\n",
    "content_tfidf_df = pd.DataFrame(content_tfidf.todense(),columns = vect.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(content_tfidf, SQL_articles['label'], test_size=0.40, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_nearest accuracy:0.7945205479452054\n"
     ]
    }
   ],
   "source": [
    "# Define the classifier classes\n",
    "k_nearest = KNeighborsClassifier(n_neighbors=15, weights='distance')\n",
    "\n",
    "# Fit the model\n",
    "k_nearest.fit(X_train,y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "k_nearest_pred = k_nearest.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"k_nearest accuracy:\" + str(accuracy_score(y_test,k_nearest_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_f accuracy:0.815068493150685\n"
     ]
    }
   ],
   "source": [
    "# Define the classifier classes\n",
    "random_f = RandomForestClassifier(max_depth=50)\n",
    "\n",
    "# Fit the model\n",
    "random_f.fit(X_train,y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "random_f_pred = random_f.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"random_f accuracy:\" + str(accuracy_score(y_test, random_f_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
