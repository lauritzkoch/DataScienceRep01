{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\krist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\krist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import porter\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2 as pc\n",
    "from IPython import display\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krist\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (0,1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "#Reading the data from 1mio-raw.csv\n",
    "dataTotal = pd.read_csv('250t-raw.csv')\n",
    "\n",
    "#We will only analyse a smaller part of the data set\n",
    "data = dataTotal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_database_login = \"dbname=datascience user=postgres password=2419\"\n",
    "SQLtables_path = \"/Users/krist/Desktop/Uni/milestone/DataScienceRep01/final_project/SQLtables/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleantext cleans the input string with the following functions: Characters are set to lowercase, \n",
    "#urls are substituted with <URL>, dates are substitured with <DATE>, emails are substitured with <EMAIL>\n",
    "#numbers are substitured with <NUM>, newlines and non-letter characters are removed.\n",
    "def cleantext(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<|>', \"\", text)\n",
    "    text = re.sub(r'(https?:\\/\\/)?w{0,3}\\.?[a-z]+\\.[a-z]\\w*[\\w\\/-]*', \"<URL>\", text)\n",
    "    text = re.sub(r'(jan\\.?(uary)?|feb\\.?(uary)?|mar\\.?(ch)?|apr\\.?(il)?|may|jun\\.(e)?|jul\\.(y)?|aug\\.?(ust)?|sep\\.?(tember)?|oct\\.?(ober)?|nov\\.?(ember)?|dec\\.?(ember)?|monday|tuesday|wednesday|thursday|friday|saturday|sunday) (the )?\\d{1,2}((th)?,?( \\d{4})?)?', \"<DATE>\", text)\n",
    "    text = re.sub(r'\\w+@\\w+\\.[a-zA-Z]{2,3}', \"<EMAIL>\", text)\n",
    "    text = re.sub(r'[0-9]+', \"<NUM>\", text)\n",
    "    text = re.sub(r'(\\\\n)+|\\s{2,}|(\\\\t+)', \" \", text)\n",
    "    text = re.sub(r'\\.|,|\\\\|-|\\?|\\(|\\)|\\||&|\"|”|“|:|!|\\+|-|–|—|\\/|\\$|%|€|#|;|\\[|\\]|©|®|…|=', \"\", text)\n",
    "    return text\n",
    "\n",
    "#cleanMetaKeywords cleans the input string with the following functions: \n",
    "#Characters are set to lowercase, newlines and non-letter characters are removed.\n",
    "def cleanMetaKeywords(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(\\\\n)+|\\s{2,}|(\\\\t+)', \" \", text)\n",
    "    text = re.sub(r'\\.|\\\\|-|\\?|\\(|\\)|\\||&|\"|”|“|:|!|\\+|-|\\'|–|—|\\/|\\$|%|€|#|;|\\[|\\]|©|®|…|=|<|>', \"\", text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def stopword(word_list):\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    return [word for word in word_list if word not in stop_words]\n",
    "\n",
    "def stemming(word_list):\n",
    "    stemmer = porter.PorterStemmer()\n",
    "\n",
    "    return [stemmer.stem(word) for word in word_list]\n",
    "\n",
    "def getSoup(url):\n",
    "    response = requests.get(url)\n",
    "    contents = response.content\n",
    "    return BeautifulSoup(contents, 'html.parser')\n",
    "\n",
    "def executeSQL(filename, cur):\n",
    "    fd = open(filename, 'r')\n",
    "    sqlFile = fd.read()\n",
    "    fd.close()\n",
    "    sqlCommands = sqlFile.split(';')\n",
    "    for command in sqlCommands:\n",
    "            cur.execute(command)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping data from Politics and Conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_nr = 1\n",
    "article_start_letters = \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr%23:group_nr%23+10]\n",
    "print(article_start_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The algorithm stops finding new articles when 'stop_searching' is set to True\n",
    "stop_searching = False\n",
    "\n",
    "#Finding the nextpage link in the first iteration is a little different, and therefore this value is needed\n",
    "first_iteration = True\n",
    "\n",
    "#The root url is the domain of wikinews\n",
    "root_link = 'https://en.wikinews.org'\n",
    "\n",
    "#next_page is the webpage that the algorithm searches for articles in next iteration of the while-loop\n",
    "next_page = root_link + '/w/index.php?title=Category:Politics_and_conflicts'\n",
    "\n",
    "#The links to the articles starting with the 'article_start_letters' are appended to 'article links'\n",
    "article_links = []\n",
    "\n",
    "#For each iteration this list gets some values if the first letter \n",
    "#of the first article in the next webpage is between A and K\n",
    "first_letter_between_B_K = []\n",
    "\n",
    "#A regex used for 'first_letter_between_B_K'\n",
    "continue_iterations = re.compile(r\"pagefrom=[A-K]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not(stop_searching):\n",
    "    soup = getSoup(next_page)\n",
    "    articles = soup.find(id=\"mw-pages\")\n",
    "    \n",
    "    links = [link.get(\"href\") for link in articles.find_all('a')]\n",
    "    \n",
    "    if first_iteration:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[0])\n",
    "        first_iteration = False\n",
    "        next_page = root_link + links[0]\n",
    "        article_links += [root_link + group_link for group_link in links[1:] if group_link[6] in article_start_letters]\n",
    "    else:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[1])\n",
    "        next_page = root_link + links[1]\n",
    "        article_links += [root_link + group_link for group_link in links[2:] if group_link[6] in article_start_letters]\n",
    "    \n",
    "    if len(first_letter_between_B_K) == 0:\n",
    "        stop_searching = True\n",
    "    \n",
    "    first_letter_between_B_K = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_source_code = [getSoup(article) for article in article_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These id's has to be different from the other articles\n",
    "article_id = range(len(data),len(data)+len(article_links))\n",
    "article_titles = [article.find('h1').get_text() for article in article_source_code]\n",
    "article_release_date = [str(article.find(id=\"publishDate\"))[50:60] for article in article_source_code]\n",
    "article_urls = article_links\n",
    "article_content = [\" \".join([p.get_text() for p in (article.find(id=\"mw-content-text\")).find_all('p')]) for article in article_source_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_articles = pd.DataFrame()\n",
    "\n",
    "scraped_articles['id'] = article_id\n",
    "scraped_articles['content'] = [cleantext(content) for content in article_content]\n",
    "scraped_articles['title'] = article_titles\n",
    "scraped_articles['release_date'] = article_release_date\n",
    "scraped_articles['url'] = article_urls\n",
    "\n",
    "scraped_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_articles.to_csv(\"SQLtables/scraped_articles.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up a connection with the SQL server. Make sure that you write your own dbname, user and password as input\n",
    "conn = pc.connect(SQL_database_login)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executeSQL('SQLfiles/createTableScraped.sql', cur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data and creating SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicate articles and NaN content-values\n",
    "data = data.drop_duplicates(subset=\"content\")\n",
    "data = data.dropna(subset=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the content\n",
    "cleaned_content = [cleantext(article_content) for article_content in data['content']]\n",
    "\n",
    "#Cleaning meta keywords\n",
    "clean_meta_keywords = [cleanMetaKeywords(metakeyword) for metakeyword in data[\"meta_keywords\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making sure that each element of 'tags', 'authors' and 'meta_keywords' are stripped stings and converting them to arrays\n",
    "data[\"tags\"] = [[tag.strip() for tag in (str(i)).split(\",\")] for i in data[\"tags\"]]\n",
    "data[\"authors\"] = [[author.strip() for author in (str(i)).split(\",\")] for i in data[\"authors\"]]\n",
    "data[\"meta_keywords\"] = [[meta_keyword.strip() for meta_keyword in (str(i)).split(\",\")] for i in clean_meta_keywords]\n",
    "data[\"content\"] = cleaned_content\n",
    "data[\"id\"] = range(0,len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entity tables\n",
    "articles = data[['id','content','url','meta_description','title']]\n",
    "\n",
    "meta_keywords = pd.DataFrame((data[['meta_keywords']].explode('meta_keywords')).drop_duplicates(subset = 'meta_keywords'))\n",
    "meta_keywords = meta_keywords.rename(columns={0: 'meta_keywords'})\n",
    "meta_keywords[\"ide\"] = range(0,len(meta_keywords))\n",
    "article_meta_keywords = pd.merge(meta_keywords, data[['id','meta_keywords']].explode('meta_keywords'), on = \"meta_keywords\")[['id','ide']]\n",
    "article_meta_keywords = article_meta_keywords.rename(columns={'id': 'article_id', 'ide': 'meta_keyword_id'})\n",
    "\n",
    "domains = pd.DataFrame((data['domain']).drop_duplicates())\n",
    "domains = domains.rename(columns={0: 'domain'})\n",
    "domains[\"ide\"] = range(0,len(domains))\n",
    "owns = pd.merge(domains, data, on = \"domain\")[['id','ide']]\n",
    "owns = owns.rename(columns={'id': 'article_id', 'ide': 'domain_id'})\n",
    "\n",
    "authors = pd.DataFrame((data[['authors']].explode('authors')).drop_duplicates())\n",
    "authors = authors.rename(columns={0: 'authors'})\n",
    "authors[\"ide\"] = range(0,len(authors))\n",
    "article_authors = pd.merge(authors, data[['id','authors']].explode('authors'), on = \"authors\")[['id','ide']]\n",
    "article_authors = article_authors.rename(columns={'id': 'article_id', 'ide': 'author_id'})\n",
    "\n",
    "types = pd.DataFrame((data['type']).drop_duplicates())\n",
    "types = types.rename(columns={0: 'type'})\n",
    "types[\"ide\"] = range(0,len(types))\n",
    "article_types = pd.merge(types, data, on = \"type\")[['id','ide']]\n",
    "article_types = article_types.rename(columns={'id': 'article_id', 'ide': 'type_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entities to CSV\n",
    "articles.to_csv(\"SQLtables/articles.csv\",index=False,header=False)\n",
    "meta_keywords.to_csv(\"SQLtables/meta_keywords.csv\",index=False,header=False)\n",
    "authors.to_csv(\"SQLtables/authors.csv\",index=False,header=False)\n",
    "domains.to_csv(\"SQLtables/domains.csv\",index=False,header=False)\n",
    "types.to_csv(\"SQLtables/types.csv\",index=False,header=False)\n",
    "\n",
    "#Relations to CSV\n",
    "owns.to_csv(\"SQLtables/owns.csv\",index=False,header=False)\n",
    "article_authors.to_csv(\"SQLtables/article_authors.csv\",index=False,header=False)\n",
    "article_meta_keywords.to_csv(\"SQLtables/article_meta_keywords.csv\",index=False,header=False)\n",
    "article_types.to_csv(\"SQLtables/article_types.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#executeSQL('SQLfiles/createTables.sql', cur)\n",
    "#executeSQL('SQLfiles/setUpTables.sql', cur)\n",
    "cur.execute(\"BEGIN TRANSACTION;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.article(id integer, content text COLLATE pg_catalog.\\\"default\\\", url text COLLATE pg_catalog.\\\"default\\\", meta_description text COLLATE pg_catalog.\\\"default\\\", title text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.article_meta_keywords(article_id integer, meta_keyword_id integer) WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.authors(name text COLLATE pg_catalog.\\\"default\\\", id integer) WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.authors_of(article_id integer, author_id integer) WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.domain(name text COLLATE pg_catalog.\\\"default\\\", id integer) WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.meta_keywords(meta_keyword text COLLATE pg_catalog.\\\"default\\\", id integer) WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.owns(article_id integer, domain_id integer) WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.types(type text COLLATE pg_catalog.\\\"default\\\", id integer) WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.article_types(article_id integer, type_id integer) WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"COMMIT TRANSACTION;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"BEGIN TRANSACTION;\")\n",
    "cur.execute(\"delete from article *; copy article from '\" + SQLtables_path + \"articles.csv' with (format csv);\")\n",
    "cur.execute(\"delete from authors *; copy authors from '\" + SQLtables_path + \"authors.csv' with (format csv);\")\n",
    "cur.execute(\"delete from authors_of *; copy authors_of from '\" + SQLtables_path + \"article_authors.csv' with (format csv);\")\n",
    "cur.execute(\"delete from owns *; copy owns from '\" + SQLtables_path + \"owns.csv' with (format csv);\")\n",
    "cur.execute(\"delete from meta_keywords *; copy meta_keywords from '\" + SQLtables_path + \"meta_keywords.csv' with (format csv);\")\n",
    "cur.execute(\"delete from domain *; copy domain from '\" + SQLtables_path + \"domains.csv' with (format csv);\")\n",
    "cur.execute(\"delete from article_meta_keywords *; copy article_meta_keywords from '\" + SQLtables_path + \"article_meta_keywords.csv' with (format csv);\")\n",
    "cur.execute(\"delete from types *; copy types from '\" + SQLtables_path + \"types.csv' with (format csv);\")\n",
    "cur.execute(\"delete from article_types *; copy article_types from '\" + SQLtables_path + \"article_types.csv' with (format csv);\")\n",
    "cur.execute(\"COMMIT TRANSACTION;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data from database and creating REAL/FAKE labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>life is an illusion at least on a quantum leve...</td>\n",
       "      <td>rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the los angeles police department has been den...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the white house has decided to quietly withdra...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>if you don’t believe in fate here’s a story th...</td>\n",
       "      <td>satire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>lost words hidden words otters banks and books...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178218</th>\n",
       "      <td>32975</td>\n",
       "      <td>manhattan is a place where millions of people ...</td>\n",
       "      <td>political</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178219</th>\n",
       "      <td>32976</td>\n",
       "      <td>the main reason i hate the supreme court's ove...</td>\n",
       "      <td>political</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178220</th>\n",
       "      <td>32977</td>\n",
       "      <td>notable christian and unnotable quarterback ti...</td>\n",
       "      <td>political</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178221</th>\n",
       "      <td>32978</td>\n",
       "      <td>remember a year ago when the world was simpler...</td>\n",
       "      <td>political</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178222</th>\n",
       "      <td>33045</td>\n",
       "      <td>alves wants move to the premier league in the ...</td>\n",
       "      <td>rumor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178223 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                            content       type\n",
       "0           0  life is an illusion at least on a quantum leve...      rumor\n",
       "1           1  the los angeles police department has been den...       hate\n",
       "2           2  the white house has decided to quietly withdra...       hate\n",
       "3          10  if you don’t believe in fate here’s a story th...     satire\n",
       "4          11  lost words hidden words otters banks and books...       fake\n",
       "...       ...                                                ...        ...\n",
       "178218  32975  manhattan is a place where millions of people ...  political\n",
       "178219  32976  the main reason i hate the supreme court's ove...  political\n",
       "178220  32977  notable christian and unnotable quarterback ti...  political\n",
       "178221  32978  remember a year ago when the world was simpler...  political\n",
       "178222  33045  alves wants move to the premier league in the ...      rumor\n",
       "\n",
       "[178223 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataframe\n",
    "cur.execute(\"\"\"select a.id, a.content, t.type\n",
    "            from article as a, article_types as at, types as t \n",
    "            where a.id = at.article_id and t.id = at.type_id\"\"\")\n",
    "SQL_articles1 = pd.DataFrame(cur.fetchall())\n",
    "SQL_articles1.columns = ['id', 'content', 'type']\n",
    "SQL_articles1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137393\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>type</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135667</th>\n",
       "      <td>135238</td>\n",
       "      <td>google billionaires james cameron backing spac...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118417</th>\n",
       "      <td>118387</td>\n",
       "      <td>first just when american's who can't pay atten...</td>\n",
       "      <td>political</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129373</th>\n",
       "      <td>129157</td>\n",
       "      <td>gospel of thomas being spiritually reborn head...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>1130</td>\n",
       "      <td>world world news and events china the middle e...</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164460</th>\n",
       "      <td>164461</td>\n",
       "      <td>for more news visit ☛ &lt;URL&gt;com follow us on tw...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66637</th>\n",
       "      <td>65972</td>\n",
       "      <td>christopher matthews reports for time &lt;DATE&gt; t...</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131866</th>\n",
       "      <td>131545</td>\n",
       "      <td>permanent makeup leaves ladies looking boston ...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80347</th>\n",
       "      <td>79963</td>\n",
       "      <td>many articles concerned with the energy crisis...</td>\n",
       "      <td>political</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117406</th>\n",
       "      <td>117359</td>\n",
       "      <td>yes our little texan has returned home  no sma...</td>\n",
       "      <td>political</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42427</th>\n",
       "      <td>40342</td>\n",
       "      <td>ah yes the learned sen shelby ooops sen shelby...</td>\n",
       "      <td>political</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                            content        type  \\\n",
       "135667  135238  google billionaires james cameron backing spac...        fake   \n",
       "118417  118387  first just when american's who can't pay atten...   political   \n",
       "129373  129157  gospel of thomas being spiritually reborn head...        fake   \n",
       "1106      1130  world world news and events china the middle e...  conspiracy   \n",
       "164460  164461  for more news visit ☛ <URL>com follow us on tw...        fake   \n",
       "...        ...                                                ...         ...   \n",
       "66637    65972  christopher matthews reports for time <DATE> t...  conspiracy   \n",
       "131866  131545  permanent makeup leaves ladies looking boston ...        fake   \n",
       "80347    79963  many articles concerned with the energy crisis...   political   \n",
       "117406  117359  yes our little texan has returned home  no sma...   political   \n",
       "42427    40342  ah yes the learned sen shelby ooops sen shelby...   political   \n",
       "\n",
       "       label  \n",
       "135667  Fake  \n",
       "118417  True  \n",
       "129373  Fake  \n",
       "1106    Fake  \n",
       "164460  Fake  \n",
       "...      ...  \n",
       "66637   Fake  \n",
       "131866  Fake  \n",
       "80347   True  \n",
       "117406  True  \n",
       "42427   True  \n",
       "\n",
       "[22000 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SQL_articles = SQL_articles1\n",
    "SQL_articles = SQL_articles[~SQL_articles['type'].isin(['unreliable','junksci','rumor', 'hate','unknown'])] #removes these types\n",
    "SQL_articles = SQL_articles.dropna(subset = ['type']) #drops where type is nan\n",
    "SQL_articles.loc[SQL_articles['type'].isin(['fake','satire','bias', 'conspiracy']), 'label'] = 'Fake' #labels types 'fake'\n",
    "SQL_articles.loc[SQL_articles['type'].isin(['reliable','political','clickbait']), 'label'] = 'True' #labels types 'true'\n",
    "SQL_articles = SQL_articles.sample(n=22000, random_state= 22)\n",
    "SQL_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing a baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()\n",
    "content_tfidf = vect.fit_transform(SQL_articles['content'])\n",
    "content_tfidf_df = pd.DataFrame(content_tfidf.todense(),columns = vect.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(content_tfidf, SQL_articles['label'], test_size=0.40, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_nearest accuracy:0.6748863636363637\n"
     ]
    }
   ],
   "source": [
    "# Define the classifier classes\n",
    "k_nearest = KNeighborsClassifier(n_neighbors=15, weights='distance')\n",
    "\n",
    "# Fit the model\n",
    "k_nearest.fit(X_train,y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "k_nearest_pred = k_nearest.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"k_nearest accuracy:\" + str(accuracy_score(y_test,k_nearest_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_f accuracy:0.7785\n"
     ]
    }
   ],
   "source": [
    "# Define the classifier classes\n",
    "random_f = RandomForestClassifier(max_depth=50)\n",
    "\n",
    "# Fit the model\n",
    "random_f.fit(X_train,y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "random_f_pred = random_f.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"random_f accuracy:\" + str(accuracy_score(y_test, random_f_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake News predictor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer and model input\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "#Tokenize input\n",
    "tokenized = SQL_articles['content'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "model.eval()\n",
    "\n",
    "#Pad input so that all sequences are of the same size:\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "padded = padded[:,:256]\n",
    "\n",
    "# Tell embedding model to disregard pad tokens\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  model = model.cuda()\n",
    "  device = torch.device(\"cuda\")\n",
    "\n",
    "# Convert input to a pytorch tensor\n",
    "input = torch.tensor(np.array(padded), device=device)\n",
    "attention_mask = torch.tensor(attention_mask, device=device)\n",
    "\n",
    "# Embed sequences (processing in batches to avoid memory problems)\n",
    "batch_size= 200\n",
    "embeddings = []\n",
    "\n",
    "for start_index in range(0, input.shape[0], batch_size):\n",
    "  with torch.no_grad():\n",
    "    # Call embedding model\n",
    "    embedding = model(input[start_index:start_index+batch_size], \n",
    "                      attention_mask=attention_mask[start_index:start_index+batch_size])[0][:,0,:]\n",
    "    embeddings.append(embedding)\n",
    "embeddings = torch.cat(embeddings)   # concatenate all batch outputs back into one tensor\n",
    "\n",
    "# Move embeddings back to numpy\n",
    "embeddings = embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hidden_layer_sizes is: (15, 15, 15, 15, 15, 15)\n",
      "[[0.77733333]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, SQL_articles['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [3,4]\n",
    "layer_sizes = [5,10,15]\n",
    "tuple_list = []\n",
    "\n",
    "for layer_size in layer_sizes:\n",
    "    for layer in layers:\n",
    "        tuple_list.append((layer_size,)*layer)\n",
    "        \n",
    "inputs = {'hidden_layer_sizes': tuple_list}\n",
    "\n",
    "# Define the classifier classes\n",
    "MLP = MLPClassifier()\n",
    "\n",
    "#Gridsearch\n",
    "cross_val = GridSearchCV(MLP, inputs)\n",
    "\n",
    "# Fit the model\n",
    "cross_val.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cross_val.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = np.zeros((len(layer_sizes),len(layers)))\n",
    "mts = df['mean_test_score']\n",
    "best_parameters = df['param_hidden_layer_sizes'][np.argmax(np.array(mts))]\n",
    "print(\"The best hidden_layer_sizes is: \" + str(best_parameters))\n",
    "\n",
    "for i in range(len(layer_sizes)):\n",
    "    for j in range(len(layers)):\n",
    "        hidden_layers[i,j] = mts[j+i*len(layers)]\n",
    "        \n",
    "\n",
    "sns.heatmap(hidden_layers, vmin = 0.0, vmax = 1.0)\n",
    "print(hidden_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['True' 'True' 'Fake' ... 'Fake' 'True' 'True']\n",
      "         id label\n",
      "0      8476  True\n",
      "1     10294  True\n",
      "2      3608  Fake\n",
      "3     10142  True\n",
      "4       875  True\n",
      "...     ...   ...\n",
      "6330   4490  True\n",
      "6331   8062  True\n",
      "6332   8622  Fake\n",
      "6333   4021  True\n",
      "6334   4330  True\n",
      "\n",
      "[6335 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "KAGGLE_df = pd.read_json(r'\\Users\\krist\\Desktop\\Uni\\milestone\\DataScienceRep01\\final_project\\test_set.json')\n",
    "#Cleaning the kaggle_content\n",
    "KAGGLE_df['article'] = [cleantext(article_content) for article_content in KAGGLE_df['article']]  \n",
    "\n",
    "\n",
    "content_tfidf = vect.transform(KAGGLE_df['article'])\n",
    "\n",
    "#Following line is the actual prediction based on the previously trained model 'cross_val'\n",
    "KAGGLE_df_prediction = cross_val.predict(content_tfidf)\n",
    "\n",
    "print((KAGGLE_df_prediction))\n",
    "\n",
    "to_kaggle_df = KAGGLE_df[['id','article']]\n",
    "to_kaggle_df['article'] = KAGGLE_df_prediction\n",
    "to_kaggle_df = to_kaggle_df.rename(columns={'article':'label'})\n",
    "to_kaggle_df.loc[to_kaggle_df['label'].isin([1]), 'label'] = 'REAL' #labels types 'fake'\n",
    "to_kaggle_df.loc[to_kaggle_df['label'].isin([0]), 'label'] = 'FAKE' #labels types 'true'\n",
    "#articles = articles.rename(columns={0:'id',1:'content',2:'type'})\n",
    "print(to_kaggle_df)\n",
    "to_kaggle_df.to_csv(\"Kaggle_prediction.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
