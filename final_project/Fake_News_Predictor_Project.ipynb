{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/danielstephensen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danielstephensen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import porter\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2 as pc\n",
    "from IPython import display\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data from 1mio-raw.csv\n",
    "dataTotal = pd.read_csv('250t-raw.csv')\n",
    "\n",
    "#We will only analyse a smaller part of the data set\n",
    "data = dataTotal[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_database_login = \"dbname=datascience user=postgres password=****\"\n",
    "SQLtables_path = \"/Users/krist/Desktop/Uni/milestone/DataScienceRep01/SQLtables/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleantext cleans the input string with the following functions: Characters are set to lowercase, \n",
    "#urls are substituted with <URL>, dates are substitured with <DATE>, emails are substitured with <EMAIL>\n",
    "#numbers are substitured with <NUM>, newlines and non-letter characters are removed.\n",
    "def cleantext(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<|>', \"\", text)\n",
    "    text = re.sub(r'(https?:\\/\\/)?w{0,3}\\.?[a-z]+\\.[a-z]\\w*[\\w\\/-]*', \"<URL>\", text)\n",
    "    text = re.sub(r'(jan\\.?(uary)?|feb\\.?(uary)?|mar\\.?(ch)?|apr\\.?(il)?|may|jun\\.(e)?|jul\\.(y)?|aug\\.?(ust)?|sep\\.?(tember)?|oct\\.?(ober)?|nov\\.?(ember)?|dec\\.?(ember)?|monday|tuesday|wednesday|thursday|friday|saturday|sunday) (the )?\\d{1,2}((th)?,?( \\d{4})?)?', \"<DATE>\", text)\n",
    "    text = re.sub(r'\\w+@\\w+\\.[a-zA-Z]{2,3}', \"<EMAIL>\", text)\n",
    "    text = re.sub(r'[0-9]+', \"<NUM>\", text)\n",
    "    text = re.sub(r'(\\\\n)+|\\s{2,}|(\\\\t+)', \" \", text)\n",
    "    text = re.sub(r'\\.|,|\\\\|-|\\?|\\(|\\)|\\||&|\"|”|“|:|!|\\+|-|–|—|\\/|\\$|%|€|#|;|\\[|\\]|©|®|…|=', \"\", text)\n",
    "    return text\n",
    "\n",
    "#cleanMetaKeywords cleans the input string with the following functions: \n",
    "#Characters are set to lowercase, newlines and non-letter characters are removed.\n",
    "def cleanMetaKeywords(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(\\\\n)+|\\s{2,}|(\\\\t+)', \" \", text)\n",
    "    text = re.sub(r'\\.|\\\\|-|\\?|\\(|\\)|\\||&|\"|”|“|:|!|\\+|-|\\'|–|—|\\/|\\$|%|€|#|;|\\[|\\]|©|®|…|=|<|>', \"\", text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def stopword(word_list):\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    return [word for word in word_list if word not in stop_words]\n",
    "\n",
    "def stemming(word_list):\n",
    "    stemmer = porter.PorterStemmer()\n",
    "\n",
    "    return [stemmer.stem(word) for word in word_list]\n",
    "\n",
    "def getSoup(url):\n",
    "    response = requests.get(url)\n",
    "    contents = response.content\n",
    "    return BeautifulSoup(contents, 'html.parser')\n",
    "\n",
    "def executeSQL(filename, cur):\n",
    "    fd = open(filename, 'r')\n",
    "    sqlFile = fd.read()\n",
    "    fd.close()\n",
    "    sqlCommands = sqlFile.split(';')\n",
    "    for command in sqlCommands:\n",
    "            cur.execute(command)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping data from Politics and Conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCDEFGHIJK\n"
     ]
    }
   ],
   "source": [
    "group_nr = 1\n",
    "article_start_letters = \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr%23:group_nr%23+10]\n",
    "print(article_start_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The algorithm stops finding new articles when 'stop_searching' is set to True\n",
    "stop_searching = False\n",
    "\n",
    "#Finding the nextpage link in the first iteration is a little different, and therefore this value is needed\n",
    "first_iteration = True\n",
    "\n",
    "#The root url is the domain of wikinews\n",
    "root_link = 'https://en.wikinews.org'\n",
    "\n",
    "#next_page is the webpage that the algorithm searches for articles in next iteration of the while-loop\n",
    "next_page = root_link + '/w/index.php?title=Category:Politics_and_conflicts'\n",
    "\n",
    "#The links to the articles starting with the 'article_start_letters' are appended to 'article links'\n",
    "article_links = []\n",
    "\n",
    "#For each iteration this list gets some values if the first letter \n",
    "#of the first article in the next webpage is between A and K\n",
    "first_letter_between_B_K = []\n",
    "\n",
    "#A regex used for 'first_letter_between_B_K'\n",
    "continue_iterations = re.compile(r\"pagefrom=[A-K]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not(stop_searching):\n",
    "    soup = getSoup(next_page)\n",
    "    articles = soup.find(id=\"mw-pages\")\n",
    "    \n",
    "    links = [link.get(\"href\") for link in articles.find_all('a')]\n",
    "    \n",
    "    if first_iteration:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[0])\n",
    "        first_iteration = False\n",
    "        next_page = root_link + links[0]\n",
    "        article_links += [root_link + group_link for group_link in links[1:] if group_link[6] in article_start_letters]\n",
    "    else:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[1])\n",
    "        next_page = root_link + links[1]\n",
    "        article_links += [root_link + group_link for group_link in links[2:] if group_link[6] in article_start_letters]\n",
    "    \n",
    "    if len(first_letter_between_B_K) == 0:\n",
    "        stop_searching = True\n",
    "    \n",
    "    first_letter_between_B_K = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_source_code = [getSoup(article) for article in article_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These id's has to be different from the other articles\n",
    "article_id = range(len(data),len(data)+len(article_links))\n",
    "article_titles = [article.find('h1').get_text() for article in article_source_code]\n",
    "article_release_date = [str(article.find(id=\"publishDate\"))[50:60] for article in article_source_code]\n",
    "article_urls = article_links\n",
    "article_content = [\" \".join([p.get_text() for p in (article.find(id=\"mw-content-text\")).find_all('p')]) for article in article_source_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i agree with brs that categorypolitical activi...</td>\n",
       "      <td>Category talk:Activists</td>\n",
       "      <td></td>\n",
       "      <td>https://en.wikinews.org/wiki/Category_talk:Act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;DATE&gt; in british columbia canada leadership d...</td>\n",
       "      <td>B.C. elections debate fiery but not conclusive</td>\n",
       "      <td></td>\n",
       "      <td>https://en.wikinews.org/wiki/B.C._elections_de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>wednesday &lt;DATE&gt; a suicide car bomb exploded y...</td>\n",
       "      <td>Baghdad bombing kills several people, scores i...</td>\n",
       "      <td>2010-01-27</td>\n",
       "      <td>https://en.wikinews.org/wiki/Baghdad_bombing_k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sunday &lt;DATE&gt; a judge in baghdad iraq has clea...</td>\n",
       "      <td>Baghdad judge clears pair of murdering six for...</td>\n",
       "      <td>2010-10-10</td>\n",
       "      <td>https://en.wikinews.org/wiki/Baghdad_judge_cle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>thursday &lt;DATE&gt; the bodies of over &lt;NUM&gt;&lt;NUM&gt; ...</td>\n",
       "      <td>Baghdad morgue received over 1,000 bodies in July</td>\n",
       "      <td>2005-08-18</td>\n",
       "      <td>https://en.wikinews.org/wiki/Baghdad_morgue_re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2855</th>\n",
       "      <td>2855</td>\n",
       "      <td>wednesday &lt;DATE&gt; kyrgyz authorities declared &lt;...</td>\n",
       "      <td>Kyrgyz government declares elections valid, re...</td>\n",
       "      <td>2005-03-23</td>\n",
       "      <td>https://en.wikinews.org/wiki/Kyrgyz_government...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>2856</td>\n",
       "      <td>tuesday &lt;DATE&gt; thousands of protesters seized ...</td>\n",
       "      <td>Kyrgyz president orders election probe as prot...</td>\n",
       "      <td>2005-03-22</td>\n",
       "      <td>https://en.wikinews.org/wiki/Kyrgyz_president_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2857</th>\n",
       "      <td>2857</td>\n",
       "      <td>sunday june &lt;NUM&gt; &lt;NUM&gt; kyrgyzstani citizens o...</td>\n",
       "      <td>Kyrgyzstan votes on referendum for new constit...</td>\n",
       "      <td>2010-06-27</td>\n",
       "      <td>https://en.wikinews.org/wiki/Kyrgyzstan_votes_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2858</th>\n",
       "      <td>2858</td>\n",
       "      <td>saturday june &lt;NUM&gt; &lt;NUM&gt; a second day of ethn...</td>\n",
       "      <td>Kyrgyzstan: Ethnic unrest continues, governmen...</td>\n",
       "      <td>2010-06-12</td>\n",
       "      <td>https://en.wikinews.org/wiki/Kyrgyzstan:_Ethni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2859</th>\n",
       "      <td>2859</td>\n",
       "      <td>monday june &lt;NUM&gt; &lt;NUM&gt; according to a spokesm...</td>\n",
       "      <td>Kyrgyzstan: Violence continues, death toll rises</td>\n",
       "      <td>2010-06-14</td>\n",
       "      <td>https://en.wikinews.org/wiki/Kyrgyzstan:_Viole...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2860 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            content  \\\n",
       "0        0  i agree with brs that categorypolitical activi...   \n",
       "1        1  <DATE> in british columbia canada leadership d...   \n",
       "2        2  wednesday <DATE> a suicide car bomb exploded y...   \n",
       "3        3  sunday <DATE> a judge in baghdad iraq has clea...   \n",
       "4        4  thursday <DATE> the bodies of over <NUM><NUM> ...   \n",
       "...    ...                                                ...   \n",
       "2855  2855  wednesday <DATE> kyrgyz authorities declared <...   \n",
       "2856  2856  tuesday <DATE> thousands of protesters seized ...   \n",
       "2857  2857  sunday june <NUM> <NUM> kyrgyzstani citizens o...   \n",
       "2858  2858  saturday june <NUM> <NUM> a second day of ethn...   \n",
       "2859  2859  monday june <NUM> <NUM> according to a spokesm...   \n",
       "\n",
       "                                                  title release_date  \\\n",
       "0                               Category talk:Activists                \n",
       "1        B.C. elections debate fiery but not conclusive                \n",
       "2     Baghdad bombing kills several people, scores i...   2010-01-27   \n",
       "3     Baghdad judge clears pair of murdering six for...   2010-10-10   \n",
       "4     Baghdad morgue received over 1,000 bodies in July   2005-08-18   \n",
       "...                                                 ...          ...   \n",
       "2855  Kyrgyz government declares elections valid, re...   2005-03-23   \n",
       "2856  Kyrgyz president orders election probe as prot...   2005-03-22   \n",
       "2857  Kyrgyzstan votes on referendum for new constit...   2010-06-27   \n",
       "2858  Kyrgyzstan: Ethnic unrest continues, governmen...   2010-06-12   \n",
       "2859   Kyrgyzstan: Violence continues, death toll rises   2010-06-14   \n",
       "\n",
       "                                                    url  \n",
       "0     https://en.wikinews.org/wiki/Category_talk:Act...  \n",
       "1     https://en.wikinews.org/wiki/B.C._elections_de...  \n",
       "2     https://en.wikinews.org/wiki/Baghdad_bombing_k...  \n",
       "3     https://en.wikinews.org/wiki/Baghdad_judge_cle...  \n",
       "4     https://en.wikinews.org/wiki/Baghdad_morgue_re...  \n",
       "...                                                 ...  \n",
       "2855  https://en.wikinews.org/wiki/Kyrgyz_government...  \n",
       "2856  https://en.wikinews.org/wiki/Kyrgyz_president_...  \n",
       "2857  https://en.wikinews.org/wiki/Kyrgyzstan_votes_...  \n",
       "2858  https://en.wikinews.org/wiki/Kyrgyzstan:_Ethni...  \n",
       "2859  https://en.wikinews.org/wiki/Kyrgyzstan:_Viole...  \n",
       "\n",
       "[2860 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_articles = pd.DataFrame()\n",
    "\n",
    "scraped_articles['id'] = article_id\n",
    "scraped_articles['content'] = [cleantext(content) for content in article_content]\n",
    "scraped_articles['title'] = article_titles\n",
    "scraped_articles['release_date'] = article_release_date\n",
    "scraped_articles['url'] = article_urls\n",
    "\n",
    "scraped_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_articles.to_csv(\"SQLtables/scraped_articles.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up a connection with the SQL server. Make sure that you write your own dbname, user and password as input\n",
    "conn = pc.connect(SQL_database_login)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executeSQL('SQLfiles/createTableScraped.sql', cur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data[\"title\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicate articles\n",
    "data = data.drop_duplicates(subset=\"content\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the content\n",
    "cleaned_content = [cleantext(article_content) for article_content in data['content']]\n",
    "\n",
    "#Tokenizing the cleaned data\n",
    "tokens = [tokenize(clean_text) for clean_text in cleaned_content]\n",
    "\n",
    "#Removing stopwords\n",
    "stopwords = [stopword(token_list) for token_list in tokens]\n",
    "\n",
    "#Stemming the data (this is used for the 'keywords' attribute)\n",
    "stemmed_data = [stemming(stopword_list) for stopword_list in stopwords]\n",
    "\n",
    "#Cleaning meta keywords\n",
    "clean_meta_keywords = [cleanMetaKeywords(metakeyword) for metakeyword in data[\"meta_keywords\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making sure that each element of 'tags', 'authors' and 'meta_keywords' are stripped stings and converting them to arrays\n",
    "data[\"tags\"] = [[tag.strip() for tag in (str(i)).split(\",\")] for i in data[\"tags\"]]\n",
    "data[\"authors\"] = [[author.strip() for author in (str(i)).split(\",\")] for i in data[\"authors\"]]\n",
    "data[\"meta_keywords\"] = [[meta_keyword.strip() for meta_keyword in (str(i)).split(\",\")] for i in clean_meta_keywords]\n",
    "data[\"content\"] = cleaned_content\n",
    "data[\"id\"] = range(0,len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entity tables\n",
    "articles = data[['id','content','url','meta_description','title']]\n",
    "\n",
    "meta_keywords = pd.DataFrame(set(data[['meta_keywords']].explode('meta_keywords')))\n",
    "meta_keywords = meta_keywords.rename(columns={0: 'meta_keywords'})\n",
    "meta_keywords[\"ide\"] = range(0,len(meta_keywords))\n",
    "article_meta_keywords = pd.merge(meta_keywords, data[['id','meta_keywords']].explode('meta_keywords'), on = \"meta_keywords\")[['id','ide']]\n",
    "article_meta_keywords = owns.rename(columns={'id': 'article_id', 'ide': 'meta_keyword_id'})\n",
    "\n",
    "\n",
    "domains = pd.DataFrame(set(data['domain']))\n",
    "domains = domains.rename(columns={0: 'domain'})\n",
    "domains[\"ide\"] = range(0,len(domains))\n",
    "owns = pd.merge(domains, data, on = \"domain\")[['id','ide']]\n",
    "owns = owns.rename(columns={'id': 'article_id', 'ide': 'domain_id'})\n",
    "\n",
    "authors = pd.DataFrame(set(data[['authors']].explode('authors')))\n",
    "authors = authors.rename(columns={0: 'authors'})\n",
    "authors[\"ide\"] = range(0,len(authors))\n",
    "article_authors = pd.merge(authors, data[['id','authors']].explode('authors'), on = \"authors\")[['id','ide']]\n",
    "article_authors = owns.rename(columns={'id': 'article_id', 'ide': 'author_id'})\n",
    "\n",
    "types = pd.DataFrame(set(data['type']))\n",
    "types = types.rename(columns={0: 'type'})\n",
    "types[\"ide\"] = range(0,len(types))\n",
    "article_types = pd.merge(types, data, on = \"type\")[['id','ide']]\n",
    "article_types = owns.rename(columns={'id': 'article_id', 'ide': 'type_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'SQLtables/articles.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-04fd398a4717>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Entities to CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0marticles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SQLtables/articles.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmeta_keywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SQLtables/meta_keywords.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mauthors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SQLtables/authors.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdomains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SQLtables/domains.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m         )\n\u001b[0;32m-> 3204\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             f, handles = get_handle(\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'SQLtables/articles.csv'"
     ]
    }
   ],
   "source": [
    "#Entities to CSV\n",
    "articles.to_csv(\"SQLtables/articles.csv\",index=False,header=False)\n",
    "meta_keywords.to_csv(\"SQLtables/meta_keywords.csv\",index=False,header=False)\n",
    "authors.to_csv(\"SQLtables/authors.csv\",index=False,header=False)\n",
    "domains.to_csv(\"SQLtables/domains.csv\",index=False,header=False)\n",
    "types.to_csv(\"SQLtables/types.csv\",index=False,header=False)\n",
    "\n",
    "#Relations to CSV\n",
    "owns.to_csv(\"SQLtables/owns.csv\",index=False,header=False)\n",
    "article_authors.to_csv(\"SQLtables/article_authors.csv\",index=False,header=False)\n",
    "article_meta_keywords.to_csv(\"SQLtables/article_meta_keywords.csv\",index=False,header=False)\n",
    "article_types.to_csv(\"SQLtables/article_types.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executeSQL('SQLfiles/createTables.sql', cur)\n",
    "executeSQL('SQLfiles/setUpTables.sql', cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>blackgenocide.org</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>http://blackgenocide.org/speaking_request.html</td>\n",
       "      <td>speaking engagement request contact person na...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Request Speaking Engagement</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>blackgenocide.org</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>http://blackgenocide.org/archived_articles/opp...</td>\n",
       "      <td>why we oppose planned parent hood  the followi...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Why We Oppose Planned Parenthood</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[planned parenthood, minorities, black culture...</td>\n",
       "      <td>A rationale for opposing the work of Planned P...</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>bipartisanreport.com</td>\n",
       "      <td>clickbait</td>\n",
       "      <td>http://bipartisanreport.com/cdn-cgi/l/email-pr...</td>\n",
       "      <td>the website from which you got to this page is...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Email Protection</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>blacklistednews.com</td>\n",
       "      <td>clickbait</td>\n",
       "      <td>https://www.blacklistednews.com/Egypt%E2%80%99...</td>\n",
       "      <td>egypt’s presidential campaign has kicked into ...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Egypt’s race for president kicks off with arre...</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>The Best in uncensored news, information, and ...</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>clickhole.com</td>\n",
       "      <td>satire</td>\n",
       "      <td>http://www.clickhole.com/article/serendipity-m...</td>\n",
       "      <td>if you don’t believe in fate here’s a story th...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Serendipity: This Man Made Up An Entire Person...</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[wow, love, relationships, coffee, beautiful, ...</td>\n",
       "      <td>If you don’t believe in fate, here’s a story t...</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>3883</td>\n",
       "      <td>931</td>\n",
       "      <td>awarenessact.com</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>http://awarenessact.com/remembering-apollo-1-n...</td>\n",
       "      <td>space exploration is an intriguing and necessa...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Remembering Apollo 1: NASA’s Careless Mistake ...</td>\n",
       "      <td>[Gerald Sinclair]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Nasa, apollo one, people, honest, lethal, sad...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>3887</td>\n",
       "      <td>932</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/blogging-citizen-jour...</td>\n",
       "      <td>fisa documents released  of readers think this...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>FISA Documents Released</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>3891</td>\n",
       "      <td>933</td>\n",
       "      <td>canadafreepress.com</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>http://canadafreepress.com/members/1/EliasBejj...</td>\n",
       "      <td>elias bejjani elias bejjani  chairman for the ...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Elias Bejjani</td>\n",
       "      <td>[Because Without America, There Is No Free Wor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>3898</td>\n",
       "      <td>934</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/blogging-citizen-jour...</td>\n",
       "      <td>note i do not necessarily endorse any products...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Fake “Memos” Being Released Into The Info Stre...</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>3900</td>\n",
       "      <td>935</td>\n",
       "      <td>canadafreepress.com</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>http://canadafreepress.com/members/1/EpochTime...</td>\n",
       "      <td>epoch times the epoch times a fresh look at ou...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Epoch Times</td>\n",
       "      <td>[Because Without America, There Is No Free Wor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>728 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0   id                domain        type  \\\n",
       "5           11    5     blackgenocide.org  conspiracy   \n",
       "6           13    6     blackgenocide.org  conspiracy   \n",
       "7           14    7  bipartisanreport.com   clickbait   \n",
       "9           23    9   blacklistednews.com   clickbait   \n",
       "10          26   10         clickhole.com      satire   \n",
       "..         ...  ...                   ...         ...   \n",
       "995       3883  931      awarenessact.com  conspiracy   \n",
       "996       3887  932     beforeitsnews.com        fake   \n",
       "997       3891  933   canadafreepress.com  conspiracy   \n",
       "998       3898  934     beforeitsnews.com        fake   \n",
       "999       3900  935   canadafreepress.com  conspiracy   \n",
       "\n",
       "                                                   url  \\\n",
       "5       http://blackgenocide.org/speaking_request.html   \n",
       "6    http://blackgenocide.org/archived_articles/opp...   \n",
       "7    http://bipartisanreport.com/cdn-cgi/l/email-pr...   \n",
       "9    https://www.blacklistednews.com/Egypt%E2%80%99...   \n",
       "10   http://www.clickhole.com/article/serendipity-m...   \n",
       "..                                                 ...   \n",
       "995  http://awarenessact.com/remembering-apollo-1-n...   \n",
       "996  http://beforeitsnews.com/blogging-citizen-jour...   \n",
       "997  http://canadafreepress.com/members/1/EliasBejj...   \n",
       "998  http://beforeitsnews.com/blogging-citizen-jour...   \n",
       "999  http://canadafreepress.com/members/1/EpochTime...   \n",
       "\n",
       "                                               content  \\\n",
       "5     speaking engagement request contact person na...   \n",
       "6    why we oppose planned parent hood  the followi...   \n",
       "7    the website from which you got to this page is...   \n",
       "9    egypt’s presidential campaign has kicked into ...   \n",
       "10   if you don’t believe in fate here’s a story th...   \n",
       "..                                                 ...   \n",
       "995  space exploration is an intriguing and necessa...   \n",
       "996  fisa documents released  of readers think this...   \n",
       "997  elias bejjani elias bejjani  chairman for the ...   \n",
       "998  note i do not necessarily endorse any products...   \n",
       "999  epoch times the epoch times a fresh look at ou...   \n",
       "\n",
       "                     scraped_at                 inserted_at  \\\n",
       "5    2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "6    2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "7    2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "9    2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "10   2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "..                          ...                         ...   \n",
       "995  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "996  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "997  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "998  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "999  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "\n",
       "                     updated_at  \\\n",
       "5    2018-02-02 01:19:41.756664   \n",
       "6    2018-02-02 01:19:41.756664   \n",
       "7    2018-02-02 01:19:41.756664   \n",
       "9    2018-02-02 01:19:41.756664   \n",
       "10   2018-02-02 01:19:41.756664   \n",
       "..                          ...   \n",
       "995  2018-02-02 01:19:41.756664   \n",
       "996  2018-02-02 01:19:41.756664   \n",
       "997  2018-02-02 01:19:41.756664   \n",
       "998  2018-02-02 01:19:41.756664   \n",
       "999  2018-02-02 01:19:41.756664   \n",
       "\n",
       "                                                 title  \\\n",
       "5                          Request Speaking Engagement   \n",
       "6                     Why We Oppose Planned Parenthood   \n",
       "7                                     Email Protection   \n",
       "9    Egypt’s race for president kicks off with arre...   \n",
       "10   Serendipity: This Man Made Up An Entire Person...   \n",
       "..                                                 ...   \n",
       "995  Remembering Apollo 1: NASA’s Careless Mistake ...   \n",
       "996                            FISA Documents Released   \n",
       "997                                      Elias Bejjani   \n",
       "998  Fake “Memos” Being Released Into The Info Stre...   \n",
       "999                                        Epoch Times   \n",
       "\n",
       "                                               authors  keywords  \\\n",
       "5                                                [nan]       NaN   \n",
       "6                                                [nan]       NaN   \n",
       "7                                                [nan]       NaN   \n",
       "9                                                [nan]       NaN   \n",
       "10                                               [nan]       NaN   \n",
       "..                                                 ...       ...   \n",
       "995                                  [Gerald Sinclair]       NaN   \n",
       "996                                              [nan]       NaN   \n",
       "997  [Because Without America, There Is No Free Wor...       NaN   \n",
       "998                                              [nan]       NaN   \n",
       "999  [Because Without America, There Is No Free Wor...       NaN   \n",
       "\n",
       "                                         meta_keywords  \\\n",
       "5                                                   []   \n",
       "6    [planned parenthood, minorities, black culture...   \n",
       "7                                                   []   \n",
       "9                                                   []   \n",
       "10   [wow, love, relationships, coffee, beautiful, ...   \n",
       "..                                                 ...   \n",
       "995                                                 []   \n",
       "996                                                 []   \n",
       "997                                                 []   \n",
       "998                                                 []   \n",
       "999                                                 []   \n",
       "\n",
       "                                      meta_description  \\\n",
       "5                                                  NaN   \n",
       "6    A rationale for opposing the work of Planned P...   \n",
       "7                                                  NaN   \n",
       "9    The Best in uncensored news, information, and ...   \n",
       "10   If you don’t believe in fate, here’s a story t...   \n",
       "..                                                 ...   \n",
       "995                                                NaN   \n",
       "996                                                NaN   \n",
       "997                                                NaN   \n",
       "998                                                NaN   \n",
       "999                                                NaN   \n",
       "\n",
       "                                                  tags  summary  source label  \n",
       "5                                                [nan]      NaN     NaN  Fake  \n",
       "6                                                [nan]      NaN     NaN  Fake  \n",
       "7                                                [nan]      NaN     NaN  True  \n",
       "9                                                [nan]      NaN     NaN  True  \n",
       "10                                               [nan]      NaN     NaN  Fake  \n",
       "..                                                 ...      ...     ...   ...  \n",
       "995  [Nasa, apollo one, people, honest, lethal, sad...      NaN     NaN  Fake  \n",
       "996                                              [nan]      NaN     NaN  Fake  \n",
       "997                                              [nan]      NaN     NaN  Fake  \n",
       "998                                              [nan]      NaN     NaN  Fake  \n",
       "999                                              [nan]      NaN     NaN  Fake  \n",
       "\n",
       "[728 rows x 18 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1 = data[~data['type'].isin(['unreliable','junksci','rumor', 'hate'])] #removes these types\n",
    "data_1 = data_1.dropna(subset = ['type']) #drops where type is nan\n",
    "data_1.loc[data_1['type'].isin(['fake','satire','bias', 'conspiracy']), 'label'] = 'Fake' #labels types 'fake'\n",
    "data_1.loc[data_1['type'].isin(['reliable','political','clickbait']), 'label'] = 'True' #labels types 'true'\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()\n",
    "content_tfidf = vect.fit_transform(scraped_articles['content'])\n",
    "content_tfidf_df = pd.DataFrame(content_tfidf.todense(),columns = vect.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-d4fb13839c1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(content_tfidf, articles['label'], test_size=0.40, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_nearest = KNeighborsClassifier(n_neighbors=15, weights='distance')\n",
    "\n",
    "# Fit the model\n",
    "k_nearest.fit(X_train,y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "k_nearest_pred = k_nearest.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"k_nearest accuracy:\" + str(accuracy_score(y_test,k_nearest_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
