{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/danielstephensen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danielstephensen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import porter\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2 as pc\n",
    "from IPython import display\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import seaborn as sns\n",
    "import transformers as ppb # pytorch transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3071: DtypeWarning: Columns (0,1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "#Reading the data from 1mio-raw.csv\n",
    "data = pd.read_csv('250t-raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_database_login = \"dbname=datascience user=postgres password=2419\"\n",
    "SQLtables_path = \"/Users/krist/Desktop/Uni/milestone/DataScienceRep01/final_project/SQLtables/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleantext cleans the input string with the following functions: Characters are set to lowercase, \n",
    "#urls are substituted with <URL>, dates are substitured with <DATE>, emails are substitured with <EMAIL>\n",
    "#numbers are substitured with <NUM>, newlines and non-letter characters are removed.\n",
    "def cleantext(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<|>', \"\", text)\n",
    "    text = re.sub(r'(https?:\\/\\/)?w{0,3}\\.?[a-z]+\\.[a-z]\\w*[\\w\\/-]*', \"<URL>\", text)\n",
    "    text = re.sub(r'(jan\\.?(uary)?|feb\\.?(uary)?|mar\\.?(ch)?|apr\\.?(il)?|may|jun\\.(e)?|jul\\.(y)?|aug\\.?(ust)?|sep\\.?(tember)?|oct\\.?(ober)?|nov\\.?(ember)?|dec\\.?(ember)?|monday|tuesday|wednesday|thursday|friday|saturday|sunday) (the )?\\d{1,2}((th)?,?( \\d{4})?)?', \"<DATE>\", text)\n",
    "    text = re.sub(r'\\w+@\\w+\\.[a-zA-Z]{2,3}', \"<EMAIL>\", text)\n",
    "    text = re.sub(r'[0-9]+', \"<NUM>\", text)\n",
    "    text = re.sub(r'(\\\\n)+|\\s{2,}|(\\\\t+)', \" \", text)\n",
    "    text = re.sub(r'\\.|,|\\\\|-|\\?|\\(|\\)|\\||&|\"|”|“|:|!|\\+|-|–|—|\\/|\\$|%|€|#|;|\\[|\\]|©|®|…|=', \"\", text)\n",
    "    return text\n",
    "\n",
    "#cleanMetaKeywords cleans the input string with the following functions: \n",
    "#Characters are set to lowercase, newlines and non-letter characters are removed.\n",
    "def cleanMetaKeywords(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(\\\\n)+|\\s{2,}|(\\\\t+)', \" \", text)\n",
    "    text = re.sub(r'\\.|\\\\|-|\\?|\\(|\\)|\\||&|\"|”|“|:|!|\\+|-|\\'|–|—|\\/|\\$|%|€|#|;|\\[|\\]|©|®|…|=|<|>', \"\", text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def stopword(word_list):\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    return [word for word in word_list if word not in stop_words]\n",
    "\n",
    "def stemming(word_list):\n",
    "    stemmer = porter.PorterStemmer()\n",
    "\n",
    "    return [stemmer.stem(word) for word in word_list]\n",
    "\n",
    "def getSoup(url):\n",
    "    response = requests.get(url)\n",
    "    contents = response.content\n",
    "    return BeautifulSoup(contents, 'html.parser')\n",
    "\n",
    "def executeSQL(filename, cur):\n",
    "    fd = open(filename, 'r')\n",
    "    sqlFile = fd.read()\n",
    "    fd.close()\n",
    "    sqlCommands = sqlFile.split(';')\n",
    "    for command in sqlCommands:\n",
    "            cur.execute(command)\n",
    "\n",
    "def distilBERT(content):\n",
    "    #Tokenizer and model input\n",
    "    pretrained_weights = 'distilbert-base-uncased'\n",
    "    tokenizer = ppb.DistilBertTokenizer.from_pretrained(pretrained_weights)\n",
    "    model = ppb.DistilBertModel.from_pretrained(pretrained_weights)\n",
    "\n",
    "    #Tokenize input\n",
    "    tokenized = content.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "    model.eval()\n",
    "\n",
    "    #Pad input so that all sequences are of the same size:\n",
    "    max_len = 0\n",
    "    for i in tokenized.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "    padded = padded[:,:32]\n",
    "\n",
    "    # Tell embedding model to disregard pad tokens\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "      model = model.cuda()\n",
    "      device = torch.device(\"cuda\")\n",
    "\n",
    "    # Convert input to a pytorch tensor\n",
    "    input = torch.tensor(np.array(padded), device=device)\n",
    "    attention_mask = torch.tensor(attention_mask, device=device)\n",
    "\n",
    "    # Embed sequences (processing in batches to avoid memory problems)\n",
    "    batch_size= 200\n",
    "    embeddings = []\n",
    "\n",
    "    for start_index in range(0, input.shape[0], batch_size):\n",
    "      with torch.no_grad():\n",
    "        # Call embedding model\n",
    "        embedding = model(input[start_index:start_index+batch_size], \n",
    "                          attention_mask=attention_mask[start_index:start_index+batch_size])[0][:,0,:]\n",
    "        embeddings.append(embedding)\n",
    "    embeddings = torch.cat(embeddings)   # concatenate all batch outputs back into one tensor\n",
    "\n",
    "    # Move embeddings back to numpy\n",
    "    embeddings = embeddings.cpu().numpy()\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping data from Politics and Conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_nr = 1\n",
    "article_start_letters = \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr%23:group_nr%23+10]\n",
    "print(article_start_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The algorithm stops finding new articles when 'stop_searching' is set to True\n",
    "stop_searching = False\n",
    "\n",
    "#Finding the nextpage link in the first iteration is a little different, and therefore this value is needed\n",
    "first_iteration = True\n",
    "\n",
    "#The root url is the domain of wikinews\n",
    "root_link = 'https://en.wikinews.org'\n",
    "\n",
    "#next_page is the webpage that the algorithm searches for articles in next iteration of the while-loop\n",
    "next_page = root_link + '/w/index.php?title=Category:Politics_and_conflicts'\n",
    "\n",
    "#The links to the articles starting with the 'article_start_letters' are appended to 'article links'\n",
    "article_links = []\n",
    "\n",
    "#For each iteration this list gets some values if the first letter \n",
    "#of the first article in the next webpage is between A and K\n",
    "first_letter_between_B_K = []\n",
    "\n",
    "#A regex used for 'first_letter_between_B_K'\n",
    "continue_iterations = re.compile(r\"pagefrom=[A-K]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not(stop_searching):\n",
    "    soup = getSoup(next_page)\n",
    "    articles = soup.find(id=\"mw-pages\")\n",
    "    \n",
    "    links = [link.get(\"href\") for link in articles.find_all('a')]\n",
    "    \n",
    "    if first_iteration:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[0])\n",
    "        first_iteration = False\n",
    "        next_page = root_link + links[0]\n",
    "        article_links += [root_link + group_link for group_link in links[1:] if group_link[6] in article_start_letters]\n",
    "    else:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[1])\n",
    "        next_page = root_link + links[1]\n",
    "        article_links += [root_link + group_link for group_link in links[2:] if group_link[6] in article_start_letters]\n",
    "    \n",
    "    if len(first_letter_between_B_K) == 0:\n",
    "        stop_searching = True\n",
    "    \n",
    "    first_letter_between_B_K = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_source_code = [getSoup(article) for article in article_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These id's has to be different from the other articles\n",
    "article_id = range(len(data),len(data)+len(article_links))\n",
    "article_titles = [article.find('h1').get_text() for article in article_source_code]\n",
    "article_release_date = [str(article.find(id=\"publishDate\"))[50:60] for article in article_source_code]\n",
    "article_urls = article_links\n",
    "article_content = [\" \".join([p.get_text() for p in (article.find(id=\"mw-content-text\")).find_all('p')]) for article in article_source_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_articles = pd.DataFrame()\n",
    "\n",
    "scraped_articles['id'] = article_id\n",
    "scraped_articles['content'] = [cleantext(content) for content in article_content]\n",
    "scraped_articles['title'] = article_titles\n",
    "scraped_articles['release_date'] = article_release_date\n",
    "scraped_articles['url'] = article_urls\n",
    "\n",
    "scraped_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_articles.to_csv(\"SQLtables/scraped_articles.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up a connection with the SQL server. Make sure that you write your own dbname, user and password as input\n",
    "conn = pc.connect(SQL_database_login)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executeSQL('SQLfiles/createTableScraped.sql', cur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data and creating SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicate articles and NaN content-values\n",
    "data = data.drop_duplicates(subset=\"content\")\n",
    "data = data.dropna(subset=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the content\n",
    "cleaned_content = [cleantext(article_content) for article_content in data['content']]\n",
    "\n",
    "#Cleaning meta keywords\n",
    "clean_meta_keywords = [cleanMetaKeywords(metakeyword) for metakeyword in data[\"meta_keywords\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making sure that each element of 'tags', 'authors' and 'meta_keywords' are stripped stings and converting them to arrays\n",
    "data[\"tags\"] = [[tag.strip() for tag in (str(i)).split(\",\")] for i in data[\"tags\"]]\n",
    "data[\"authors\"] = [[author.strip() for author in (str(i)).split(\",\")] for i in data[\"authors\"]]\n",
    "data[\"meta_keywords\"] = [[meta_keyword.strip() for meta_keyword in (str(i)).split(\",\")] for i in clean_meta_keywords]\n",
    "data[\"content\"] = cleaned_content\n",
    "data[\"id\"] = range(0,len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entity tables\n",
    "articles = data[['id','content','url','meta_description','title']]\n",
    "\n",
    "meta_keywords = pd.DataFrame((data[['meta_keywords']].explode('meta_keywords')).drop_duplicates(subset = 'meta_keywords'))\n",
    "meta_keywords = meta_keywords.rename(columns={0: 'meta_keywords'})\n",
    "meta_keywords[\"ide\"] = range(0,len(meta_keywords))\n",
    "article_meta_keywords = pd.merge(meta_keywords, data[['id','meta_keywords']].explode('meta_keywords'), on = \"meta_keywords\")[['id','ide']]\n",
    "article_meta_keywords = article_meta_keywords.rename(columns={'id': 'article_id', 'ide': 'meta_keyword_id'})\n",
    "\n",
    "domains = pd.DataFrame((data['domain']).drop_duplicates())\n",
    "domains = domains.rename(columns={0: 'domain'})\n",
    "domains[\"ide\"] = range(0,len(domains))\n",
    "owns = pd.merge(domains, data, on = \"domain\")[['id','ide']]\n",
    "owns = owns.rename(columns={'id': 'article_id', 'ide': 'domain_id'})\n",
    "\n",
    "authors = pd.DataFrame((data[['authors']].explode('authors')).drop_duplicates())\n",
    "authors = authors.rename(columns={0: 'authors'})\n",
    "authors[\"ide\"] = range(0,len(authors))\n",
    "article_authors = pd.merge(authors, data[['id','authors']].explode('authors'), on = \"authors\")[['id','ide']]\n",
    "article_authors = article_authors.rename(columns={'id': 'article_id', 'ide': 'author_id'})\n",
    "\n",
    "types = pd.DataFrame((data['type']).drop_duplicates())\n",
    "types = types.rename(columns={0: 'type'})\n",
    "types[\"ide\"] = range(0,len(types))\n",
    "article_types = pd.merge(types, data, on = \"type\")[['id','ide']]\n",
    "article_types = article_types.rename(columns={'id': 'article_id', 'ide': 'type_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entities to CSV\n",
    "articles.to_csv(\"SQLtables/articles.csv\",index=False,header=False)\n",
    "meta_keywords.to_csv(\"SQLtables/meta_keywords.csv\",index=False,header=False)\n",
    "authors.to_csv(\"SQLtables/authors.csv\",index=False,header=False)\n",
    "domains.to_csv(\"SQLtables/domains.csv\",index=False,header=False)\n",
    "types.to_csv(\"SQLtables/types.csv\",index=False,header=False)\n",
    "\n",
    "#Relations to CSV\n",
    "owns.to_csv(\"SQLtables/owns.csv\",index=False,header=False)\n",
    "article_authors.to_csv(\"SQLtables/article_authors.csv\",index=False,header=False)\n",
    "article_meta_keywords.to_csv(\"SQLtables/article_meta_keywords.csv\",index=False,header=False)\n",
    "article_types.to_csv(\"SQLtables/article_types.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "executeSQL('SQLfiles/createTables.sql', cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "executeSQL('SQLfiles/setUpTables.sql', cur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data from database and creating REAL/FAKE labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>life is an illusion at least on a quantum leve...</td>\n",
       "      <td>rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the los angeles police department has been den...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the white house has decided to quietly withdra...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>if you don’t believe in fate here’s a story th...</td>\n",
       "      <td>satire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>lost words hidden words otters banks and books...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178218</th>\n",
       "      <td>32975</td>\n",
       "      <td>manhattan is a place where millions of people ...</td>\n",
       "      <td>political</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178219</th>\n",
       "      <td>32976</td>\n",
       "      <td>the main reason i hate the supreme court's ove...</td>\n",
       "      <td>political</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178220</th>\n",
       "      <td>32977</td>\n",
       "      <td>notable christian and unnotable quarterback ti...</td>\n",
       "      <td>political</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178221</th>\n",
       "      <td>32978</td>\n",
       "      <td>remember a year ago when the world was simpler...</td>\n",
       "      <td>political</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178222</th>\n",
       "      <td>33045</td>\n",
       "      <td>alves wants move to the premier league in the ...</td>\n",
       "      <td>rumor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178223 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                            content       type\n",
       "0           0  life is an illusion at least on a quantum leve...      rumor\n",
       "1           1  the los angeles police department has been den...       hate\n",
       "2           2  the white house has decided to quietly withdra...       hate\n",
       "3          10  if you don’t believe in fate here’s a story th...     satire\n",
       "4          11  lost words hidden words otters banks and books...       fake\n",
       "...       ...                                                ...        ...\n",
       "178218  32975  manhattan is a place where millions of people ...  political\n",
       "178219  32976  the main reason i hate the supreme court's ove...  political\n",
       "178220  32977  notable christian and unnotable quarterback ti...  political\n",
       "178221  32978  remember a year ago when the world was simpler...  political\n",
       "178222  33045  alves wants move to the premier league in the ...      rumor\n",
       "\n",
       "[178223 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataframe\n",
    "cur.execute(\"\"\"select a.id, a.content, t.type\n",
    "            from article as a, article_types as at, types as t \n",
    "            where a.id = at.article_id and t.id = at.type_id\"\"\")\n",
    "SQL_articles = pd.DataFrame(cur.fetchall())\n",
    "SQL_articles.columns = ['id', 'content', 'type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>type</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>174792</th>\n",
       "      <td>135592</td>\n",
       "      <td>&lt;NUM&gt; to science and technology on monday &lt;DAT...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154018</th>\n",
       "      <td>118446</td>\n",
       "      <td>absolutely  i already do sure  i'll give it a ...</td>\n",
       "      <td>political</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167768</th>\n",
       "      <td>129344</td>\n",
       "      <td>indian prime minister’s gift to president fran...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>1126</td>\n",
       "      <td>subscribe to canada free press for free the me...</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217030</th>\n",
       "      <td>164422</td>\n",
       "      <td>msnbc's bashir again gives platform to columni...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179235</th>\n",
       "      <td>139288</td>\n",
       "      <td>the dmca digital millenium copyright act alrea...</td>\n",
       "      <td>political</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34684</th>\n",
       "      <td>30347</td>\n",
       "      <td>cutting social security are you prepared to de...</td>\n",
       "      <td>political</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89133</th>\n",
       "      <td>68921</td>\n",
       "      <td>i would have to say icke hands down as number ...</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53553</th>\n",
       "      <td>43765</td>\n",
       "      <td>reports out today indicate that magic johnson ...</td>\n",
       "      <td>political</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181536</th>\n",
       "      <td>141122</td>\n",
       "      <td>we learn from the evertrusty wikipedia that al...</td>\n",
       "      <td>political</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                            content        type  \\\n",
       "174792  135592  <NUM> to science and technology on monday <DAT...        fake   \n",
       "154018  118446  absolutely  i already do sure  i'll give it a ...   political   \n",
       "167768  129344  indian prime minister’s gift to president fran...        fake   \n",
       "1199      1126  subscribe to canada free press for free the me...  conspiracy   \n",
       "217030  164422  msnbc's bashir again gives platform to columni...        fake   \n",
       "...        ...                                                ...         ...   \n",
       "179235  139288  the dmca digital millenium copyright act alrea...   political   \n",
       "34684    30347  cutting social security are you prepared to de...   political   \n",
       "89133    68921  i would have to say icke hands down as number ...  conspiracy   \n",
       "53553    43765  reports out today indicate that magic johnson ...   political   \n",
       "181536  141122  we learn from the evertrusty wikipedia that al...   political   \n",
       "\n",
       "       label  \n",
       "174792  Fake  \n",
       "154018  True  \n",
       "167768  Fake  \n",
       "1199    Fake  \n",
       "217030  Fake  \n",
       "...      ...  \n",
       "179235  True  \n",
       "34684   True  \n",
       "89133   Fake  \n",
       "53553   True  \n",
       "181536  True  \n",
       "\n",
       "[40000 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SQL_articles = SQL_articles[~SQL_articles['type'].isin(['unreliable','junksci','rumor', 'hate','unknown'])] #removes these types\n",
    "SQL_articles = SQL_articles.dropna(subset = ['type']) #drops where type is nan\n",
    "SQL_articles.loc[SQL_articles['type'].isin(['fake','satire','bias', 'conspiracy']), 'label'] = 'Fake' #labels types 'fake'\n",
    "SQL_articles.loc[SQL_articles['type'].isin(['reliable','political','clickbait']), 'label'] = 'True' #labels types 'true'\n",
    "SQL_articles = SQL_articles.sample(n=40000, random_state= 22)\n",
    "SQL_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing a baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()\n",
    "content_tfidf = vect.fit_transform(SQL_articles['content'])\n",
    "content_tfidf_df = pd.DataFrame(content_tfidf.todense(),columns = vect.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(content_tfidf, SQL_articles['label'], test_size=0.40, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_nearest accuracy:0.7095625\n"
     ]
    }
   ],
   "source": [
    "# Define the classifier classes\n",
    "k_nearest = KNeighborsClassifier(n_neighbors=15, weights='distance')\n",
    "\n",
    "# Fit the model\n",
    "k_nearest.fit(X_train,y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "k_nearest_pred = k_nearest.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"k_nearest accuracy:\" + str(accuracy_score(y_test,k_nearest_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc accuracy:0.825\n"
     ]
    }
   ],
   "source": [
    "# Define the classifier classes\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "# Fit the model\n",
    "svc.fit(X_train,y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "svc_pred = svc.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"svc accuracy:\" + str(accuracy_score(y_test,svc_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake News Predictor - word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (921 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "embeddings = distilBERT(SQL_articles['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake News Predictor - fitting and testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, SQL_articles['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=MLPClassifier(),\n",
       "             param_grid={'hidden_layer_sizes': [(2,), (2, 2), (2, 2, 2),\n",
       "                                                (2, 2, 2, 2), (2, 2, 2, 2, 2),\n",
       "                                                (5,), (5, 5), (5, 5, 5),\n",
       "                                                (5, 5, 5, 5), (5, 5, 5, 5, 5),\n",
       "                                                (8,), (8, 8), (8, 8, 8),\n",
       "                                                (8, 8, 8, 8), (8, 8, 8, 8, 8),\n",
       "                                                (11,), (11, 11), (11, 11, 11),\n",
       "                                                (11, 11, 11, 11),\n",
       "                                                (11, 11, 11, 11, 11), (14,),\n",
       "                                                (14, 14), (14, 14, 14),\n",
       "                                                (14, 14, 14, 14),\n",
       "                                                (14, 14, 14, 14, 14)]})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = [1,2,3,4,5]\n",
    "layer_sizes = [2,5,8,11,14]\n",
    "tuple_list = []\n",
    "\n",
    "for layer_size in layer_sizes:\n",
    "    for layer in layers:\n",
    "        tuple_list.append((layer_size,)*layer)\n",
    "        \n",
    "inputs = {'hidden_layer_sizes': tuple_list}\n",
    "\n",
    "# Define the classifier classes\n",
    "MLP = MLPClassifier()\n",
    "\n",
    "#Gridsearch\n",
    "cross_val = GridSearchCV(MLP, inputs)\n",
    "\n",
    "# Fit the model\n",
    "cross_val.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cross_val.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hidden_layer_sizes is: (2,)\n",
      "[[0.7599     0.75663333 0.7594     0.662      0.66186667]\n",
      " [0.75726667 0.7536     0.75436667 0.74743333 0.7475    ]\n",
      " [0.75196667 0.74543333 0.74693333 0.74666667 0.75056667]\n",
      " [0.74183333 0.74016667 0.73873333 0.7371     0.7362    ]\n",
      " [0.7421     0.72866667 0.7322     0.7317     0.7375    ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_layers = np.zeros((len(layer_sizes),len(layers)))\n",
    "mts = df['mean_test_score']\n",
    "best_parameters = df['param_hidden_layer_sizes'][np.argmax(np.array(mts))]\n",
    "print(\"The best hidden_layer_sizes is: \" + str(best_parameters))\n",
    "\n",
    "for i in range(len(layer_sizes)):\n",
    "    for j in range(len(layers)):\n",
    "        hidden_layers[i,j] = mts[j+i*len(layers)]\n",
    "        \n",
    "\n",
    "sns.heatmap(hidden_layers, vmin = 0.0, vmax = 1.0)\n",
    "print(hidden_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the kaggle data set\n",
    "KAGGLE_df = pd.read_json(r'test_set.json')\n",
    "\n",
    "#Cleaning the kaggle_content\n",
    "KAGGLE_df['article'] = [cleantext(article_content) for article_content in KAGGLE_df['article']]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorizing the content with word embedding\n",
    "embeddings = distilBERT(KAGGLE_df['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['True' 'True' 'True' ... 'True' 'True' 'True']\n",
      "         id label\n",
      "0      8476  REAL\n",
      "1     10294  REAL\n",
      "2      3608  REAL\n",
      "3     10142  REAL\n",
      "4       875  REAL\n",
      "...     ...   ...\n",
      "6330   4490  REAL\n",
      "6331   8062  FAKE\n",
      "6332   8622  REAL\n",
      "6333   4021  REAL\n",
      "6334   4330  REAL\n",
      "\n",
      "[6335 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Following line is the actual prediction based on the trained neural network model 'cross_val' \n",
    "KAGGLE_df_prediction = cross_val.predict(embeddings)\n",
    "\n",
    "to_kaggle_df = KAGGLE_df[['id','article']]\n",
    "to_kaggle_df['article'] = KAGGLE_df_prediction\n",
    "to_kaggle_df = to_kaggle_df.rename(columns={'article':'label'})\n",
    "to_kaggle_df.loc[to_kaggle_df['label'].isin(['True']), 'label'] = 'REAL' #labels types 'fake'\n",
    "to_kaggle_df.loc[to_kaggle_df['label'].isin(['Fake']), 'label'] = 'FAKE' #labels types 'true'\n",
    "to_kaggle_df.to_csv(\"Kaggle/Kaggle_neural_prediction.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorizing the content with TF-IDF\n",
    "content_tfidf = vect.transform(KAGGLE_df['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following line is the actual prediction based on the trained kNN model 'k_nearest' \n",
    "KAGGLE_df_prediction = k_nearest.predict(content_tfidf)\n",
    "\n",
    "to_kaggle_df = KAGGLE_df[['id','article']]\n",
    "to_kaggle_df['article'] = KAGGLE_df_prediction\n",
    "to_kaggle_df = to_kaggle_df.rename(columns={'article':'label'})\n",
    "to_kaggle_df.loc[to_kaggle_df['label'].isin(['True']), 'label'] = 'REAL' #labels types 'fake'\n",
    "to_kaggle_df.loc[to_kaggle_df['label'].isin(['Fake']), 'label'] = 'FAKE' #labels types 'true'\n",
    "to_kaggle_df.to_csv(\"Kaggle/Kaggle_kNN_prediction.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following line is the actual prediction based on the trained SVC model 'svc' \n",
    "KAGGLE_df_prediction = svc.predict(content_tfidf)\n",
    "\n",
    "to_kaggle_df = KAGGLE_df[['id','article']]\n",
    "to_kaggle_df['article'] = KAGGLE_df_prediction\n",
    "to_kaggle_df = to_kaggle_df.rename(columns={'article':'label'})\n",
    "to_kaggle_df.loc[to_kaggle_df['label'].isin(['True']), 'label'] = 'REAL' #labels types 'fake'\n",
    "to_kaggle_df.loc[to_kaggle_df['label'].isin(['Fake']), 'label'] = 'FAKE' #labels types 'true'\n",
    "to_kaggle_df.to_csv(\"Kaggle/Kaggle_SVC_prediction.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all three datasets\n",
    "trainFile = 'train.tsv'\n",
    "testFile = 'test.tsv'\n",
    "valFile = 'valid.tsv'\n",
    "\n",
    "# add header to all three datasets\n",
    "traindata = pd.read_csv(r'liar/'+trainFile, delimiter='\\t', encoding='utf-8', names=\n",
    "                        [\"json ID\", \"label\", \"statement\", \"subject\", \"speaker\", \"job title\", \"state\", \"party\",\n",
    "                         \"barely true\", \"false\", \"half true\", \"mostly true\", \"pants on fire\", \"los\", \"justification\"])\n",
    "\n",
    "testdata = pd.read_csv(r'liar/'+testFile, delimiter='\\t', encoding='utf-8', names=\n",
    "                        [\"json ID\", \"label\", \"statement\", \"subject\", \"speaker\", \"job title\", \"state\", \"party\",\n",
    "                         \"barely true\", \"false\", \"half true\", \"mostly true\", \"pants on fire\", \"los\", \"justification\"])\n",
    "\n",
    "valdata = pd.read_csv(r'liar/'+valFile, delimiter='\\t', encoding='utf-8', names=\n",
    "                        [\"json ID\", \"label\", \"statement\", \"subject\", \"speaker\", \"job title\", \"state\", \"party\",\n",
    "                         \"barely true\", \"false\", \"half true\", \"mostly true\", \"pants on fire\", \"los\", \"justification\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11972.json</td>\n",
       "      <td>building a wall on the &lt;URL&gt;mexico border will...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11685.json</td>\n",
       "      <td>wisconsin is on pace to double the number of l...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11096.json</td>\n",
       "      <td>says john mccain has done nothing to help the ...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5209.json</td>\n",
       "      <td>suzanne bonamici supports a plan that will cut...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9524.json</td>\n",
       "      <td>when asked by a reporter whether hes at the ce...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>7334.json</td>\n",
       "      <td>says his budget provides the highest state fun...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>9788.json</td>\n",
       "      <td>ive been here almost every day</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>10710.json</td>\n",
       "      <td>in the early &lt;NUM&gt;s sen edward kennedy secretl...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>3186.json</td>\n",
       "      <td>says an epa permit languished under strickland...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>6743.json</td>\n",
       "      <td>says the governor is going around the state ta...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1267 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                            content label\n",
       "0     11972.json  building a wall on the <URL>mexico border will...  True\n",
       "1     11685.json  wisconsin is on pace to double the number of l...  Fake\n",
       "2     11096.json  says john mccain has done nothing to help the ...  Fake\n",
       "3      5209.json  suzanne bonamici supports a plan that will cut...  True\n",
       "4      9524.json  when asked by a reporter whether hes at the ce...  Fake\n",
       "...          ...                                                ...   ...\n",
       "1262   7334.json  says his budget provides the highest state fun...  True\n",
       "1263   9788.json                     ive been here almost every day  Fake\n",
       "1264  10710.json  in the early <NUM>s sen edward kennedy secretl...  Fake\n",
       "1265   3186.json  says an epa permit languished under strickland...  Fake\n",
       "1266   6743.json  says the governor is going around the state ta...  Fake\n",
       "\n",
       "[1267 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating new dataFrame with id, content label\n",
    "LIAR_data = testdata[['json ID','statement','label']]\n",
    "LIAR_data = LIAR_data.rename(columns={'json ID':'id', 'statement':'content'})\n",
    "\n",
    "#making sure that false labels are 0 and true labels are 1\n",
    "LIAR_data.loc[LIAR_data['label'].isin(['pants-fire','false','barely-true']),'label'] = 'Fake'\n",
    "LIAR_data.loc[LIAR_data['label'].isin(['mostly-true','true','half-true']),'label'] = 'True'\n",
    "\n",
    "#cleaning content\n",
    "LIAR_data['content'] = [cleantext(article_content) for article_content in LIAR_data['content']] \n",
    "\n",
    "LIAR_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizing content with TF-IDF\n",
    "liar_content_tfidf = vect.transform(LIAR_data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liar KNN accuracy:0.5059194948697711\n",
      "svc accuracy:0.5516969218626677\n"
     ]
    }
   ],
   "source": [
    "#computing predictions with kNN on the LIAR data set\n",
    "Liar_KNN_pred = k_nearest.predict(liar_content_tfidf)\n",
    "print(\"Liar KNN accuracy:\" + str(accuracy_score(LIAR_data['label'],Liar_KNN_pred)))#+ str(accuracy_score(y_test,Liar_KNN_pred)))\n",
    "\n",
    "#computing predictions with SVC on the LIAR data set\n",
    "liar_svc_pred = svc.predict(liar_content_tfidf)\n",
    "print(\"svc accuracy:\" + str(accuracy_score(LIAR_data['label'],liar_svc_pred)))#str(accuracy_score(y_test,liar_svc_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (847 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#Vectorizing the content with word embedding\n",
    "embeddings = distilBERT(LIAR_data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liar neural network accuracy:0.5509076558800315\n"
     ]
    }
   ],
   "source": [
    "#computing predictions with neural network on the LIAR data set\n",
    "Liar_nn_pred = cross_val.predict(embeddings)\n",
    "print(\"Liar neural network accuracy:\" + str(accuracy_score(LIAR_data['label'],Liar_nn_pred)))#+ str(accuracy_score(y_test,Liar_KNN_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
