{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/danielstephensen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danielstephensen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import porter\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2 as pc\n",
    "from IPython import display\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_database_login = \"dbname=datascience user=postgres password=****\"\n",
    "SQLtables_path = \"/Users/krist/Desktop/Uni/milestone/DataScienceRep01/SQLtables/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleantext cleans the input string with the following functions: Characters are set to lowercase, \n",
    "#urls are substituted with <URL>, dates are substitured with <DATE>, emails are substitured with <EMAIL>\n",
    "#numbers are substitured with <NUM>, newlines and non-letter characters are removed.\n",
    "def cleantext(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<|>', \"\", text)\n",
    "    text = re.sub(r'(https?:\\/\\/)?w{0,3}\\.?[a-z]+\\.[a-z]\\w*[\\w\\/-]*', \"<URL>\", text)\n",
    "    text = re.sub(r'(jan\\.?(uary)?|feb\\.?(uary)?|mar\\.?(ch)?|apr\\.?(il)?|may|jun\\.(e)?|jul\\.(y)?|aug\\.?(ust)?|sep\\.?(tember)?|oct\\.?(ober)?|nov\\.?(ember)?|dec\\.?(ember)?|monday|tuesday|wednesday|thursday|friday|saturday|sunday) (the )?\\d{1,2}((th)?,?( \\d{4})?)?', \"<DATE>\", text)\n",
    "    text = re.sub(r'\\w+@\\w+\\.[a-zA-Z]{2,3}', \"<EMAIL>\", text)\n",
    "    text = re.sub(r'[0-9]+', \"<NUM>\", text)\n",
    "    text = re.sub(r'(\\\\n)+|\\s{2,}|(\\\\t+)', \" \", text)\n",
    "    text = re.sub(r'\\.|,|\\\\|-|\\?|\\(|\\)|\\||&|\"|”|“|:|!|\\+|-|–|—|\\/|\\$|%|€|#|;|\\[|\\]|©|®|…|=', \"\", text)\n",
    "    return text\n",
    "\n",
    "#cleanMetaKeywords cleans the input string with the following functions: \n",
    "#Characters are set to lowercase, newlines and non-letter characters are removed.\n",
    "def cleanMetaKeywords(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(\\\\n)+|\\s{2,}|(\\\\t+)', \" \", text)\n",
    "    text = re.sub(r'\\.|\\\\|-|\\?|\\(|\\)|\\||&|\"|”|“|:|!|\\+|-|\\'|–|—|\\/|\\$|%|€|#|;|\\[|\\]|©|®|…|=|<|>', \"\", text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def stopword(word_list):\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    return [word for word in word_list if word not in stop_words]\n",
    "\n",
    "def stemming(word_list):\n",
    "    stemmer = porter.PorterStemmer()\n",
    "\n",
    "    return [stemmer.stem(word) for word in word_list]\n",
    "\n",
    "def getSoup(url):\n",
    "    response = requests.get(url)\n",
    "    contents = response.content\n",
    "    return BeautifulSoup(contents, 'html.parser')\n",
    "\n",
    "def executeSQL(filename, cur):\n",
    "    fd = open(filename, 'r')\n",
    "    sqlFile = fd.read()\n",
    "    fd.close()\n",
    "    sqlCommands = sqlFile.split(';')\n",
    "    for command in sqlCommands:\n",
    "            cur.execute(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping data from Politics and Conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCDEFGHIJK\n"
     ]
    }
   ],
   "source": [
    "group_nr = 1\n",
    "article_start_letters = \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr%23:group_nr%23+10]\n",
    "print(article_start_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The algorithm stops finding new articles when 'stop_searching' is set to True\n",
    "stop_searching = False\n",
    "\n",
    "#Finding the nextpage link in the first iteration is a little different, and therefore this value is needed\n",
    "first_iteration = True\n",
    "\n",
    "#The root url is the domain of wikinews\n",
    "root_link = 'https://en.wikinews.org'\n",
    "\n",
    "#next_page is the webpage that the algorithm searches for articles in next iteration of the while-loop\n",
    "next_page = root_link + '/w/index.php?title=Category:Politics_and_conflicts'\n",
    "\n",
    "#The links to the articles starting with the 'article_start_letters' are appended to 'article links'\n",
    "article_links = []\n",
    "\n",
    "#For each iteration this list gets some values if the first letter \n",
    "#of the first article in the next webpage is between A and K\n",
    "first_letter_between_B_K = []\n",
    "\n",
    "#A regex used for 'first_letter_between_B_K'\n",
    "continue_iterations = re.compile(r\"pagefrom=[A-K]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not(stop_searching):\n",
    "    soup = getSoup(next_page)\n",
    "    articles = soup.find(id=\"mw-pages\")\n",
    "    \n",
    "    links = [link.get(\"href\") for link in articles.find_all('a')]\n",
    "    \n",
    "    if first_iteration:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[0])\n",
    "        first_iteration = False\n",
    "        next_page = root_link + links[0]\n",
    "        article_links += [root_link + group_link for group_link in links[1:] if group_link[6] in article_start_letters]\n",
    "    else:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[1])\n",
    "        next_page = root_link + links[1]\n",
    "        article_links += [root_link + group_link for group_link in links[2:] if group_link[6] in article_start_letters]\n",
    "    \n",
    "    if len(first_letter_between_B_K) == 0:\n",
    "        stop_searching = True\n",
    "    \n",
    "    first_letter_between_B_K = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_source_code = [getSoup(article) for article in article_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = range(0,len(article_links))\n",
    "article_titles = [article.find('h1').get_text() for article in article_source_code]\n",
    "article_release_date = [str(article.find(id=\"publishDate\"))[50:60] for article in article_source_code]\n",
    "article_urls = article_links\n",
    "article_content = [\" \".join([p.get_text() for p in (article.find(id=\"mw-content-text\")).find_all('p')]) for article in article_source_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i agree with brs that categorypolitical activi...</td>\n",
       "      <td>Category talk:Activists</td>\n",
       "      <td></td>\n",
       "      <td>https://en.wikinews.org/wiki/Category_talk:Act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;DATE&gt; in british columbia canada leadership d...</td>\n",
       "      <td>B.C. elections debate fiery but not conclusive</td>\n",
       "      <td></td>\n",
       "      <td>https://en.wikinews.org/wiki/B.C._elections_de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>wednesday &lt;DATE&gt; a suicide car bomb exploded y...</td>\n",
       "      <td>Baghdad bombing kills several people, scores i...</td>\n",
       "      <td>2010-01-27</td>\n",
       "      <td>https://en.wikinews.org/wiki/Baghdad_bombing_k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sunday &lt;DATE&gt; a judge in baghdad iraq has clea...</td>\n",
       "      <td>Baghdad judge clears pair of murdering six for...</td>\n",
       "      <td>2010-10-10</td>\n",
       "      <td>https://en.wikinews.org/wiki/Baghdad_judge_cle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>thursday &lt;DATE&gt; the bodies of over &lt;NUM&gt;&lt;NUM&gt; ...</td>\n",
       "      <td>Baghdad morgue received over 1,000 bodies in July</td>\n",
       "      <td>2005-08-18</td>\n",
       "      <td>https://en.wikinews.org/wiki/Baghdad_morgue_re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2855</th>\n",
       "      <td>2855</td>\n",
       "      <td>wednesday &lt;DATE&gt; kyrgyz authorities declared &lt;...</td>\n",
       "      <td>Kyrgyz government declares elections valid, re...</td>\n",
       "      <td>2005-03-23</td>\n",
       "      <td>https://en.wikinews.org/wiki/Kyrgyz_government...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>2856</td>\n",
       "      <td>tuesday &lt;DATE&gt; thousands of protesters seized ...</td>\n",
       "      <td>Kyrgyz president orders election probe as prot...</td>\n",
       "      <td>2005-03-22</td>\n",
       "      <td>https://en.wikinews.org/wiki/Kyrgyz_president_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2857</th>\n",
       "      <td>2857</td>\n",
       "      <td>sunday june &lt;NUM&gt; &lt;NUM&gt; kyrgyzstani citizens o...</td>\n",
       "      <td>Kyrgyzstan votes on referendum for new constit...</td>\n",
       "      <td>2010-06-27</td>\n",
       "      <td>https://en.wikinews.org/wiki/Kyrgyzstan_votes_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2858</th>\n",
       "      <td>2858</td>\n",
       "      <td>saturday june &lt;NUM&gt; &lt;NUM&gt; a second day of ethn...</td>\n",
       "      <td>Kyrgyzstan: Ethnic unrest continues, governmen...</td>\n",
       "      <td>2010-06-12</td>\n",
       "      <td>https://en.wikinews.org/wiki/Kyrgyzstan:_Ethni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2859</th>\n",
       "      <td>2859</td>\n",
       "      <td>monday june &lt;NUM&gt; &lt;NUM&gt; according to a spokesm...</td>\n",
       "      <td>Kyrgyzstan: Violence continues, death toll rises</td>\n",
       "      <td>2010-06-14</td>\n",
       "      <td>https://en.wikinews.org/wiki/Kyrgyzstan:_Viole...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2860 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            content  \\\n",
       "0        0  i agree with brs that categorypolitical activi...   \n",
       "1        1  <DATE> in british columbia canada leadership d...   \n",
       "2        2  wednesday <DATE> a suicide car bomb exploded y...   \n",
       "3        3  sunday <DATE> a judge in baghdad iraq has clea...   \n",
       "4        4  thursday <DATE> the bodies of over <NUM><NUM> ...   \n",
       "...    ...                                                ...   \n",
       "2855  2855  wednesday <DATE> kyrgyz authorities declared <...   \n",
       "2856  2856  tuesday <DATE> thousands of protesters seized ...   \n",
       "2857  2857  sunday june <NUM> <NUM> kyrgyzstani citizens o...   \n",
       "2858  2858  saturday june <NUM> <NUM> a second day of ethn...   \n",
       "2859  2859  monday june <NUM> <NUM> according to a spokesm...   \n",
       "\n",
       "                                                  title release_date  \\\n",
       "0                               Category talk:Activists                \n",
       "1        B.C. elections debate fiery but not conclusive                \n",
       "2     Baghdad bombing kills several people, scores i...   2010-01-27   \n",
       "3     Baghdad judge clears pair of murdering six for...   2010-10-10   \n",
       "4     Baghdad morgue received over 1,000 bodies in July   2005-08-18   \n",
       "...                                                 ...          ...   \n",
       "2855  Kyrgyz government declares elections valid, re...   2005-03-23   \n",
       "2856  Kyrgyz president orders election probe as prot...   2005-03-22   \n",
       "2857  Kyrgyzstan votes on referendum for new constit...   2010-06-27   \n",
       "2858  Kyrgyzstan: Ethnic unrest continues, governmen...   2010-06-12   \n",
       "2859   Kyrgyzstan: Violence continues, death toll rises   2010-06-14   \n",
       "\n",
       "                                                    url  \n",
       "0     https://en.wikinews.org/wiki/Category_talk:Act...  \n",
       "1     https://en.wikinews.org/wiki/B.C._elections_de...  \n",
       "2     https://en.wikinews.org/wiki/Baghdad_bombing_k...  \n",
       "3     https://en.wikinews.org/wiki/Baghdad_judge_cle...  \n",
       "4     https://en.wikinews.org/wiki/Baghdad_morgue_re...  \n",
       "...                                                 ...  \n",
       "2855  https://en.wikinews.org/wiki/Kyrgyz_government...  \n",
       "2856  https://en.wikinews.org/wiki/Kyrgyz_president_...  \n",
       "2857  https://en.wikinews.org/wiki/Kyrgyzstan_votes_...  \n",
       "2858  https://en.wikinews.org/wiki/Kyrgyzstan:_Ethni...  \n",
       "2859  https://en.wikinews.org/wiki/Kyrgyzstan:_Viole...  \n",
       "\n",
       "[2860 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_articles = pd.DataFrame()\n",
    "\n",
    "scraped_articles['id'] = article_id\n",
    "scraped_articles['content'] = [cleantext(content) for content in article_content]\n",
    "scraped_articles['title'] = article_titles\n",
    "scraped_articles['release_date'] = article_release_date\n",
    "scraped_articles['url'] = article_urls\n",
    "\n",
    "scraped_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_articles.to_csv(\"SQLtables/scraped_articles.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up a connection with the SQL server. Make sure that you write your own dbname, user and password as input\n",
    "conn = pc.connect(SQL_database_login)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executeSQL('SQLfiles/createTableScraped.sql', cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()\n",
    "content_tfidf = vect.fit_transform(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
