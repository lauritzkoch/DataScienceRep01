{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lk/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/lk/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import porter\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2 as pc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group members: Angelina Näsström (nzv947), Daniel Stephensen (fbp131), Kristina Wilke (mlt790), Lauritz Koch (hdg618)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the following procedures: cleaning, tokenizing, removing stopwords and stemming the data. When cleaning the data we made sure of the following: \n",
    "1. all letters are in lowercase\n",
    "2. all urls are written as < URL >\n",
    "3. all dates are written as < DATE >\n",
    "4. all emails are written as < EMAIL >\n",
    "5. all numbers are written as < NUM >\n",
    "6. all unimportant symbols are removed\n",
    "\n",
    "Converting all letters to lowercase makes it easier to compare different words. Point 2-5 are useful because it makes it possible to count the number of urls, dates, emails and numbers. Also, removing these makes sure that they are not treated as words. Removing unimportant symbols makes sure that these are not treated as words. \n",
    "\n",
    "Tokenization makes processing of the data easier, as it eliminates blank spaces and punctuations etc, making the text more homogeneous. In the tokenization process, we, for example, made all the data lower-case, thus not having two different results when processing 'Hello' and 'hello'.\n",
    "\n",
    "Removing stopwords is useful because these words do not help giving meaning to the documents, in other words they are noise.\n",
    "\n",
    "Stemming the data is useful because it makes sure that different variants of the same word is converted into the rood of the word. This way it is possible to make sure that two different words (same word with different endings) are understood the same way, because they actually have the exact same meaning.\n",
    "\n",
    "Implementing task 2 we have used the Pandas library, nltk library and re library. The Pandas library has just been used to read the data from the 'news_sample.csv' file. word_tokenize is a sublibrary of nltk that has some useful functions for tokenizing. stopwords is a sublibrary of nltk.corpus that has some useful functions for removing stopwords. porter is a sublibrary of nltk.stem that has some useful functions for stemming data. These three sublibraries are useful because you do not need to create your own complex functions to tokenize, remove stopwords and stem the data. We have not used the clean_text library because we it did not have all the functionality needed for the task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/lk/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3156: DtypeWarning: Columns (0,1) have mixed types.Specify dtype option on import or set low_memory=False.\n  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "datasample = pd.read_csv('news_sample.csv')\n",
    "data = pd.read_csv('1mio-raw.csv/1mio-raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleantext(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<|>', \"\", text)\n",
    "    text = re.sub(r'(https?:\\/\\/)?w{0,3}\\.?[a-z]+\\.[a-z]\\w*[\\w\\/-]*', \"<URL>\", text)\n",
    "    text = re.sub(r'(jan\\.?(uary)?|feb\\.?(uary)?|mar\\.?(ch)?|apr\\.?(il)?|may|jun\\.(e)?|jul\\.(y)?|aug\\.?(ust)?|sep\\.?(tember)?|oct\\.?(ober)?|nov\\.?(ember)?|dec\\.?(ember)?|monday|tuesday|wednesday|thursday|friday|saturday|sunday) (the )?\\d{1,2}((th)?,?( \\d{4})?)?', \"<DATE>\", text)\n",
    "    text = re.sub(r'\\w+@\\w+\\.[a-zA-Z]{2,3}', \"<EMAIL>\", text)\n",
    "    text = re.sub(r'[0-9]+', \"<NUM>\", text)\n",
    "    text = re.sub(r'(\\\\n)+|\\s{2,}|(\\\\t+)', \" \", text)\n",
    "    text = re.sub(r'\\.|,|\\\\|-|\\?|\\(|\\)|\\||&|\"|”|“|:|!|\\+|-|–|—|\\/|\\$|%|€|#|;|\\[|\\]|©|®|…|=', \"\", text)\n",
    "    return text\n",
    "\n",
    "def cleanMetaKeywords(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<|>', \"\", text)\n",
    "    text = re.sub(r'(\\\\n)+|\\s{2,}|(\\\\t+)', \" \", text)\n",
    "    text = re.sub(r'\\.|\\\\|-|\\?|\\(|\\)|\\||&|\"|”|“|:|!|\\+|-|\\'|–|—|\\/|\\$|%|€|#|;|\\[|\\]|©|®|…|=', \"\", text)\n",
    "    return text\n"
   ]
  },
  {
   "source": [
    "Below is Three part job of \n",
    "1) Cleaning data\n",
    "2) tokenization \n",
    "3) Stemming\n",
    "\n",
    "The processed data will be saved in the 'Keywords' column, such that the original data can be untouched in 'content'\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "michael hayden, sthole countries, daca, haiti, el salvador, africa\nHomeland Security, Trump Administration, Immigration, Media Bias, ISIS/Islamic State, Gun Rights, Donald Trump, Russia\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Setup\n",
    "stop_words = stopwords.words('english')\n",
    "stemmer = porter.PorterStemmer()\n",
    "\n",
    "data[\"content\"] = [cleantext(i) for i in data[\"content\"]] #Clean\n",
    "clean_data_tokens = [word_tokenize(i) for i in data[\"content\"]] #Creating tokens\n",
    "data[\"keywords\"] = [item for item in clean_data_tokens if item not in stop_words] #Stopword removal\n",
    "data[\"keywords\"] = data[\"keywords\"].apply(lambda x: set([stemmer.stem(y) for y in x])) #Stem\n",
    "\n",
    "#data[\"meta_keywords\"] = data[\"meta_keywords\"].apply(lambda x: [cleanMetaKeywords(i) for i in x])\n",
    "data[\"meta_keywords\"] =[cleanMetaKeywords(i) for i in data[\"meta_keywords\"]]\n",
    "\n",
    "#print((data[\"meta_keywords\"][247]))\n",
    "#print((data[\"tags\"][247]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating tags for explosion\n",
    "#Updating Authors for explosion \n",
    "\n",
    "#Convert each row in column 'tags' to str (from float???)\n",
    "data[\"tags\"] = [(str(i)).split(\",\") for i in data[\"tags\"]] \n",
    "data[\"authors\"] = [(str(i)).split(\",\") for i in data[\"authors\"]]\n",
    "data[\"meta_keywords\"] =[(str(i)).split(\",\") for i in data[\"meta_keywords\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0                                                     []\n",
       "1                                                     []\n",
       "2                                                     []\n",
       "3                                                     []\n",
       "4                                                     []\n",
       "                             ...                        \n",
       "245                                                   []\n",
       "246                                                   []\n",
       "247    [michael hayden,  sthole countries,  daca,  ha...\n",
       "248    [antonio sabato jr,  oprah winfrey,  president...\n",
       "249    [bill clinton,  myanmar,  calls,  release,  re...\n",
       "Name: meta_keywords, Length: 250, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 297
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relation tables\n",
    "article_tags_relation_table = data[['id','tags']].explode('tags')\n",
    "owns_relation_table = data[['id', 'domain']]\n",
    "authors_of_article_table = data[['id','authors']].explode('authors')\n",
    "article_keywords_relation_table = data[['id','keywords']].explode('keywords')\n",
    "meta_article_keywords_relation_table  = data[['id','meta_keywords']].explode('meta_keywords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entity tables\n",
    "articles_table = data[['id','content','type','url','scraped_at','inserted_at','updated_at','meta_description','title']]\n",
    "keywords_table = pd.DataFrame(set(data['keywords'].explode('keywords')))\n",
    "meta_keywords_table = pd.DataFrame(set(data['meta_keywords'].explode('meta_keywords')))\n",
    "tags_table= pd.DataFrame(set(data['tags'].explode('tags')))\n",
    "domain_table = pd.DataFrame(set(data['domain']))\n",
    "authors_table = pd.DataFrame(set(data['authors'].explode('authors').explode('authors')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entities to CSV\n",
    "articles_table.to_csv(\"SQLtables/articles_table.csv\",index=False,header=False)\n",
    "keywords_table.to_csv(\"SQLtables/keywords_table.csv\",index=False,header=False)\n",
    "meta_keywords_table.to_csv(\"SQLtables/meta_keywords_table.csv\",index=False,header=False)\n",
    "tags_table.to_csv(\"SQLtables/tags_table.csv\",index=False,header=False)\n",
    "authors_table.to_csv(\"SQLtables/authors_table.csv\",index=False,header=False)\n",
    "domain_table.to_csv(\"SQLtables/domain_table.csv\",index=False,header=False)\n",
    "#relations to CSV\n",
    "owns_relation_table.to_csv(\"SQLtables/owns_table.csv\",index=False,header=False)\n",
    "authors_of_article_table.to_csv(\"SQLtables/authors_of_article.csv\",index=False,header=False)\n",
    "article_tags_relation_table.to_csv(\"SQLtables/article_tags_relation_table.csv\",index=False,header=False)\n",
    "article_keywords_relation_table.to_csv(\"SQLtables/article_keywords_relation_table.csv\",index=False,header=False)\n",
    "meta_article_keywords_relation_table.to_csv(\"SQLtables/meta_article_keywords_relation_table.csv\",index=False,header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pc.connect(\"dbname=datascience user=lk password=l\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#cur.execute(\"delete from article\")\n",
    "#cur.execute(\"delete from authors\")\n",
    "#cur.execute(\"delete from authors_of\")\n",
    "#cur.execute(\"delete from domain\")\n",
    "#cur.execute(\"delete from owns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"copy authors_of from '/home/lk/Desktop/datalogi/DataScience/git/DataScienceRep01/SQLtables/authors_of_article.csv' with (format csv)\")\n",
    "cur.execute(\"Select * from keywords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#create Stemmed StopWorded vocab\n",
    "#stsw_vocab = set(stemmed_tokens)\n",
    "#stsw_vocab.remove('<')\n",
    "#stsw_vocab.remove('>')\n",
    "#print(stsw_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-trivial observation 1: How many percent of articles with the word \"trump\" in it is fake news?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_articles = [word_tokenize(i) for i in clean_data]\n",
    "articles_vocabulary = [set(i) for i in tokenized_articles]\n",
    "trump_included = [i for i in range(len(articles_vocabulary)) if \"trump\" in articles_vocabulary[i]]\n",
    "trump_fake_news = 0\n",
    "\n",
    "for i in range(len(trump_included)):\n",
    "    if data['type'][i] == \"fake\":\n",
    "        trump_fake_news += 1\n",
    "\n",
    "print(int(trump_fake_news*100/len(trump_included)),\"% of articles where the name 'trump' is present, is a fake news article\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-trivial observation 2: Is the number of articles spread out tolerably evenly between the domains?\n",
    "\n",
    "non-trivial observation 3: Is there a link between which domain an article comes from and if it is fake news?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_articles = [word_tokenize(i) for i in clean_data]\n",
    "articles_vocabulary = [set(i) for i in tokenized_articles]\n",
    "#Missing author corellation\n",
    "\n",
    "domainList = data['domain']\n",
    "TypeList = data['type']\n",
    "domains = set(domainList)\n",
    "fakeDomainScore = np.zeros(len(domains))\n",
    "totalDomainScore = np.zeros(len(domains)) \n",
    "for i in range (len(domainList)):\n",
    "    if (data['type'][i] == 'fake'):\n",
    "        index = 0 \n",
    "        for domain in domains:\n",
    "            if  data['domain'][i] == domain:\n",
    "                fakeDomainScore[index] += 1\n",
    "            index+=1\n",
    "    index = 0 \n",
    "    for domain in domains:\n",
    "        if  data['domain'][i] == domain:\n",
    "            totalDomainScore[index] += 1\n",
    "        index+=1\n",
    "print(\"Each of the 29 domains present in the corpus has the following amount of articles in the corpus:\\n\", totalDomainScore)\n",
    "print(\"\\nEach of the 29 domains present in the corpus has the following amount of fake news articles:\\n\", fakeDomainScore)\n",
    "print(\"\\nThis means that Beforeitsnews.com has\", int(totalDomainScore[np.where(fakeDomainScore == 155)]),\"of the articles in the corpus and\", int(fakeDomainScore[np.where(fakeDomainScore == 155)]*100/sum(totalDomainScore)), \"% of all articles. Thus, the number of articles in the corpus are very unevenly spreed between the domains\")\n",
    "print(\"\\nAlso,\", int(fakeDomainScore[np.where(fakeDomainScore == 155)]*100/totalDomainScore[np.where(fakeDomainScore == 155)]), \"% of Beforeitsnews.com's articles are fake news and no other domain has fake news in its articles. Thus, there is a link between which domain an article comes from and if it is fake news (The link is probably a little to big)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-trivial observation 4: How many articles have missing author value? \n",
    "\n",
    "non-trivial observation 5: How much does missing author increase the likelihood that an article is fake news? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = [i for i in data[\"authors\"]]\n",
    "no_author_counter = 0\n",
    "no_author_fake_news = 0\n",
    "no_author_total = 0\n",
    "author_fake_news = 0\n",
    "author_total = 0\n",
    "\n",
    "for i in range(len(authors)):\n",
    "    if not type(authors[i]) == str:\n",
    "        no_author_counter += 1\n",
    "        if data[\"type\"][i] == \"fake\":\n",
    "            no_author_fake_news += 1\n",
    "        no_author_total += 1\n",
    "    elif data[\"type\"][i] == \"fake\":\n",
    "        author_fake_news += 1\n",
    "        author_total += 1\n",
    "    else: \n",
    "        author_total += 1\n",
    "\n",
    "print(int(no_author_counter*100/len(authors)), \"% of the articles does not have an author\")\n",
    "print(int(no_author_fake_news*100/no_author_total),'% of the no-author articles are fake news')\n",
    "print(int(author_fake_news*100/author_total),'% of the articles are fake news')\n",
    "print('Thus we see, that having no author on an article only adds two percent points to the likelihood of it being fake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following 'article start letters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_nr = 1\n",
    "article_start_letters = \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr%23:group_nr%23+10]\n",
    "print(article_start_letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_searching = 0\n",
    "next_page = 'https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts'\n",
    "article_links = []\n",
    "first_letter_between_B_K = []\n",
    "count = 0\n",
    "\n",
    "continue_iterations = re.compile(r\"pagefrom=[A-K]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_iteration = True\n",
    "\n",
    "while stop_searching == 0:\n",
    "    response = requests.get(next_page)\n",
    "    contents = response.content\n",
    "    soup = BeautifulSoup(contents, 'html.parser')\n",
    "    articles = soup.find(id=\"mw-pages\")\n",
    "    \n",
    "    links = [link.get(\"href\") for link in articles.find_all('a')]\n",
    "    \n",
    "    if first_iteration:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[0])\n",
    "        first_iteration = False\n",
    "        next_page = 'https://en.wikinews.org'+links[0]\n",
    "        article_links += ['https://en.wikinews.org' + group_link for group_link in links[1:] if group_link[6] in article_start_letters]\n",
    "    else:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[1])\n",
    "        next_page = 'https://en.wikinews.org'+links[1]\n",
    "        article_links += ['https://en.wikinews.org' + group_link for group_link in links[2:] if group_link[6] in article_start_letters]\n",
    "    \n",
    "    if len(first_letter_between_B_K) == 0:\n",
    "        stop_searching = 1\n",
    "    \n",
    "    first_letter_between_B_K = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following box takes about 20 minutes to run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_source_code = [BeautifulSoup(requests.get(article).content, 'html.parser') for article in article_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_titles = [article.find('h1').get_text() for article in article_source_code]\n",
    "article_release_date = [str(article.find(id=\"publishDate\"))[50:60] for article in article_source_code]\n",
    "article_sources = [\", \".join([element.get('href') for element in ((article.find('ul')).find_all('a', rel = 'nofollow', class_ ='external text'))]) for article in article_source_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_articles = pd.DataFrame()\n",
    "pd_articles['Titles'] = article_titles\n",
    "pd_articles['Release_Date'] = article_release_date\n",
    "pd_articles['Sources']= article_sources\n",
    "\n",
    "pd_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python379jvsc74a57bd0e5d6c7a09279b85059beea2303df8896997618c01a271d81f21053bdf903c867",
   "display_name": "Python 3.7.9 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}