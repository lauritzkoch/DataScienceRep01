{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import porter\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group members: Angelina Näsström (nzv947), Daniel Stephensen (fbp131), Kristina Wilke (mlt790), Lauritz Koch (hdg618)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the following procedures: cleaning, tokenizing, removing stopwords and stemming the data. When cleaning the data we made sure of the following: \n",
    "1. all letters are in lowercase\n",
    "2. all urls are written as < URL >\n",
    "3. all dates are written as < DATE >\n",
    "4. all emails are written as < EMAIL >\n",
    "5. all numbers are written as < NUM >\n",
    "6. all unimportant symbols are removed\n",
    "\n",
    "Converting all letters to lowercase makes it easier to compare different words. Point 2-5 are useful because it makes it possible to count the number of urls, dates, emails and numbers. Also, removing these makes sure that they are not treated as words. Removing unimportant symbols makes sure that these are not treated as words. \n",
    "\n",
    "Tokenization makes processing of the data easier, as it eliminates blank spaces and punctuations etc, making the text more homogeneous. In the tokenization process, we, for example, made all the data lower-case, thus not having two different results when processing 'Hello' and 'hello'.\n",
    "\n",
    "Removing stopwords is useful because these words do not help giving meaning to the documents, in other words they are noise.\n",
    "\n",
    "Stemming the data is useful because it makes sure that different variants of the same word is converted into the rood of the word. This way it is possible to make sure that two different words (same word with different endings) are understood the same way, because they actually have the exact same meaning.\n",
    "\n",
    "Implementing task 2 we have used the Pandas library, nltk library and re library. The Pandas library has just been used to read the data from the 'news_sample.csv' file. word_tokenize is a sublibrary of nltk that has some useful functions for tokenizing. stopwords is a sublibrary of nltk.corpus that has some useful functions for removing stopwords. porter is a sublibrary of nltk.stem that has some useful functions for stemming data. These three sublibraries are useful because you do not need to create your own complex functions to tokenize, remove stopwords and stem the data. We have not used the clean_text library because we it did not have all the functionality needed for the task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('news_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleantext(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<|>', \"\", text)\n",
    "    text = re.sub(r'(https?:\\/\\/)?w{0,3}\\.?[a-z]+\\.[a-z]\\w*[\\w\\/-]*', \"<URL>\", text)\n",
    "    text = re.sub(r'(jan\\.?(uary)?|feb\\.?(uary)?|mar\\.?(ch)?|apr\\.?(il)?|may|jun\\.(e)?|jul\\.(y)?|aug\\.?(ust)?|sep\\.?(tember)?|oct\\.?(ober)?|nov\\.?(ember)?|dec\\.?(ember)?|monday|tuesday|wednesday|thursday|friday|saturday|sunday) (the )?\\d{1,2}((th)?,?( \\d{4})?)?', \"<DATE>\", text)\n",
    "    text = re.sub(r'\\w+@\\w+\\.[a-zA-Z]{2,3}', \"<EMAIL>\", text)\n",
    "    text = re.sub(r'[0-9]+', \"<NUM>\", text)\n",
    "    text = re.sub(r'(\\\\n)+|\\s{2,}|(\\\\t+)', \" \", text)\n",
    "    text = re.sub(r'\\.|,|\\\\|-|\\?|\\(|\\)|\\||&|\"|”|“|:|!|\\+|-|–|—|\\/|\\$|%|€|#|;|\\[|\\]|©|®|…|=', \"\", text)\n",
    "    return text\n",
    "\n",
    "clean_data = [cleantext(i) for i in data[\"content\"]]\n",
    "string_of_contents = \" \".join(clean_data)\n",
    "print(string_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below should be replaced with actual data from after Cleaning\n",
    "cleaned_data_SAMPLE_FOR_CODING_PURPOSES = string_of_contents\n",
    "\n",
    "#Create tokens based on clean_data. cl = clean, da = data\n",
    "clda_tokens = word_tokenize(cleaned_data_SAMPLE_FOR_CODING_PURPOSES)\n",
    "\n",
    "#print((clda_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StopWordsCLeanDAta_tokens\n",
    "stop_words = stopwords.words('english')\n",
    "swclda_tokens = [word for word in clda_tokens if not word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = porter.PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in swclda_tokens]\n",
    "\n",
    "#create Stemmed StopWorded vocab\n",
    "stsw_vocab = set(stemmed_tokens)\n",
    "stsw_vocab.remove('<')\n",
    "stsw_vocab.remove('>')\n",
    "print(stsw_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-trivial observation 1: How many percent of articles with the word \"trump\" in it is fake news?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_articles = [word_tokenize(i) for i in clean_data]\n",
    "articles_vocabulary = [set(i) for i in tokenized_articles]\n",
    "trump_included = [i for i in range(len(articles_vocabulary)) if \"trump\" in articles_vocabulary[i]]\n",
    "trump_fake_news = 0\n",
    "\n",
    "for i in range(len(trump_included)):\n",
    "    if data['type'][i] == \"fake\":\n",
    "        trump_fake_news += 1\n",
    "\n",
    "print(int(trump_fake_news*100/len(trump_included)),\"% of articles where the name 'trump' is present, is a fake news article\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-trivial observation 2: Is the number of articles spread out tolerably evenly between the domains?\n",
    "\n",
    "non-trivial observation 3: Is there a link between which domain an article comes from and if it is fake news?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_articles = [word_tokenize(i) for i in clean_data]\n",
    "articles_vocabulary = [set(i) for i in tokenized_articles]\n",
    "#Missing author corellation\n",
    "\n",
    "domainList = data['domain']\n",
    "TypeList = data['type']\n",
    "domains = set(domainList)\n",
    "fakeDomainScore = np.zeros(len(domains))\n",
    "totalDomainScore = np.zeros(len(domains)) \n",
    "for i in range (len(domainList)):\n",
    "    if (data['type'][i] == 'fake'):\n",
    "        index = 0 \n",
    "        for domain in domains:\n",
    "            if  data['domain'][i] == domain:\n",
    "                fakeDomainScore[index] += 1\n",
    "            index+=1\n",
    "    index = 0 \n",
    "    for domain in domains:\n",
    "        if  data['domain'][i] == domain:\n",
    "            totalDomainScore[index] += 1\n",
    "        index+=1\n",
    "print(\"Each of the 29 domains present in the corpus has the following amount of articles in the corpus:\\n\", totalDomainScore)\n",
    "print(\"\\nEach of the 29 domains present in the corpus has the following amount of fake news articles:\\n\", fakeDomainScore)\n",
    "print(\"\\nThis means that Beforeitsnews.com has\", int(totalDomainScore[np.where(fakeDomainScore == 155)]),\"of the articles in the corpus and\", int(fakeDomainScore[np.where(fakeDomainScore == 155)]*100/sum(totalDomainScore)), \"% of all articles. Thus, the number of articles in the corpus are very unevenly spreed between the domains\")\n",
    "print(\"\\nAlso,\", int(fakeDomainScore[np.where(fakeDomainScore == 155)]*100/totalDomainScore[np.where(fakeDomainScore == 155)]), \"% of Beforeitsnews.com's articles are fake news and no other domain has fake news in its articles. Thus, there is a link between which domain an article comes from and if it is fake news (The link is probably a little to big)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-trivial observation 4: How many articles have missing author value? \n",
    "\n",
    "non-trivial observation 5: How much does missing author increase the likelihood that an article is fake news? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = [i for i in data[\"authors\"]]\n",
    "no_author_counter = 0\n",
    "no_author_fake_news = 0\n",
    "no_author_total = 0\n",
    "author_fake_news = 0\n",
    "author_total = 0\n",
    "\n",
    "for i in range(len(authors)):\n",
    "    if not type(authors[i]) == str:\n",
    "        no_author_counter += 1\n",
    "        if data[\"type\"][i] == \"fake\":\n",
    "            no_author_fake_news += 1\n",
    "        no_author_total += 1\n",
    "    elif data[\"type\"][i] == \"fake\":\n",
    "        author_fake_news += 1\n",
    "        author_total += 1\n",
    "    else: \n",
    "        author_total += 1\n",
    "\n",
    "print(int(no_author_counter*100/len(authors)), \"% of the articles does not have an author\")\n",
    "print(int(no_author_fake_news*100/no_author_total),'% of the no-author articles are fake news')\n",
    "print(int(author_fake_news*100/author_total),'% of the articles are fake news')\n",
    "print('Thus we see, that having no author on an article only adds two percent points to the likelihood of it being fake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following 'article start letters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_nr = 1\n",
    "article_start_letters = \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr%23:group_nr%23+10]\n",
    "print(article_start_letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_searching = 0\n",
    "next_page = 'https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts'\n",
    "article_links = []\n",
    "first_letter_between_B_K = []\n",
    "count = 0\n",
    "\n",
    "continue_iterations = re.compile(r\"pagefrom=[A-K]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_iteration = True\n",
    "\n",
    "while stop_searching == 0:\n",
    "    response = requests.get(next_page)\n",
    "    contents = response.content\n",
    "    soup = BeautifulSoup(contents, 'html.parser')\n",
    "    articles = soup.find(id=\"mw-pages\")\n",
    "    \n",
    "    links = [link.get(\"href\") for link in articles.find_all('a')]\n",
    "    \n",
    "    if first_iteration:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[0])\n",
    "        first_iteration = False\n",
    "        next_page = 'https://en.wikinews.org'+links[0]\n",
    "        article_links += ['https://en.wikinews.org' + group_link for group_link in links[1:] if group_link[6] in article_start_letters]\n",
    "    else:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[1])\n",
    "        next_page = 'https://en.wikinews.org'+links[1]\n",
    "        article_links += ['https://en.wikinews.org' + group_link for group_link in links[2:] if group_link[6] in article_start_letters]\n",
    "    \n",
    "    if len(first_letter_between_B_K) == 0:\n",
    "        stop_searching = 1\n",
    "    \n",
    "    first_letter_between_B_K = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following box takes about 20 minutes to run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_source_code = [BeautifulSoup(requests.get(article).content, 'html.parser') for article in article_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_titles = [article.find('h1').get_text() for article in article_source_code]\n",
    "article_release_date = [str(article.find(id=\"publishDate\"))[50:60] for article in article_source_code]\n",
    "article_sources = [\", \".join([element.get('href') for element in ((article.find('ul')).find_all('a', rel = 'nofollow', class_ ='external text'))]) for article in article_source_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_articles = pd.DataFrame()\n",
    "pd_articles['Titles'] = article_titles\n",
    "pd_articles['Release_Date'] = article_release_date\n",
    "pd_articles['Sources']= article_sources\n",
    "\n",
    "pd_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
