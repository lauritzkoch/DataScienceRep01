{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/danielstephensen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danielstephensen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import porter\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2 as pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document consists of three parts: \n",
    "1. All function definitions used in the implementation are defined\n",
    "2. Milestone 1 tasks\n",
    "3. Milestone 2 tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going further and executing the code, make sure to change the database name, username and password for your SQL database and the path to the path to the 'SQLtables' repository with the csv files in it, so it adjusted to your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_database_login = \"dbname=***** user=***** password=*****\"\n",
    "SQLtables_path = \"/Users/danielstephensen/Desktop/DataScienceGit/SQLtables/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleantext cleans the input string with the following functions: Characters are set to lowercase, \n",
    "#urls are substituted with <URL>, dates are substitured with <DATE>, emails are substitured with <EMAIL>\n",
    "#numbers are substitured with <NUM>, newlines and non-letter characters are removed.\n",
    "def cleantext(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<|>', \"\", text)\n",
    "    text = re.sub(r'(https?:\\/\\/)?w{0,3}\\.?[a-z]+\\.[a-z]\\w*[\\w\\/-]*', \"<URL>\", text)\n",
    "    text = re.sub(r'(jan\\.?(uary)?|feb\\.?(uary)?|mar\\.?(ch)?|apr\\.?(il)?|may|jun\\.(e)?|jul\\.(y)?|aug\\.?(ust)?|sep\\.?(tember)?|oct\\.?(ober)?|nov\\.?(ember)?|dec\\.?(ember)?|monday|tuesday|wednesday|thursday|friday|saturday|sunday) (the )?\\d{1,2}((th)?,?( \\d{4})?)?', \"<DATE>\", text)\n",
    "    text = re.sub(r'\\w+@\\w+\\.[a-zA-Z]{2,3}', \"<EMAIL>\", text)\n",
    "    text = re.sub(r'[0-9]+', \"<NUM>\", text)\n",
    "    text = re.sub(r'(\\\\n)+|\\s{2,}|(\\\\t+)', \" \", text)\n",
    "    text = re.sub(r'\\.|,|\\\\|-|\\?|\\(|\\)|\\||&|\"|”|“|:|!|\\+|-|–|—|\\/|\\$|%|€|#|;|\\[|\\]|©|®|…|=', \"\", text)\n",
    "    return text\n",
    "\n",
    "#cleanMetaKeywords cleans the input string with the following functions: \n",
    "#Characters are set to lowercase, newlines and non-letter characters are removed.\n",
    "def cleanMetaKeywords(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(\\\\n)+|\\s{2,}|(\\\\t+)', \" \", text)\n",
    "    text = re.sub(r'\\.|\\\\|-|\\?|\\(|\\)|\\||&|\"|”|“|:|!|\\+|-|\\'|–|—|\\/|\\$|%|€|#|;|\\[|\\]|©|®|…|=|<|>', \"\", text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def stopword(word_list):\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    return [word for word in word_list if word not in stop_words]\n",
    "\n",
    "def stemming(word_list):\n",
    "    stemmer = porter.PorterStemmer()\n",
    "\n",
    "    return [stemmer.stem(word) for word in word_list]\n",
    "\n",
    "def getSoup(url):\n",
    "    response = requests.get(next_page)\n",
    "    contents = response.content\n",
    "    return BeautifulSoup(contents, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group members: Angelina Näsström (nzv947), Daniel Stephensen (fbp131), Kristina Wilke (mlt790), Lauritz Koch (hdg618)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the following procedures: cleaning, tokenizing, removing stopwords and stemming the data. When cleaning the data we made sure of the following: \n",
    "1. all letters are in lowercase\n",
    "2. all urls are written as < URL >\n",
    "3. all dates are written as < DATE >\n",
    "4. all emails are written as < EMAIL >\n",
    "5. all numbers are written as < NUM >\n",
    "6. all unimportant symbols are removed\n",
    "\n",
    "Converting all letters to lowercase makes it easier to compare different words. Point 2-5 are useful because it makes it possible to count the number of urls, dates, emails and numbers. Also, removing these makes sure that they are not treated as words. Removing unimportant symbols makes sure that these are not treated as words. \n",
    "\n",
    "Tokenization makes processing of the data easier, as it eliminates blank spaces and punctuations etc, making the text more homogeneous. In the tokenization process, we, for example, made all the data lower-case, thus not having two different results when processing 'Hello' and 'hello'.\n",
    "\n",
    "Removing stopwords is useful because these words do not help giving meaning to the documents, in other words they are noise.\n",
    "\n",
    "Stemming the data is useful because it makes sure that different variants of the same word is converted into the rood of the word. This way it is possible to make sure that two different words (same word with different endings) are understood the same way, because they actually have the exact same meaning.\n",
    "\n",
    "Implementing task 2 we have used the Pandas library, nltk library and re library. The Pandas library has just been used to read the data from the 'news_sample.csv' file. word_tokenize is a sublibrary of nltk that has some useful functions for tokenizing. stopwords is a sublibrary of nltk.corpus that has some useful functions for removing stopwords. porter is a sublibrary of nltk.stem that has some useful functions for stemming data. These three sublibraries are useful because you do not need to create your own complex functions to tokenize, remove stopwords and stem the data. We have not used the clean_text library because we it did not have all the functionality needed for the task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Reading the data from news_sample.csv\n",
    "datasample = pd.read_csv('news_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the content\n",
    "cleaned_content = [cleantext(article_content) for article_content in datasample['content']]\n",
    "\n",
    "#Tokenizing the cleaned data\n",
    "tokens = [tokenize(clean_text) for clean_text in cleaned_content]\n",
    "\n",
    "#Removing stopwords\n",
    "stopwords = [stopword(token_list) for token_list in tokens]\n",
    "\n",
    "#Stemming the data\n",
    "stemmed_data = [stemming(stopword_list) for stopword_list in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-trivial observation 1: How many percent of articles with the word \"trump\" in it is fake news?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 % of articles where the name 'trump' is present, is a fake news article\n"
     ]
    }
   ],
   "source": [
    "articles_vocabulary = [set(i) for i in tokens]\n",
    "trump_included = [vocabulary for vocabulary in range(len(articles_vocabulary)) if \"trump\" in articles_vocabulary[vocabulary]]\n",
    "trump_fake_news = 0\n",
    "\n",
    "for i in range(len(trump_included)):\n",
    "    if datasample['type'][i] == \"fake\":\n",
    "        trump_fake_news += 1\n",
    "\n",
    "print(int(trump_fake_news*100/len(trump_included)),\"% of articles where the name 'trump' is present, is a fake news article\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-trivial observation 2: Is the number of articles spread out tolerably evenly between the domains?\n",
    "\n",
    "non-trivial observation 3: Is there a link between which domain an article comes from and if it is fake news?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each of the 29 domains present in the corpus has the following amount of articles in the corpus:\n",
      " [  1.  24.   1.   2.   6.   1.   1.   1.   1.   1.   1.   1. 155.   5.\n",
      "  17.   2.   1.   1.   4.   1.   2.   2.   3.   1.   1.   1.   7.   4.\n",
      "   2.]\n",
      "\n",
      "Each of the 29 domains present in the corpus has the following amount of fake news articles:\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 155.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.]\n",
      "\n",
      "This means that Beforeitsnews.com has 155 of the articles in the corpus and 62 % of all articles. Thus, the number of articles in the corpus are very unevenly spreed between the domains\n",
      "\n",
      "Also, 100 % of Beforeitsnews.com's articles are fake news and no other domain has fake news in its articles. Thus, there is a link between which domain an article comes from and if it is fake news\n"
     ]
    }
   ],
   "source": [
    "domainList = datasample['domain']\n",
    "typeList = datasample['type']\n",
    "domains = set(domainList)\n",
    "\n",
    "fakeDomainScore = np.zeros(len(domains))\n",
    "totalDomainScore = np.zeros(len(domains)) \n",
    "\n",
    "for i in range (len(domainList)):\n",
    "    if (typeList[i] == 'fake'):\n",
    "        index = 0 \n",
    "        for domain in domains:\n",
    "            if  domainList[i] == domain:\n",
    "                fakeDomainScore[index] += 1\n",
    "            index+=1\n",
    "    index = 0 \n",
    "    for domain in domains:\n",
    "        if  domainList[i] == domain:\n",
    "            totalDomainScore[index] += 1\n",
    "        index+=1\n",
    "\n",
    "print(\"Each of the 29 domains present in the corpus has the following amount of articles in the corpus:\\n\", totalDomainScore)\n",
    "print(\"\\nEach of the 29 domains present in the corpus has the following amount of fake news articles:\\n\", fakeDomainScore)\n",
    "print(\"\\nThis means that Beforeitsnews.com has\", int(totalDomainScore[np.where(fakeDomainScore == 155)]),\"of the articles in the corpus and\", int(fakeDomainScore[np.where(fakeDomainScore == 155)]*100/sum(totalDomainScore)), \"% of all articles. Thus, the number of articles in the corpus are very unevenly spreed between the domains\")\n",
    "print(\"\\nAlso,\", int(fakeDomainScore[np.where(fakeDomainScore == 155)]*100/totalDomainScore[np.where(fakeDomainScore == 155)]), \"% of Beforeitsnews.com's articles are fake news and no other domain has fake news in its articles. Thus, there is a link between which domain an article comes from and if it is fake news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-trivial observation 4: How many articles have missing author value? \n",
    "\n",
    "non-trivial observation 5: How much does missing author increase the likelihood that an article is fake news? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 % of the articles does not have an author\n",
      "63 % of the no-author articles are fake news\n",
      "61 % of the articles are fake news\n",
      "\n",
      "Thus we see, that having no author on an article only adds two percent points to the likelihood of it being fake\n"
     ]
    }
   ],
   "source": [
    "authors = datasample[\"authors\"]\n",
    "no_author_counter = 0\n",
    "no_author_fake_news = 0\n",
    "no_author_total = 0\n",
    "author_fake_news = 0\n",
    "author_total = 0\n",
    "\n",
    "for i in range(len(authors)):\n",
    "    if not type(authors[i]) == str:\n",
    "        no_author_counter += 1\n",
    "        if datasample[\"type\"][i] == \"fake\":\n",
    "            no_author_fake_news += 1\n",
    "        no_author_total += 1\n",
    "    elif datasample[\"type\"][i] == \"fake\":\n",
    "        author_fake_news += 1\n",
    "        author_total += 1\n",
    "    else: \n",
    "        author_total += 1\n",
    "\n",
    "print(int(no_author_counter*100/len(authors)), \"% of the articles does not have an author\")\n",
    "print(int(no_author_fake_news*100/no_author_total),'% of the no-author articles are fake news')\n",
    "print(int(author_fake_news*100/author_total),'% of the articles are fake news')\n",
    "print('\\nThus we see, that having no author on an article only adds two percent points to the likelihood of it being fake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following 'article start letters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCDEFGHIJK\n"
     ]
    }
   ],
   "source": [
    "group_nr = 1\n",
    "article_start_letters = \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr%23:group_nr%23+10]\n",
    "print(article_start_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The algorithm stops finding new articles when 'stop_searching' is set to True\n",
    "stop_searching = False\n",
    "\n",
    "#Finding the nextpage link in the first iteration is a little different, and therefore this value is needed\n",
    "first_iteration = True\n",
    "\n",
    "#The root url is the domain of wikinews\n",
    "root_link = 'https://en.wikinews.org'\n",
    "\n",
    "#next_page is the webpage that the algorithm searches for articles in next iteration of the while-loop\n",
    "next_page = root_link + '/w/index.php?title=Category:Politics_and_conflicts'\n",
    "\n",
    "#The links to the articles starting with the 'article_start_letters' are appended to 'article links'\n",
    "article_links = []\n",
    "\n",
    "#For each iteration this list gets some values if the first letter \n",
    "#of the first article in the next webpage is between A and K\n",
    "first_letter_between_B_K = []\n",
    "\n",
    "#A regex used for 'first_letter_between_B_K'\n",
    "continue_iterations = re.compile(r\"pagefrom=[A-K]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not(stop_searching):\n",
    "    soup = getSoup(next_page)\n",
    "    articles = soup.find(id=\"mw-pages\")\n",
    "    \n",
    "    links = [link.get(\"href\") for link in articles.find_all('a')]\n",
    "    \n",
    "    if first_iteration:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[0])\n",
    "        first_iteration = False\n",
    "        next_page = root_link + links[0]\n",
    "        article_links += [root_link + group_link for group_link in links[1:] if group_link[6] in article_start_letters]\n",
    "    else:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[1])\n",
    "        next_page = root_link + links[1]\n",
    "        article_links += [root_link + group_link for group_link in links[2:] if group_link[6] in article_start_letters]\n",
    "    \n",
    "    if len(first_letter_between_B_K) == 0:\n",
    "        stop_searching = True\n",
    "    \n",
    "    first_letter_between_B_K = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following box takes some time to run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_source_code = [getSoup(article) for article in article_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-53-085f02f1c052>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-53-085f02f1c052>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    article_content = [\" \".join([p.get_text() for p in ((article.find(id=\"mw-content-text\")).find(class=\"mw-parser-output\")).findall('p')]) for article in article_source_code]\u001b[0m\n\u001b[0m                                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "article_id = range(1,len(article_links))\n",
    "article_titles = [article.find('h1').get_text() for article in article_source_code]\n",
    "article_release_date = [str(article.find(id=\"publishDate\"))[50:60] for article in article_source_code]\n",
    "article_urls = article_links\n",
    "article_content = [\" \".join([p.get_text() for p in ((article.find(id=\"mw-content-text\")).find(class=\"mw-parser-output\")).findall('p')]) for article in article_source_code]\n",
    "article_sources = [\", \".join([element.get('href') for element in ((article.find('ul')).find_all('a', rel = 'nofollow', class_ ='external text'))]) for article in article_source_code]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titles</th>\n",
       "      <th>Release_Date</th>\n",
       "      <th>Sources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Category talk:Activists</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B.C. elections debate fiery but not conclusive</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Baghdad bombing kills several people, scores i...</td>\n",
       "      <td>2010-01-27</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Baghdad judge clears pair of murdering six for...</td>\n",
       "      <td>2010-10-10</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baghdad morgue received over 1,000 bodies in July</td>\n",
       "      <td>2005-08-18</td>\n",
       "      <td>http://news.independent.co.uk/world/fisk/artic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2853</th>\n",
       "      <td>Kyrgyz government declares elections valid, re...</td>\n",
       "      <td>2005-03-23</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2854</th>\n",
       "      <td>Kyrgyz president orders election probe as prot...</td>\n",
       "      <td>2005-03-22</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2855</th>\n",
       "      <td>Kyrgyzstan votes on referendum for new constit...</td>\n",
       "      <td>2010-06-27</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>Kyrgyzstan: Ethnic unrest continues, governmen...</td>\n",
       "      <td>2010-06-12</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2857</th>\n",
       "      <td>Kyrgyzstan: Violence continues, death toll rises</td>\n",
       "      <td>2010-06-14</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2858 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Titles Release_Date  \\\n",
       "0                               Category talk:Activists                \n",
       "1        B.C. elections debate fiery but not conclusive                \n",
       "2     Baghdad bombing kills several people, scores i...   2010-01-27   \n",
       "3     Baghdad judge clears pair of murdering six for...   2010-10-10   \n",
       "4     Baghdad morgue received over 1,000 bodies in July   2005-08-18   \n",
       "...                                                 ...          ...   \n",
       "2853  Kyrgyz government declares elections valid, re...   2005-03-23   \n",
       "2854  Kyrgyz president orders election probe as prot...   2005-03-22   \n",
       "2855  Kyrgyzstan votes on referendum for new constit...   2010-06-27   \n",
       "2856  Kyrgyzstan: Ethnic unrest continues, governmen...   2010-06-12   \n",
       "2857   Kyrgyzstan: Violence continues, death toll rises   2010-06-14   \n",
       "\n",
       "                                                Sources  \n",
       "0                                                        \n",
       "1                                                        \n",
       "2                                                        \n",
       "3                                                        \n",
       "4     http://news.independent.co.uk/world/fisk/artic...  \n",
       "...                                                 ...  \n",
       "2853                                                     \n",
       "2854                                                     \n",
       "2855                                                     \n",
       "2856                                                     \n",
       "2857                                                     \n",
       "\n",
       "[2858 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_articles = pd.DataFrame()\n",
    "\n",
    "pd_articles['Id'] = article_id\n",
    "pd_articles['Titles'] = article_titles\n",
    "pd_articles['Release_Date'] = article_release_date\n",
    "pd_articles['url'] = article_urls\n",
    "pd_articles['content'] = article_content\n",
    "pd_articles['Sources']= article_sources\n",
    "\n",
    "pd_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Reading the data from 1mio-raw.csv\n",
    "dataTotal = pd.read_csv('1mio-raw.csv/1mio-raw.csv')\n",
    "\n",
    "#We will only analyse a smaller part of the data set\n",
    "data = dataTotal[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'illus', 'least', 'quantum', 'level', 'theori', 'recent', 'confirm', 'set', 'research', 'final', 'mean', 'test', 'john', 'wheeler', '’', 'delayedchoic', 'theori', 'conclud', 'physicist', 'right', '<', 'num', '>', 'mr', 'wheeler', '’', 'propos', 'experi', 'involv', 'move', 'object', 'given', 'choic', 'act', 'like', 'wave', 'particl', 'former', 'act', 'vibrat', 'frequenc', 'distinguish', 'wave', 'latter', 'frequenc', 'determin', 'posit', 'space', 'unlik', 'wave', 'point', '‘', 'decid', '’', 'act', 'like', 'one', 'time', 'technolog', 'avail', 'conduct', 'strong', 'experi', 'scientist', 'abl', 'carri']\n"
     ]
    }
   ],
   "source": [
    "#Cleaning the content\n",
    "cleaned_content = [cleantext(article_content) for article_content in data['content']]\n",
    "\n",
    "#Tokenizing the cleaned data\n",
    "tokens = [tokenize(clean_text) for clean_text in cleaned_content]\n",
    "\n",
    "#Removing stopwords\n",
    "stopwords = [stopword(token_list) for token_list in tokens]\n",
    "\n",
    "#Stemming the data (this is used for the 'keywords' attribute)\n",
    "stemmed_data = [stemming(stopword_list) for stopword_list in stopwords]\n",
    "\n",
    "#Cleaning meta keywords\n",
    "clean_meta_keywords = [cleanMetaKeywords(metakeyword) for metakeyword in data[\"meta_keywords\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-75fb2397a38c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"tags\"] = [[tag.strip for tag in (str(i)).split(\",\")] for i in data[\"tags\"]]\n",
      "<ipython-input-14-75fb2397a38c>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"authors\"] = [[author.strip for author in (str(i)).split(\",\")] for i in data[\"authors\"]]\n",
      "<ipython-input-14-75fb2397a38c>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"meta_keywords\"] = [[meta_keyword.strip for meta_keyword in (str(i)).split(\",\")] for i in clean_meta_keywords]\n",
      "<ipython-input-14-75fb2397a38c>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"keywords\"] = stemmed_data\n"
     ]
    }
   ],
   "source": [
    "#Making sure that each element of 'tags', 'authors' and 'meta_keywords' are stripped stings and converting them to arrays\n",
    "data[\"tags\"] = [[tag.strip for tag in (str(i)).split(\",\")] for i in data[\"tags\"]]\n",
    "data[\"authors\"] = [[author.strip for author in (str(i)).split(\",\")] for i in data[\"authors\"]]\n",
    "data[\"meta_keywords\"] = [[meta_keyword.strip for meta_keyword in (str(i)).split(\",\")] for i in clean_meta_keywords]\n",
    "data[\"keywords\"] = stemmed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relation tables\n",
    "article_tags_relation_table = data[['id','tags']].explode('tags')\n",
    "owns_relation_table = data[['id', 'domain']]\n",
    "authors_of_article_table = data[['id','authors']].explode('authors')\n",
    "article_keywords_relation_table = data[['id','keywords']].explode('keywords')\n",
    "meta_article_keywords_relation_table  = data[['id','meta_keywords']].explode('meta_keywords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entity tables\n",
    "articles_table = data[['id','content','type','url','scraped_at','inserted_at','updated_at','meta_description','title']]\n",
    "keywords_table = pd.DataFrame(set(data[['keywords']].explode('keywords')))\n",
    "meta_keywords_table = pd.DataFrame(set(data[['meta_keywords']].explode('meta_keywords')))\n",
    "tags_table= pd.DataFrame(set(data[['tags']].explode('tags')))\n",
    "domain_table = pd.DataFrame(set(data['domain']))\n",
    "authors_table = pd.DataFrame(set(data[['authors']].explode('authors')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entities to CSV\n",
    "articles_table.to_csv(\"SQLtables/articles_table.csv\",index=False,header=False)\n",
    "keywords_table.to_csv(\"SQLtables/keywords_table.csv\",index=False,header=False)\n",
    "meta_keywords_table.to_csv(\"SQLtables/meta_keywords_table.csv\",index=False,header=False)\n",
    "tags_table.to_csv(\"SQLtables/tags_table.csv\",index=False,header=False)\n",
    "authors_table.to_csv(\"SQLtables/authors_table.csv\",index=False,header=False)\n",
    "domain_table.to_csv(\"SQLtables/domain_table.csv\",index=False,header=False)\n",
    "\n",
    "#Relations to CSV\n",
    "owns_relation_table.to_csv(\"SQLtables/owns_table.csv\",index=False,header=False)\n",
    "authors_of_article_table.to_csv(\"SQLtables/authors_of_article.csv\",index=False,header=False)\n",
    "article_tags_relation_table.to_csv(\"SQLtables/article_tags_relation_table.csv\",index=False,header=False)\n",
    "article_keywords_relation_table.to_csv(\"SQLtables/article_keywords_relation_table.csv\",index=False,header=False)\n",
    "meta_article_keywords_relation_table.to_csv(\"SQLtables/meta_article_keywords_relation_table.csv\",index=False,header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up a connection with the SQL server. Make sure that you write your own dbname, user and password as input\n",
    "conn = pc.connect(SQL_database_login)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating tables if they are not on the SQL server already\n",
    "cur.execute(\"BEGIN TRANSACTION;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.article(id integer, content text COLLATE pg_catalog.\\\"default\\\", type text COLLATE pg_catalog.\\\"default\\\", url text COLLATE pg_catalog.\\\"default\\\", scraped_at date, inserted_at date, updated_at date, meta_description text COLLATE pg_catalog.\\\"default\\\", title text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.article_keyword(id integer, keyword text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.article_meta_keywords(id integer, meta_keyword text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.article_tags(id integer,tag text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.authors(name text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.authors_of(article_id integer, author_name text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.domain(name text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.keywords(keyword text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.meta_keywords(meta_keyword text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.owns(article_id integer, domain_name text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.tags(tag text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"COMMIT TRANSACTION;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "InsufficientPrivilege",
     "evalue": "could not open file \"/Users/danielstephensen/Desktop/DataScienceGit/SQLtables/articles_table.csv\" for reading: Permission denied\nHINT:  COPY FROM instructs the PostgreSQL server process to read a file. You may want a client-side facility such as psql's \\copy.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInsufficientPrivilege\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-383b881c4e3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#(first all current tuples in the tables are deleted and then the new data is copied into the tables)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BEGIN TRANSACTION;\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delete from article *; copy article from '\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"articles_table.csv' with (format csv);\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delete from authors *; copy article from '\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"authors_table.csv' with (format csv);\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delete from authors_of *; copy article from '\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"authors_of_article.csv' with (format csv);\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInsufficientPrivilege\u001b[0m: could not open file \"/Users/danielstephensen/Desktop/DataScienceGit/SQLtables/articles_table.csv\" for reading: Permission denied\nHINT:  COPY FROM instructs the PostgreSQL server process to read a file. You may want a client-side facility such as psql's \\copy.\n"
     ]
    }
   ],
   "source": [
    "#Setting up the SQL tables from the csv files created above \n",
    "#(first all current tuples in the tables are deleted and then the new data is copied into the tables)\n",
    "cur.execute(\"BEGIN TRANSACTION;\")\n",
    "cur.execute(\"delete from article *; copy article from '\" + SQLtables_path + \"articles_table.csv' with (format csv);\")\n",
    "cur.execute(\"delete from authors *; copy article from '\" + SQLtables_path + \"authors_table.csv' with (format csv);\")\n",
    "cur.execute(\"delete from authors_of *; copy article from '\" + SQLtables_path + \"authors_of_article.csv' with (format csv);\")\n",
    "cur.execute(\"delete from owns *; copy article from '\" + SQLtables_path + \"owns_table.csv' with (format csv);\")\n",
    "cur.execute(\"delete from tags *; copy article from '\" + SQLtables_path + \"tags_table.csv' with (format csv);\")\n",
    "cur.execute(\"delete from keywords *; copy article from '\" + SQLtables_path + \"keywords_table.csv' with (format csv);\")\n",
    "cur.execute(\"delete from meta_keywords *; copy article from '\" + SQLtables_path + \"meta_keywords_table.csv' with (format csv);\")\n",
    "cur.execute(\"delete from domain *; copy article from '\" + SQLtables_path + \"domain_table.csv' with (format csv);\")\n",
    "cur.execute(\"delete from article_tags *; copy article from '\" + SQLtables_path + \"article_tags_relation_table.csv' with (format csv);\")\n",
    "cur.execute(\"delete from article_keyword *; copy article from '\" + SQLtables_path + \"article_keywords_relation_table.csv' with (format csv);\")\n",
    "cur.execute(\"delete from article_meta_keywords *; copy article from '\" + SQLtables_path + \"meta_article_keywords_relation_table.csv' with (format csv);\")\n",
    "cur.execute(\"COMMIT TRANSACTION;\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choice of Schema\n",
    "We have designed our database around the articles, using article_ID as the main primary key, which other relations refeer to.\n",
    "\n",
    "the table 'article' has the attributes which uniquely belonging to the given article. Attributes such as information about the time it was scraped, the URL, the type etc. all uniquely belongs to an article.\n",
    "\n",
    "We chose to create functional dependencies for the attributes 'tags', 'meta_keywords' and 'keywords' since each were consisting of up to a list of elements, making it hard to explore in SQL.\n",
    "\n",
    "Thus the relation tags consist of an unique row for each tag of each article. The same is true for meta_keywords and keywords.\n",
    "\n",
    "The relation Domain is one-to-many relation, since each article has one domain, but one domain can have many articles. Thus it makes sence to make a relation for domain.\n",
    "\n",
    "The relation Authors is a many-to-many relationship since each author can write many articles and any article can have multiple authors.\n",
    "\n",
    "Each relation thus binds our tables 'Authors', 'Domain', 'Tags', 'Meta_keywords' and 'keywords'. The tables right now only consist of a distinct list of author name, domain name, tag, meta_keyword and keyword. But this design enables easy scaling, if we want to add more information each table. For example the age of each author would easily be added to the 'authors' table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "InFailedSqlTransaction",
     "evalue": "current transaction is aborted, commands ignored until end of transaction block\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInFailedSqlTransaction\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d495b0e4b0f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Milestone 2 Task 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#1 [sql] List the domains of news articles of reliable type and scraped at or after January 15, 2018. NOTE: Do not include duplicate domains in your answer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select distinct domain_name from owns where article_id in (select id from article a where type = 'reliable' and a.scraped_at > '2018-01-24')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#2 [sql] List the name(s) of the most prolific author(s) of news articles of fake type. An author is among the most prolific if it has authored as many or more fake news articles as any other author in the dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInFailedSqlTransaction\u001b[0m: current transaction is aborted, commands ignored until end of transaction block\n"
     ]
    }
   ],
   "source": [
    "#Milestone 2 Task 3\n",
    "#1 [sql] List the domains of news articles of reliable type and scraped at or after January 15, 2018. NOTE: Do not include duplicate domains in your answer.\n",
    "cur.execute(\"select distinct domain_name from owns where article_id in (select id from article a where type = 'reliable' and a.scraped_at > '2018-01-24')\")\n",
    "print(cur.fetchall())\n",
    "#2 [sql] List the name(s) of the most prolific author(s) of news articles of fake type. An author is among the most prolific if it has authored as many or more fake news articles as any other author in the dataset.\n",
    "#We chose to fetch top 5 authors with most fake news articles. \n",
    "cur.execute(\"select author_name, count(*) from authors_of where article_id in (select id from article where type = 'fake') and author_name != 'nan' group by author_name order by count(*) desc limit 5 \")\n",
    "print(cur.fetchall())\n",
    "\n",
    "#3. [sql] Count the pairs of article IDs that exhibit the exact same set of meta-keywords, but only return the pairs where the set of meta-keywords is not empty. \n",
    "#cur.execute(\"select aa.id, bb.id from (select  from article_meta_keywords where meta_keyword !='') aa inner join (select   from article_meta_keywords where meta_keyword !='') bb on aa.meta_keyword = bb.meta_keyword  where aa.id < bb.id order by aa.id\")\n",
    "print(cur.fetchall())\n",
    "\n",
    "#cur.execute(\" select  aa.id, bb.id from (select count(meta_keyword) , id from article_meta_keywords where meta_keyword !=''group by id order by count desc) aa inner join  (select count(meta_keyword) , id from  article_meta_keywords  where meta_keyword !=''group by id order by count desc) bb on aa.count = bb.count  where aa.id < bb.id  order by aa.id  \")\n",
    "print(cur.fetchall())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 relational algebra\n",
    "\n",
    "reliable_domains := π{domain_name} (σ{a.type = 'reliable' and a.scraped_at > '2018-01-24' and a.id = o.article_id}(owns X article ))\n",
    "\n",
    "reliable_domains := π{domain_name} owns ⋈{a.type = 'reliable' and a.scraped_at > '2018-01-24' and a.id = o.article_id} article\n",
    "\n",
    "2# extended relational algebra\n",
    "\n",
    "top_5_fake_authors :=\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article_tags\n",
    "attribute: tag, article_id\n",
    "\n",
    "functional dependency: article_id\n",
    "\n",
    "primary key: none\n",
    "\n",
    "owns_relation\n",
    "attribute: domain, article_id\n",
    "\n",
    "functional dependency: article_id\n",
    "\n",
    "primary key: none\n",
    "\n",
    "authors_of_article\n",
    "attribute: author, article_id\n",
    "\n",
    "functional dependency: article_id\n",
    "\n",
    "primary key: none\n",
    "\n",
    "meta_article_keywords\n",
    "attribute: meta_keywords, article_id\n",
    "\n",
    "functional dependency: article_id\n",
    "\n",
    "primary key: none\n",
    "\n",
    "article_keyword\n",
    "attribute: keyword, article_id\n",
    "\n",
    "functional dependency: article_id\n",
    "\n",
    "primary key none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cur' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-d65d4e80241b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Milestone 2 Task 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#1 [sql] List the domains of news articles of reliable type and scraped at or after January 15, 2018. NOTE: Do not include duplicate domains in your answer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m cur.execute(\"\"\"\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mselect\u001b[0m \u001b[0mdistinct\u001b[0m \u001b[0mdomain_name\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mowns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0marticle_id\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cur' is not defined"
     ]
    }
   ],
   "source": [
    "#Milestone 2 Task 3\n",
    "#1 [sql] List the domains of news articles of reliable type and scraped at or after January 15, 2018. NOTE: Do not include duplicate domains in your answer.    \n",
    "cur.execute(\"\"\"\n",
    "select distinct domain_name from owns \n",
    "where article_id in \n",
    "(select id from article a \n",
    "where type = 'reliable' and a.scraped_at > '2018-01-24')\"\"\")\n",
    "print(cur.fetchall())\n",
    "#2 [sql] List the name(s) of the most prolific author(s) of news articles of fake type. An author is among the most prolific if it has authored as many or more fake news articles as any other author in the dataset.\n",
    "#We chose to fetch top 5 authors with most fake news articles. \n",
    "cur.execute(\"\"\"\n",
    "select author_name, count(*) from authors_of \n",
    "where article_id in \n",
    "    (select id from article \n",
    "    where type = 'fake') \n",
    "and author_name != 'nan' \n",
    "group by author_name \n",
    "order by count(*) desc limit 5 \"\"\")\n",
    "print(cur.fetchall())\n",
    "\n",
    "#3. [sql] Count the pairs of article IDs that exhibit the exact same set of meta-keywords, but only return the pairs where the set of meta-keywords is not empty. \n",
    "cur.execute(\"\"\"\n",
    "select count(*) from \n",
    "    (select * from \n",
    "        (select count(*) , * from \n",
    "            (select aa.id, bb.id from \n",
    "                (select * from article_meta_keywords where meta_keyword !='') aa \n",
    "            inner join \n",
    "                (select * from article_meta_keywords where meta_keyword !='') bb\n",
    "            on aa.meta_keyword = bb.meta_keyword  \n",
    "            where aa.id < bb.id \n",
    "            order by aa.id) keyword_match(col1, col2) \n",
    "            group by (col1, col2) \n",
    "            order by col1 asc) n_of_matches(matches_n, matches_id, b)\n",
    "\n",
    "        , \n",
    "\n",
    "            (select id, count(id) from article_meta_keywords\n",
    "            group by id \n",
    "            order by count desc) \n",
    "        n_of_metaKeywords(count_id, count_n) \n",
    "    where n_of_matches.matches_id = n_of_metaKeywords.count_id \n",
    "    and n_of_matches.matches_n= n_of_metaKeywords.count_n \n",
    "    order by matches_n desc )\n",
    " total\"\"\")\n",
    "print(cur.fetchall())\n",
    "#Bonus: get the list of id's with matching meta_keywords: \n",
    "cur.execute(\"\"\"\n",
    "select matches_n, matches_id, matched_with from \n",
    "    (select count(*) , * from \n",
    "        (select aa.id, bb.id from \n",
    "            (select * from article_meta_keywords where meta_keyword !='') aa \n",
    "            inner join \n",
    "            (select * from article_meta_keywords where meta_keyword !='') bb \n",
    "        on aa.meta_keyword = bb.meta_keyword  \n",
    "        where aa.id < bb.id order by aa.id) keyword_match(col1, col2)\n",
    "        group by (col1, col2) \n",
    "        order by col1 asc) n_of_matches(matches_n, matches_id, matched_with)\n",
    "\n",
    "        , \n",
    "\n",
    "        (select id, count(id) from article_meta_keywords\n",
    "        group by id order by count desc) \n",
    "        n_of_metaKeywords(count_id, count_n)\n",
    "\n",
    "where \n",
    "n_of_matches.matches_id = n_of_metaKeywords.count_id and\n",
    "n_of_matches.matches_n= n_of_metaKeywords.count_n\n",
    "order by matches_n desc\n",
    "\"\"\")\n",
    "print(cur.fetchall()[:30])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
