{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/danielstephensen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danielstephensen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import porter\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2 as pc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group members: Angelina Näsström (nzv947), Daniel Stephensen (fbp131), Kristina Wilke (mlt790), Lauritz Koch (hdg618)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the following procedures: cleaning, tokenizing, removing stopwords and stemming the data. When cleaning the data we made sure of the following: \n",
    "1. all letters are in lowercase\n",
    "2. all urls are written as < URL >\n",
    "3. all dates are written as < DATE >\n",
    "4. all emails are written as < EMAIL >\n",
    "5. all numbers are written as < NUM >\n",
    "6. all unimportant symbols are removed\n",
    "\n",
    "Converting all letters to lowercase makes it easier to compare different words. Point 2-5 are useful because it makes it possible to count the number of urls, dates, emails and numbers. Also, removing these makes sure that they are not treated as words. Removing unimportant symbols makes sure that these are not treated as words. \n",
    "\n",
    "Tokenization makes processing of the data easier, as it eliminates blank spaces and punctuations etc, making the text more homogeneous. In the tokenization process, we, for example, made all the data lower-case, thus not having two different results when processing 'Hello' and 'hello'.\n",
    "\n",
    "Removing stopwords is useful because these words do not help giving meaning to the documents, in other words they are noise.\n",
    "\n",
    "Stemming the data is useful because it makes sure that different variants of the same word is converted into the rood of the word. This way it is possible to make sure that two different words (same word with different endings) are understood the same way, because they actually have the exact same meaning.\n",
    "\n",
    "Implementing task 2 we have used the Pandas library, nltk library and re library. The Pandas library has just been used to read the data from the 'news_sample.csv' file. word_tokenize is a sublibrary of nltk that has some useful functions for tokenizing. stopwords is a sublibrary of nltk.corpus that has some useful functions for removing stopwords. porter is a sublibrary of nltk.stem that has some useful functions for stemming data. These three sublibraries are useful because you do not need to create your own complex functions to tokenize, remove stopwords and stem the data. We have not used the clean_text library because we it did not have all the functionality needed for the task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3071: DtypeWarning: Columns (0,1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "#datasample is the data sample used in milestone 1 and dataTotal is the data sample used in milestone 2\n",
    "datasample = pd.read_csv('news_sample.csv')\n",
    "dataTotal = pd.read_csv('1mio-raw.csv/1mio-raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will only analyse a smaller part of the data set\n",
    "data = dataTotal[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cleantext cleans the input string with the following functions: Characters are set to lowercase, \n",
    "#urls are substituted with <URL>, dates are substitured with <DATE>, emails are substitured with <EMAIL>\n",
    "#numbers are substitured with <NUM>, newlines and non-letter characters are removed.\n",
    "def cleantext(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<|>', \"\", text)\n",
    "    text = re.sub(r'(https?:\\/\\/)?w{0,3}\\.?[a-z]+\\.[a-z]\\w*[\\w\\/-]*', \"<URL>\", text)\n",
    "    text = re.sub(r'(jan\\.?(uary)?|feb\\.?(uary)?|mar\\.?(ch)?|apr\\.?(il)?|may|jun\\.(e)?|jul\\.(y)?|aug\\.?(ust)?|sep\\.?(tember)?|oct\\.?(ober)?|nov\\.?(ember)?|dec\\.?(ember)?|monday|tuesday|wednesday|thursday|friday|saturday|sunday) (the )?\\d{1,2}((th)?,?( \\d{4})?)?', \"<DATE>\", text)\n",
    "    text = re.sub(r'\\w+@\\w+\\.[a-zA-Z]{2,3}', \"<EMAIL>\", text)\n",
    "    text = re.sub(r'[0-9]+', \"<NUM>\", text)\n",
    "    text = re.sub(r'(\\\\n)+|\\s{2,}|(\\\\t+)', \" \", text)\n",
    "    text = re.sub(r'\\.|,|\\\\|-|\\?|\\(|\\)|\\||&|\"|”|“|:|!|\\+|-|–|—|\\/|\\$|%|€|#|;|\\[|\\]|©|®|…|=', \"\", text)\n",
    "    return text\n",
    "\n",
    "#cleanMetaKeywords cleans the input string with the following functions: \n",
    "#Characters are set to lowercase, newlines and non-letter characters are removed.\n",
    "def cleanMetaKeywords(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(\\\\n)+|\\s{2,}|(\\\\t+)', \" \", text)\n",
    "    text = re.sub(r'\\.|\\\\|-|\\?|\\(|\\)|\\||&|\"|”|“|:|!|\\+|-|\\'|–|—|\\/|\\$|%|€|#|;|\\[|\\]|©|®|…|=|<|>', \"\", text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is Three part job of \n",
    "1) Cleaning data\n",
    "2) Tokenization \n",
    "3) Removing stopwords\n",
    "4) Stemming\n",
    "\n",
    "The processed data will be saved in the 'Keywords' column, such that the original data can be untouched in 'content'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Setup\n",
    "stop_words = stopwords.words('english')\n",
    "stemmer = porter.PorterStemmer()\n",
    "\n",
    "#Cleaning content data\n",
    "cleaned_text_table = [cleantext(i) for i in data[\"content\"]] \n",
    "\n",
    "#Creating tokens\n",
    "clean_data_tokens = [word_tokenize(i) for i in cleaned_text_table] \n",
    "\n",
    "#Stopword removal\n",
    "non_stop_words = list(map(lambda token_list: [word for word in token_list if word not in stop_words], clean_data_tokens))\n",
    "\n",
    "#Stemming the data \n",
    "stemmed_data = list(map(lambda non_stop_word_list: set([stemmer.stem(non_stopword) for non_stopword in non_stop_word_list]), non_stop_words))\n",
    "\n",
    "#Cleaning meta keywords\n",
    "clean_meta_keywords = [cleanMetaKeywords(i) for i in data[\"meta_keywords\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-75fb2397a38c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"tags\"] = [[tag.strip for tag in (str(i)).split(\",\")] for i in data[\"tags\"]]\n",
      "<ipython-input-14-75fb2397a38c>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"authors\"] = [[author.strip for author in (str(i)).split(\",\")] for i in data[\"authors\"]]\n",
      "<ipython-input-14-75fb2397a38c>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"meta_keywords\"] = [[meta_keyword.strip for meta_keyword in (str(i)).split(\",\")] for i in clean_meta_keywords]\n",
      "<ipython-input-14-75fb2397a38c>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"keywords\"] = stemmed_data\n"
     ]
    }
   ],
   "source": [
    "#Making sure that each element of 'tags', 'authors' and 'meta_keywords' are stings and converting them to arrays\n",
    "data[\"tags\"] = [[tag.strip for tag in (str(i)).split(\",\")] for i in data[\"tags\"]]\n",
    "data[\"authors\"] = [[author.strip for author in (str(i)).split(\",\")] for i in data[\"authors\"]]\n",
    "data[\"meta_keywords\"] = [[meta_keyword.strip for meta_keyword in (str(i)).split(\",\")] for i in clean_meta_keywords]\n",
    "data[\"keywords\"] = stemmed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relation tables\n",
    "article_tags_relation_table = data[['id','tags']].explode('tags')\n",
    "owns_relation_table = data[['id', 'domain']]\n",
    "authors_of_article_table = data[['id','authors']].explode('authors')\n",
    "article_keywords_relation_table = data[['id','keywords']].explode('keywords')\n",
    "meta_article_keywords_relation_table  = data[['id','meta_keywords']].explode('meta_keywords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entity tables\n",
    "articles_table = data[['id','content','type','url','scraped_at','inserted_at','updated_at','meta_description','title']]\n",
    "keywords_table = pd.DataFrame(set(data[['keywords']].explode('keywords')))\n",
    "meta_keywords_table = pd.DataFrame(set(data[['meta_keywords']].explode('meta_keywords')))\n",
    "tags_table= pd.DataFrame(set(data[['tags']].explode('tags')))\n",
    "domain_table = pd.DataFrame(set(data['domain']))\n",
    "authors_table = pd.DataFrame(set(data[['authors']].explode('authors')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entities to CSV\n",
    "articles_table.to_csv(\"SQLtables/articles_table.csv\",index=False,header=False)\n",
    "keywords_table.to_csv(\"SQLtables/keywords_table.csv\",index=False,header=False)\n",
    "meta_keywords_table.to_csv(\"SQLtables/meta_keywords_table.csv\",index=False,header=False)\n",
    "tags_table.to_csv(\"SQLtables/tags_table.csv\",index=False,header=False)\n",
    "authors_table.to_csv(\"SQLtables/authors_table.csv\",index=False,header=False)\n",
    "domain_table.to_csv(\"SQLtables/domain_table.csv\",index=False,header=False)\n",
    "\n",
    "#Relations to CSV\n",
    "owns_relation_table.to_csv(\"SQLtables/owns_table.csv\",index=False,header=False)\n",
    "authors_of_article_table.to_csv(\"SQLtables/authors_of_article.csv\",index=False,header=False)\n",
    "article_tags_relation_table.to_csv(\"SQLtables/article_tags_relation_table.csv\",index=False,header=False)\n",
    "article_keywords_relation_table.to_csv(\"SQLtables/article_keywords_relation_table.csv\",index=False,header=False)\n",
    "meta_article_keywords_relation_table.to_csv(\"SQLtables/meta_article_keywords_relation_table.csv\",index=False,header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up a connection with the SQL server. Make sure that you write your own dbname, user and password as input\n",
    "conn = pc.connect(\"dbname=**** user=**** password=****\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating tables if they are not on the SQL server already\n",
    "cur.execute(\"BEGIN TRANSACTION;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.article(id integer, content text COLLATE pg_catalog.\\\"default\\\", type text COLLATE pg_catalog.\\\"default\\\", url text COLLATE pg_catalog.\\\"default\\\", scraped_at date, inserted_at date, updated_at date, meta_description text COLLATE pg_catalog.\\\"default\\\", title text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.article_keyword(id integer, keyword text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.article_meta_keywords(id integer, meta_keyword text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.article_tags(id integer,tag text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.authors(name text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.authors_of(article_id integer, author_name text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.domain(name text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.keywords(keyword text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.meta_keywords(meta_keyword text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.owns(article_id integer, domain_name text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS public.tags(tag text COLLATE pg_catalog.\\\"default\\\") WITH (OIDS = FALSE) TABLESPACE pg_default;\")\n",
    "cur.execute(\"COMMIT TRANSACTION;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "syntax error at or near \"\\\"\nLINE 1: delete from article *; \\copy article from '/Users/danielstep...\n                               ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSyntaxError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-0adc65cd6d8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#(first all current tuples in the tables are deleted and then the new data is copied into the tables)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BEGIN TRANSACTION;\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delete from article *; \\copy article from '\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"articles_table.csv' with (format csv);\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delete from authors *; \\copy article from '\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"authors_table.csv' with (format csv);\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delete from authors_of *; \\copy article from '\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"authors_of_article.csv' with (format csv);\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSyntaxError\u001b[0m: syntax error at or near \"\\\"\nLINE 1: delete from article *; \\copy article from '/Users/danielstep...\n                               ^\n"
     ]
    }
   ],
   "source": [
    "#Path to the repository with the above csv files in it. Make sure that the path is adapted to your computer!\n",
    "path = '/Users/danielstephensen/Desktop/DataScienceGit/SQLtables/'\n",
    "\n",
    "#Setting up the SQL tables from the csv files created above \n",
    "#(first all current tuples in the tables are deleted and then the new data is copied into the tables)\n",
    "cur.execute(\"BEGIN TRANSACTION;\")\n",
    "cur.execute(\"delete from article *; copy article from '\" + path + \"articles_table.csv' with (format csv);\")\n",
    "cur.execute(\"delete from authors *; copy article from '\" + path + \"authors_table.csv' with (format csv);\")\n",
    "cur.execute(\"delete from authors_of *; copy article from '\" + path + \"authors_of_article.csv' with (format csv);\")\n",
    "cur.execute(\"delete from owns *; copy article from '\" + path + \"owns_table.csv' with (format csv);\")\n",
    "cur.execute(\"delete from tags *; copy article from '\" + path + \"tags_table.csv' with (format csv);\")\n",
    "cur.execute(\"delete from keywords *; copy article from '\" + path + \"keywords_table.csv' with (format csv);\")\n",
    "cur.execute(\"delete from meta_keywords *; copy article from '\" + path + \"meta_keywords_table.csv' with (format csv);\")\n",
    "cur.execute(\"delete from domain *; copy article from '\" + path + \"domain_table.csv' with (format csv);\")\n",
    "cur.execute(\"delete from article_tags *; copy article from '\" + path + \"article_tags_relation_table.csv' with (format csv);\")\n",
    "cur.execute(\"delete from article_keyword *; copy article from '\" + path + \"article_keywords_relation_table.csv' with (format csv);\")\n",
    "cur.execute(\"delete from article_meta_keywords *; copy article from '\" + path + \"meta_article_keywords_relation_table.csv' with (format csv);\")\n",
    "cur.execute(\"COMMIT TRANSACTION;\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "InFailedSqlTransaction",
     "evalue": "current transaction is aborted, commands ignored until end of transaction block\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInFailedSqlTransaction\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d495b0e4b0f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Milestone 2 Task 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#1 [sql] List the domains of news articles of reliable type and scraped at or after January 15, 2018. NOTE: Do not include duplicate domains in your answer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select distinct domain_name from owns where article_id in (select id from article a where type = 'reliable' and a.scraped_at > '2018-01-24')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#2 [sql] List the name(s) of the most prolific author(s) of news articles of fake type. An author is among the most prolific if it has authored as many or more fake news articles as any other author in the dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInFailedSqlTransaction\u001b[0m: current transaction is aborted, commands ignored until end of transaction block\n"
     ]
    }
   ],
   "source": [
    "#Milestone 2 Task 3\n",
    "#1 [sql] List the domains of news articles of reliable type and scraped at or after January 15, 2018. NOTE: Do not include duplicate domains in your answer.\n",
    "cur.execute(\"select distinct domain_name from owns where article_id in (select id from article a where type = 'reliable' and a.scraped_at > '2018-01-24')\")\n",
    "print(cur.fetchall())\n",
    "#2 [sql] List the name(s) of the most prolific author(s) of news articles of fake type. An author is among the most prolific if it has authored as many or more fake news articles as any other author in the dataset.\n",
    "#We chose to fetch top 5 authors with most fake news articles. \n",
    "cur.execute(\"select author_name, count(*) from authors_of where article_id in (select id from article where type = 'fake') and author_name != 'nan' group by author_name order by count(*) desc limit 5 \")\n",
    "print(cur.fetchall())\n",
    "\n",
    "#3. [sql] Count the pairs of article IDs that exhibit the exact same set of meta-keywords, but only return the pairs where the set of meta-keywords is not empty. \n",
    "#cur.execute(\"select aa.id, bb.id from (select  from article_meta_keywords where meta_keyword !='') aa inner join (select   from article_meta_keywords where meta_keyword !='') bb on aa.meta_keyword = bb.meta_keyword  where aa.id < bb.id order by aa.id\")\n",
    "print(cur.fetchall())\n",
    "\n",
    "#cur.execute(\" select  aa.id, bb.id from (select count(meta_keyword) , id from article_meta_keywords where meta_keyword !=''group by id order by count desc) aa inner join  (select count(meta_keyword) , id from  article_meta_keywords  where meta_keyword !=''group by id order by count desc) bb on aa.count = bb.count  where aa.id < bb.id  order by aa.id  \")\n",
    "print(cur.fetchall())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 relational algebra\n",
    "\n",
    "reliable_domains := π{domain_name} (σ{a.type = 'reliable' and a.scraped_at > '2018-01-24' and a.id = o.article_id}(owns X article ))\n",
    "\n",
    "reliable_domains := π{domain_name} owns ⋈{a.type = 'reliable' and a.scraped_at > '2018-01-24' and a.id = o.article_id} article\n",
    "\n",
    "2# extended relational algebra\n",
    "\n",
    "top_5_fake_authors :=\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-trivial observation 1: How many percent of articles with the word \"trump\" in it is fake news?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_articles = [word_tokenize(i) for i in clean_data]\n",
    "articles_vocabulary = [set(i) for i in tokenized_articles]\n",
    "trump_included = [i for i in range(len(articles_vocabulary)) if \"trump\" in articles_vocabulary[i]]\n",
    "trump_fake_news = 0\n",
    "\n",
    "for i in range(len(trump_included)):\n",
    "    if data['type'][i] == \"fake\":\n",
    "        trump_fake_news += 1\n",
    "\n",
    "print(int(trump_fake_news*100/len(trump_included)),\"% of articles where the name 'trump' is present, is a fake news article\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-trivial observation 2: Is the number of articles spread out tolerably evenly between the domains?\n",
    "\n",
    "non-trivial observation 3: Is there a link between which domain an article comes from and if it is fake news?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_articles = [word_tokenize(i) for i in clean_data]\n",
    "articles_vocabulary = [set(i) for i in tokenized_articles]\n",
    "#Missing author corellation\n",
    "\n",
    "domainList = data['domain']\n",
    "TypeList = data['type']\n",
    "domains = set(domainList)\n",
    "fakeDomainScore = np.zeros(len(domains))\n",
    "totalDomainScore = np.zeros(len(domains)) \n",
    "for i in range (len(domainList)):\n",
    "    if (data['type'][i] == 'fake'):\n",
    "        index = 0 \n",
    "        for domain in domains:\n",
    "            if  data['domain'][i] == domain:\n",
    "                fakeDomainScore[index] += 1\n",
    "            index+=1\n",
    "    index = 0 \n",
    "    for domain in domains:\n",
    "        if  data['domain'][i] == domain:\n",
    "            totalDomainScore[index] += 1\n",
    "        index+=1\n",
    "print(\"Each of the 29 domains present in the corpus has the following amount of articles in the corpus:\\n\", totalDomainScore)\n",
    "print(\"\\nEach of the 29 domains present in the corpus has the following amount of fake news articles:\\n\", fakeDomainScore)\n",
    "print(\"\\nThis means that Beforeitsnews.com has\", int(totalDomainScore[np.where(fakeDomainScore == 155)]),\"of the articles in the corpus and\", int(fakeDomainScore[np.where(fakeDomainScore == 155)]*100/sum(totalDomainScore)), \"% of all articles. Thus, the number of articles in the corpus are very unevenly spreed between the domains\")\n",
    "print(\"\\nAlso,\", int(fakeDomainScore[np.where(fakeDomainScore == 155)]*100/totalDomainScore[np.where(fakeDomainScore == 155)]), \"% of Beforeitsnews.com's articles are fake news and no other domain has fake news in its articles. Thus, there is a link between which domain an article comes from and if it is fake news (The link is probably a little to big)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-trivial observation 4: How many articles have missing author value? \n",
    "\n",
    "non-trivial observation 5: How much does missing author increase the likelihood that an article is fake news? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = [i for i in data[\"authors\"]]\n",
    "no_author_counter = 0\n",
    "no_author_fake_news = 0\n",
    "no_author_total = 0\n",
    "author_fake_news = 0\n",
    "author_total = 0\n",
    "\n",
    "for i in range(len(authors)):\n",
    "    if not type(authors[i]) == str:\n",
    "        no_author_counter += 1\n",
    "        if data[\"type\"][i] == \"fake\":\n",
    "            no_author_fake_news += 1\n",
    "        no_author_total += 1\n",
    "    elif data[\"type\"][i] == \"fake\":\n",
    "        author_fake_news += 1\n",
    "        author_total += 1\n",
    "    else: \n",
    "        author_total += 1\n",
    "\n",
    "print(int(no_author_counter*100/len(authors)), \"% of the articles does not have an author\")\n",
    "print(int(no_author_fake_news*100/no_author_total),'% of the no-author articles are fake news')\n",
    "print(int(author_fake_news*100/author_total),'% of the articles are fake news')\n",
    "print('Thus we see, that having no author on an article only adds two percent points to the likelihood of it being fake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following 'article start letters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_nr = 1\n",
    "article_start_letters = \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr%23:group_nr%23+10]\n",
    "print(article_start_letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_searching = 0\n",
    "next_page = 'https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts'\n",
    "article_links = []\n",
    "first_letter_between_B_K = []\n",
    "count = 0\n",
    "\n",
    "continue_iterations = re.compile(r\"pagefrom=[A-K]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_iteration = True\n",
    "\n",
    "while stop_searching == 0:\n",
    "    response = requests.get(next_page)\n",
    "    contents = response.content\n",
    "    soup = BeautifulSoup(contents, 'html.parser')\n",
    "    articles = soup.find(id=\"mw-pages\")\n",
    "    \n",
    "    links = [link.get(\"href\") for link in articles.find_all('a')]\n",
    "    \n",
    "    if first_iteration:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[0])\n",
    "        first_iteration = False\n",
    "        next_page = 'https://en.wikinews.org'+links[0]\n",
    "        article_links += ['https://en.wikinews.org' + group_link for group_link in links[1:] if group_link[6] in article_start_letters]\n",
    "    else:\n",
    "        first_letter_between_B_K = continue_iterations.findall(links[1])\n",
    "        next_page = 'https://en.wikinews.org'+links[1]\n",
    "        article_links += ['https://en.wikinews.org' + group_link for group_link in links[2:] if group_link[6] in article_start_letters]\n",
    "    \n",
    "    if len(first_letter_between_B_K) == 0:\n",
    "        stop_searching = 1\n",
    "    \n",
    "    first_letter_between_B_K = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following box takes about 20 minutes to run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_source_code = [BeautifulSoup(requests.get(article).content, 'html.parser') for article in article_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_titles = [article.find('h1').get_text() for article in article_source_code]\n",
    "article_release_date = [str(article.find(id=\"publishDate\"))[50:60] for article in article_source_code]\n",
    "article_sources = [\", \".join([element.get('href') for element in ((article.find('ul')).find_all('a', rel = 'nofollow', class_ ='external text'))]) for article in article_source_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_articles = pd.DataFrame()\n",
    "pd_articles['Titles'] = article_titles\n",
    "pd_articles['Release_Date'] = article_release_date\n",
    "pd_articles['Sources']= article_sources\n",
    "\n",
    "pd_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
